{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "1hk63l1o9gy7HpSZIKElMAvQsIgKg375M",
      "authorship_tag": "ABX9TyMPmtFp3WZxPvZtIA13FOec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathanrbelanger-lang/Janus_Arc/blob/main/Janus_Hero_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title [Run] Janus Architecture Testing Script: Runs head to head tests to determine an optimal configuration for a 40m Model\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# UPDATE THIS PATH TO YOUR BIN FILE\n",
        "DATA_FILE = \"/content/drive/MyDrive/Project_XAI_Physical_Janus/data/processed/TinyStories-train_full.bin\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/Project_XAI_Physical_Janus/data/results/janus_v2_clean\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"‚öîÔ∏è JANUS GAUNTLET: The Clean Room\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. THE ARCHITECTURE (INLINE - NO SAFEGUARDS) ---\n",
        "\n",
        "class CleanConfig:\n",
        "    def __init__(self, **kwargs):\n",
        "        # Defaults\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 16 # Default, overridden in tests\n",
        "        self.max_seq_len = 128\n",
        "        self.dropout = 0.0\n",
        "        self.lambda_diversity = 0.0\n",
        "        self.lambda_coherence = 0.0\n",
        "        self.spatial_schedule = 'cubic'\n",
        "        self.enable_steering = True\n",
        "\n",
        "        for k, v in kwargs.items(): setattr(self, k, v)\n",
        "\n",
        "        # Calculated\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "\n",
        "class CleanScheduler:\n",
        "    def __init__(self, config):\n",
        "        self.total_layers = config.n_layers\n",
        "        self.base_div = config.lambda_diversity\n",
        "        self.base_coh = config.lambda_coherence\n",
        "        self.spatial = config.spatial_schedule\n",
        "\n",
        "    def get_lambdas(self, step, max_steps, layer_id):\n",
        "        # Time Ramp (Trapezoidal)\n",
        "        t_mult = 0.0\n",
        "        if max_steps > 0:\n",
        "            prog = step / max_steps\n",
        "            if prog < 0.2: t_mult = prog / 0.2\n",
        "            elif prog < 0.8: t_mult = 1.0\n",
        "            else: t_mult = max(0.0, 1.0 - (prog - 0.8)/0.2)\n",
        "\n",
        "        # Space Ramp\n",
        "        ratio = (layer_id + 1) / self.total_layers\n",
        "        if self.spatial == 'cubic': s_mult = ratio ** 3\n",
        "        elif self.spatial == 'linear': s_mult = ratio\n",
        "        else: s_mult = 1.0\n",
        "\n",
        "        return (self.base_coh * t_mult * s_mult, self.base_div * t_mult * s_mult)\n",
        "\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_model // config.n_heads\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head) # <--- THIS GUARANTEES LOSS DIVERGENCE\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, lambdas):\n",
        "        B, S, D = x.shape\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        # Steering Physics\n",
        "        steer_loss = 0.0\n",
        "        if l_div > 0.0:\n",
        "            # Orthogonality: Penalize cosine sim between head outputs\n",
        "            # Flatten to (H, B*S*D)\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config, layer_id)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "    def forward(self, x, lambdas):\n",
        "        a, s = self.attn(self.ln1(x), lambdas)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config, i) for i in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.scheduler = CleanScheduler(config)\n",
        "        self.step = 0; self.max_steps = 1\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "\n",
        "        total_steer = 0.0\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            lambdas = self.scheduler.get_lambdas(self.step, self.max_steps, i)\n",
        "            x, s = block(x, lambdas)\n",
        "            total_steer += s\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, total_steer\n",
        "\n",
        "# --- 3. LOADER (MEMMAP) ---\n",
        "class BinLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        print(f\"üì¶ Data Loaded: {len(self.data):,} tokens\")\n",
        "\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# --- 4. THE GAUNTLET ---\n",
        "def run_probe(name, config_dict, loader, steps=500):\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = CleanConfig(**config_dict)\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "\n",
        "    losses = []\n",
        "    pbar = tqdm(range(steps), desc=name, leave=False)\n",
        "\n",
        "    model.train()\n",
        "    for step in pbar:\n",
        "        x, y = loader.get_batch()\n",
        "        model.step = step\n",
        "        model.max_steps = steps\n",
        "\n",
        "        loss, steer = model(x, y)\n",
        "        total = loss + steer\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        if step % 20 == 0:\n",
        "            pbar.set_description(f\"{name} | L:{loss.item():.3f}\")\n",
        "\n",
        "    return np.mean(losses[-100:])\n",
        "\n",
        "def run_pressure_ramp(name, config_dict, loader, steps=500):\n",
        "    print(f\"\\nüèãÔ∏è Ramping Pressure on {name}...\")\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = CleanConfig(**config_dict)\n",
        "    # Start at 0 pressure\n",
        "    cfg.lambda_diversity = 0.0\n",
        "\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "\n",
        "    baseline = 10.0\n",
        "\n",
        "    model.train()\n",
        "    for step in range(steps):\n",
        "        x, y = loader.get_batch()\n",
        "\n",
        "        # Manually ramp pressure in the scheduler\n",
        "        curr_p = 0.50 * (step / steps) # Max 0.50\n",
        "        model.scheduler.base_div = curr_p\n",
        "        model.scheduler.base_coh = curr_p * 0.2\n",
        "        model.step = step\n",
        "        model.max_steps = steps\n",
        "\n",
        "        loss, steer = model(x, y)\n",
        "        total = loss + steer\n",
        "\n",
        "        optimizer.zero_grad(); total.backward(); optimizer.step()\n",
        "\n",
        "        if step == 50: baseline = loss.item()\n",
        "\n",
        "        # Explosion Check (30% spike)\n",
        "        if step > 100 and loss.item() > baseline * 1.3:\n",
        "            print(f\"üí• FRACTURE at Pressure {curr_p:.4f} (Loss {loss.item():.3f})\")\n",
        "            return curr_p * 0.75 # 25% safety buffer\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"   Step {step} | P:{curr_p:.3f} | L:{loss.item():.3f}\")\n",
        "\n",
        "    print(\"‚úÖ No Fracture. Cap at 0.50.\")\n",
        "    return 0.40 # Safe conservative cap\n",
        "\n",
        "def main():\n",
        "    loader = BinLoader(DATA_FILE, 128, 64)\n",
        "    manifest = {}\n",
        "\n",
        "    # --- PHASE 1: GEOMETRY ---\n",
        "    print(\"\\n\\n=== PHASE 1: GEOMETRY DUEL ===\")\n",
        "    # Note: We add tiny pressure (0.01) to force differentiation if needed,\n",
        "    # but the scaling factor alone should differentiate them.\n",
        "    score_32 = run_probe(\"16x32d\", {'n_heads': 16, 'lambda_diversity': 0.01}, loader)\n",
        "    score_64 = run_probe(\"8x64d\",  {'n_heads': 8, 'lambda_diversity': 0.01}, loader)\n",
        "\n",
        "    print(f\"\\nüèÅ RESULTS: 16x32d={score_32:.4f} | 8x64d={score_64:.4f}\")\n",
        "    if score_64 < score_32:\n",
        "        print(\"üèÜ WINNER: 64-dim Heads\")\n",
        "        best_geo = {'n_heads': 8}\n",
        "    else:\n",
        "        print(\"üèÜ WINNER: 32-dim Heads\")\n",
        "        best_geo = {'n_heads': 16}\n",
        "\n",
        "    manifest.update(best_geo)\n",
        "\n",
        "    # --- PHASE 2: PRESSURE ---\n",
        "    print(\"\\n\\n=== PHASE 2: PRESSURE TEST ===\")\n",
        "    safe_p = run_pressure_ramp(\"Winner\", best_geo, loader)\n",
        "    print(f\"üõ°Ô∏è Safe Pressure: {safe_p:.4f}\")\n",
        "    manifest['lambda_diversity'] = safe_p\n",
        "    manifest['lambda_coherence'] = safe_p * 0.2\n",
        "\n",
        "    # --- PHASE 3: SPATIAL ---\n",
        "    print(\"\\n\\n=== PHASE 3: SPATIAL DUEL ===\")\n",
        "    print(f\"Testing schedules at P={safe_p:.4f}\")\n",
        "\n",
        "    score_lin = run_probe(\"Linear\", {**best_geo, 'lambda_diversity': safe_p, 'spatial_schedule': 'linear'}, loader)\n",
        "    score_cub = run_probe(\"Cubic\",  {**best_geo, 'lambda_diversity': safe_p, 'spatial_schedule': 'cubic'}, loader)\n",
        "\n",
        "    print(f\"\\nüèÅ RESULTS: Linear={score_lin:.4f} | Cubic={score_cub:.4f}\")\n",
        "    if score_cub < score_lin:\n",
        "        print(\"üèÜ WINNER: Cubic\")\n",
        "        manifest['spatial_schedule'] = 'cubic'\n",
        "    else:\n",
        "        print(\"üèÜ WINNER: Linear\")\n",
        "        manifest['spatial_schedule'] = 'linear'\n",
        "\n",
        "    # --- SAVE ---\n",
        "    manifest['n_layers'] = 12\n",
        "    manifest['d_model'] = 512\n",
        "    out_path = os.path.join(RESULTS_DIR, \"janus_v2_manifest.json\")\n",
        "    with open(out_path, 'w') as f:\n",
        "        json.dump(manifest, f, indent=4)\n",
        "    print(f\"\\nüíæ Manifest Saved: {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d28pxn1RnmWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title [RUN] JSmallHConfirmation\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# PATHS\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v2\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ JANUS HERO V2: The Homeostatic Run\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIGURATION (LOCKED) ---\n",
        "class HeroConfig:\n",
        "    def __init__(self):\n",
        "        # ARCHITECTURE (Winner of Geometry Duel)\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8  # 64-dim heads\n",
        "        self.d_head = 64  # Explicit\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # HOMEOSTASIS (67% of Max Safe)\n",
        "        self.max_lambda_div = 0.27\n",
        "        self.max_lambda_coh = 0.054 # 20% rule\n",
        "        self.spatial_schedule = 'cubic'\n",
        "\n",
        "        # TRAINING\n",
        "        self.max_steps = 5000\n",
        "        self.batch_size = 32\n",
        "        self.grad_accum = 2 # Effective Batch 64\n",
        "        self.val_interval = 250\n",
        "        self.val_steps = 50\n",
        "\n",
        "# --- 3. THE ENGINE (INLINE) ---\n",
        "class CleanScheduler:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.current_div = 0.0\n",
        "        self.current_coh = 0.0\n",
        "\n",
        "    def step(self, step_num):\n",
        "        \"\"\"3-Stage Burn Strategy\"\"\"\n",
        "        # Stage 1: IGNITION (0 - 750) -> Zero Pressure\n",
        "        if step_num < 750:\n",
        "            self.current_div = 0.0\n",
        "            self.current_coh = 0.0\n",
        "\n",
        "        # Stage 2: PRESSURIZATION (750 - 2000) -> Ramp to Target\n",
        "        elif step_num < 2000:\n",
        "            progress = (step_num - 750) / (2000 - 750)\n",
        "            self.current_div = self.config.max_lambda_div * progress\n",
        "            self.current_coh = self.config.max_lambda_coh * progress\n",
        "\n",
        "        # Stage 3: CRUISING (2000+) -> Hold Steady\n",
        "        else:\n",
        "            self.current_div = self.config.max_lambda_div\n",
        "            self.current_coh = self.config.max_lambda_coh\n",
        "\n",
        "        return self.current_div\n",
        "\n",
        "    def get_lambdas(self, layer_id):\n",
        "        # Spatial Component (Cubic)\n",
        "        ratio = (layer_id + 1) / self.config.n_layers\n",
        "        s_mult = ratio ** 3\n",
        "        return (self.current_coh * s_mult, self.current_div * s_mult)\n",
        "\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        # --- PHYSICS & TELEMETRY ---\n",
        "        metrics = {}\n",
        "        steer_loss = 0.0\n",
        "\n",
        "        # 1. Steering (Training Only)\n",
        "        if l_div > 0.0 and self.training:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        # 2. Telemetry (If requested)\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                # Sigma_P (Focus/Entropy)\n",
        "                entropy = -torch.sum(attn_probs * torch.log(attn_probs + 1e-9), dim=-1)\n",
        "                max_ent = math.log(S)\n",
        "                metrics['sigma_p'] = (1.0 - (entropy / max_ent)).mean(dim=[0, 2]) # Avg over Batch/Seq\n",
        "\n",
        "                # Sigma_A (Uniqueness/Orthogonality)\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                # Eff_Rank (Dimensionality)\n",
        "                # Sample subset for speed\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "                # Kurtosis (Sharpness)\n",
        "                # flat_probs: (H, B*S*S) - Expensive, estimate on sample\n",
        "                # Skipping full kurtosis to save VRAM, using Sigma_P as proxy for sharpness\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.scheduler = CleanScheduler(config)\n",
        "\n",
        "    def forward(self, idx, targets=None, step_num=0, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "\n",
        "        # Update Scheduler\n",
        "        current_div = self.scheduler.step(step_num)\n",
        "\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            lambdas = self.scheduler.get_lambdas(i)\n",
        "            # Only calculate expensive metrics for deep layers or periodically\n",
        "            do_metrics = return_metrics # Can optimize later\n",
        "            x, s, m = block(x, lambdas, do_metrics)\n",
        "            total_steer += s\n",
        "            if do_metrics: all_metrics.append(m)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, total_steer, all_metrics, current_div\n",
        "\n",
        "# --- 4. DATA LOADER (SPLIT) ---\n",
        "class SplitLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        total_tokens = len(self.data)\n",
        "        split_idx = int(total_tokens * 0.95)\n",
        "\n",
        "        self.train_data = self.data[:split_idx]\n",
        "        self.val_data = self.data[split_idx:]\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        print(f\"üì¶ Data Split | Train: {len(self.train_data):,} | Val: {len(self.val_data):,}\")\n",
        "\n",
        "    def get_batch(self, split='train'):\n",
        "        source = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(source) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(source[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(source[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# --- 5. BLACK BOX (LOGGER) ---\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "    def log(self, step, loss, val_loss, pressure, metrics_list):\n",
        "        # Flatten metrics for storage\n",
        "        # We store averages per layer to keep file size manageable\n",
        "        row = {\n",
        "            \"step\": step,\n",
        "            \"loss\": loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure\n",
        "        }\n",
        "\n",
        "        for i, layer_m in enumerate(metrics_list):\n",
        "            if not layer_m: continue\n",
        "            # Average across heads for the summary log\n",
        "            row[f\"L{i}_sigma_p\"] = layer_m['sigma_p'].mean().item()\n",
        "            row[f\"L{i}_sigma_a\"] = layer_m['sigma_a'].mean().item()\n",
        "            row[f\"L{i}_eff_rank\"] = layer_m['eff_rank'].mean().item()\n",
        "\n",
        "        self.buffer.append(row)\n",
        "\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        # Append mode\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_hero.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            existing = pd.read_parquet(fpath)\n",
        "            df = pd.concat([existing, df])\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. MAIN LOOP ---\n",
        "def run_hero():\n",
        "    # Init\n",
        "    cfg = HeroConfig()\n",
        "    loader = SplitLoader(DATA_FILE, cfg.max_seq_len, cfg.batch_size)\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4, weight_decay=1e-4)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    # Checkpoint check\n",
        "    start_step = 0\n",
        "    ckpt_path = os.path.join(SAVE_DIR, \"ckpt_latest.pt\")\n",
        "    if os.path.exists(ckpt_path):\n",
        "        print(\"üîÑ Resuming from Checkpoint...\")\n",
        "        ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "        optimizer.load_state_dict(ckpt['optim'])\n",
        "        start_step = ckpt['step']\n",
        "\n",
        "    print(f\"\\nüèÉ STARTING RUN: {start_step} -> {cfg.max_steps}\")\n",
        "\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    accum_loss = 0.0\n",
        "\n",
        "    for step in pbar:\n",
        "        # 1. Training Step\n",
        "        model.train()\n",
        "\n",
        "        # Gradient Accumulation\n",
        "        batch_loss = 0.0\n",
        "        batch_steer = 0.0\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch('train')\n",
        "            # Only calc metrics on the last micro-batch to save compute\n",
        "            do_metrics = (_ == cfg.grad_accum - 1)\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics, pressure = model(x, y, step, return_metrics=do_metrics)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "            if steer > 0: batch_steer += steer.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss = batch_loss / cfg.grad_accum\n",
        "\n",
        "        # 2. Validation & Logging (Interval)\n",
        "        val_loss = 0.0\n",
        "        if step % cfg.val_interval == 0 or step == cfg.max_steps - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(cfg.val_steps):\n",
        "                    vx, vy = loader.get_batch('val')\n",
        "                    # No steering in validation\n",
        "                    vl, _, _, _ = model(vx, vy, step, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            # Log to BlackBox\n",
        "            recorder.log(step, avg_loss, val_loss, pressure, metrics)\n",
        "            recorder.flush()\n",
        "\n",
        "            # Save Checkpoint\n",
        "            torch.save({\n",
        "                'step': step,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': optimizer.state_dict()\n",
        "            }, ckpt_path)\n",
        "\n",
        "            # Update Bar\n",
        "            desc = f\"L:{avg_loss:.3f}|V:{val_loss:.3f}|P:{pressure:.3f}\"\n",
        "            pbar.set_description(desc)\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{avg_loss:.3f}|P:{pressure:.3f}\")\n",
        "\n",
        "    # Final Save\n",
        "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"janus_hero_final.pt\"))\n",
        "    print(\"\\n‚úÖ MISSION COMPLETE.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_hero()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-Ap33t3up-gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Janus Hero v2 Telemetry Analyzer\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "TELEMETRY_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v2/telemetry_hero.parquet\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üïµÔ∏è JANUS AUTOPSY: Forensic Analysis Tool\")\n",
        "print(f\"üìÇ Input: {TELEMETRY_PATH}\")\n",
        "print(f\"üìÑ Output: {REPORT_DIR}\")\n",
        "\n",
        "# --- 2. THE ANALYST ENGINE ---\n",
        "class JanusAutopsy:\n",
        "    def __init__(self, filepath):\n",
        "        self.df = pd.read_parquet(filepath)\n",
        "        self.pdf_path = os.path.join(REPORT_DIR, \"janus_hero_autopsy.pdf\")\n",
        "        self.phases = {\n",
        "            'Ignition': (0, 750),\n",
        "            'Pressurization': (750, 2000),\n",
        "            'Cruising': (2000, 5000)\n",
        "        }\n",
        "\n",
        "        # Pre-calc global averages if not present\n",
        "        if 'sigma_a_avg' not in self.df.columns:\n",
        "            # Detect layer columns dynamically\n",
        "            l_cols = [c for c in self.df.columns if 'sigma_a' in c and 'L' in c]\n",
        "            self.df['sigma_a_avg'] = self.df[l_cols].mean(axis=1)\n",
        "\n",
        "        if 'eff_rank_avg' not in self.df.columns:\n",
        "            l_cols = [c for c in self.df.columns if 'eff_rank' in c and 'L' in c]\n",
        "            self.df['eff_rank_avg'] = self.df[l_cols].mean(axis=1)\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(self.df)} steps of telemetry.\")\n",
        "\n",
        "    def run_full_analysis(self):\n",
        "        with PdfPages(self.pdf_path) as pdf:\n",
        "            # 1. Title Page & Global Vitals\n",
        "            self._plot_global_vitals(pdf)\n",
        "\n",
        "            # 2. Phase Correlation Matrices\n",
        "            self._plot_phase_correlations(pdf)\n",
        "\n",
        "            # 3. Layer Tomography (The MRI)\n",
        "            self._plot_layer_tomography(pdf, metric='sigma_a', title=\"Uniqueness (Sigma_A)\")\n",
        "            self._plot_layer_tomography(pdf, metric='eff_rank', title=\"Dimensionality (Eff Rank)\")\n",
        "\n",
        "            # 4. Elasticity (Pressure Response)\n",
        "            self._plot_elasticity(pdf)\n",
        "\n",
        "            # 5. The Business Metric (Perplexity vs Pressure)\n",
        "            self._plot_perplexity_pressure(pdf)\n",
        "\n",
        "        print(f\"\\n‚ú® Analysis Complete. Report saved to: {self.pdf_path}\")\n",
        "\n",
        "    def _plot_global_vitals(self, pdf):\n",
        "        \"\"\"Page 1: The Heartbeat (Loss, Pressure, Redundancy)\"\"\"\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
        "\n",
        "        # Plot Loss\n",
        "        sns.lineplot(data=self.df, x='step', y='loss', ax=axes[0], color='tab:red', label='Train Loss')\n",
        "        if 'val_loss' in self.df.columns:\n",
        "            sns.lineplot(data=self.df, x='step', y='val_loss', ax=axes[0], color='tab:orange', label='Val Loss', linestyle='--')\n",
        "        axes[0].set_title('Global Loss Trajectory')\n",
        "        axes[0].set_ylabel('Cross Entropy')\n",
        "        axes[0].legend()\n",
        "\n",
        "        # Plot Pressure\n",
        "        sns.lineplot(data=self.df, x='step', y='pressure', ax=axes[1], color='tab:green', linewidth=2)\n",
        "        axes[1].set_title('Diversity Pressure (Lambda)')\n",
        "        axes[1].set_ylabel('Force')\n",
        "\n",
        "        # Plot Redundancy (Inverted Sigma_A for intuitive \"Redundancy\" view, or raw Sigma_A)\n",
        "        # Let's plot raw Sigma_A (Uniqueness)\n",
        "        sns.lineplot(data=self.df, x='step', y='sigma_a_avg', ax=axes[2], color='tab:blue')\n",
        "        axes[2].set_title('Average Head Uniqueness (Sigma_A)')\n",
        "        axes[2].set_ylabel('Orthogonality Score')\n",
        "\n",
        "        # Mark Phases\n",
        "        for ax in axes:\n",
        "            for phase, (start, end) in self.phases.items():\n",
        "                ax.axvline(x=start, color='gray', linestyle=':', alpha=0.5)\n",
        "                if end < self.df['step'].max():\n",
        "                    ax.axvline(x=end, color='gray', linestyle=':', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        pdf.savefig(fig)\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_phase_correlations(self, pdf):\n",
        "        \"\"\"Page 2: Correlation Matrices by Phase\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "        cols = ['loss', 'val_loss', 'pressure', 'sigma_a_avg', 'eff_rank_avg']\n",
        "        # Filter cols that exist\n",
        "        cols = [c for c in cols if c in self.df.columns]\n",
        "\n",
        "        for i, (phase_name, (start, end)) in enumerate(self.phases.items()):\n",
        "            mask = (self.df['step'] >= start) & (self.df['step'] < end)\n",
        "            subset = self.df.loc[mask, cols]\n",
        "\n",
        "            if len(subset) > 10:\n",
        "                corr = subset.corr()\n",
        "                sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1, ax=axes[i], cbar=False)\n",
        "                axes[i].set_title(f\"Phase: {phase_name}\")\n",
        "            else:\n",
        "                axes[i].text(0.5, 0.5, \"Insufficient Data\", ha='center')\n",
        "\n",
        "        plt.suptitle(\"Phase Transition Correlations\")\n",
        "        plt.tight_layout()\n",
        "        pdf.savefig(fig)\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_layer_tomography(self, pdf, metric, title):\n",
        "        \"\"\"Page 3/4: The Layer-wise MRI\"\"\"\n",
        "        # Extract layer columns\n",
        "        l_cols = [c for c in self.df.columns if metric in c and 'L' in c]\n",
        "        # Sort by layer index L0, L1...\n",
        "        l_cols.sort(key=lambda x: int(x.split('_')[0].replace('L','')))\n",
        "\n",
        "        if not l_cols: return\n",
        "\n",
        "        # Pivot data for heatmap (Layers x Time)\n",
        "        # We need to downsample time for visibility if huge\n",
        "        sample_rate = max(1, len(self.df) // 100)\n",
        "        subset = self.df.iloc[::sample_rate].copy()\n",
        "\n",
        "        heatmap_data = subset[l_cols].T # Rows=Layers, Cols=Time\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.heatmap(heatmap_data, cmap='viridis', ax=ax, cbar_kws={'label': metric})\n",
        "\n",
        "        ax.set_title(f\"Layer Tomography: {title}\")\n",
        "        ax.set_ylabel(\"Network Depth (Layer 0 -> 11)\")\n",
        "        ax.set_xlabel(\"Training Time (Steps)\")\n",
        "\n",
        "        # Fix X-axis labels to show steps\n",
        "        xticks = np.linspace(0, len(subset), 10)\n",
        "        xlabels = [int(subset.iloc[int(i) if i < len(subset) else -1]['step']) for i in xticks]\n",
        "        ax.set_xticks(xticks)\n",
        "        ax.set_xticklabels(xlabels)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        pdf.savefig(fig)\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_elasticity(self, pdf):\n",
        "        \"\"\"Page 5: The Modulus of Elasticity (dUniqueness / dPressure)\"\"\"\n",
        "        # Calculate Rolling Correlation between Pressure and Sigma_A\n",
        "        window = 200\n",
        "\n",
        "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        rolling_corr = self.df['pressure'].rolling(window).corr(self.df['sigma_a_avg'])\n",
        "\n",
        "        sns.lineplot(x=self.df['step'], y=rolling_corr, color='purple', ax=ax1, linewidth=2)\n",
        "        ax1.set_title(\"Elasticity: Correlation(Pressure, Uniqueness) over Time\")\n",
        "        ax1.set_ylabel(\"Correlation Coefficient (Pearson)\")\n",
        "        ax1.axhline(0, color='black', linewidth=1)\n",
        "        ax1.axhline(-1, color='red', linestyle='--', alpha=0.3)\n",
        "\n",
        "        # Annotate\n",
        "        ax1.text(self.df['step'].max()*0.5, -0.8, \"Strong Response (Elastic)\", color='red', ha='center')\n",
        "        ax1.text(self.df['step'].max()*0.5, 0.2, \"No Response (Plastic/Collapsed)\", color='gray', ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        pdf.savefig(fig)\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_perplexity_pressure(self, pdf):\n",
        "        \"\"\"Page 6: The Golden Cross\"\"\"\n",
        "        if 'perplexity' not in self.df.columns: return\n",
        "\n",
        "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Plot Perplexity\n",
        "        color = 'tab:orange'\n",
        "        ax1.set_xlabel('Step')\n",
        "        ax1.set_ylabel('Perplexity (Lower is Better)', color=color)\n",
        "        sns.lineplot(data=self.df, x='step', y='perplexity', ax=ax1, color=color)\n",
        "        ax1.tick_params(axis='y', labelcolor=color)\n",
        "        ax1.set_ylim(bottom=0)\n",
        "\n",
        "        # Twin axis for Pressure\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:green'\n",
        "        ax2.set_ylabel('Diversity Pressure', color=color)\n",
        "        sns.lineplot(data=self.df, x='step', y='pressure', ax=ax2, color=color, alpha=0.3, linestyle='--')\n",
        "        ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        plt.title(\"The 'Golden Cross': Perplexity vs. Pressure\")\n",
        "        plt.tight_layout()\n",
        "        pdf.savefig(fig)\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(TELEMETRY_PATH):\n",
        "        analyst = JanusAutopsy(TELEMETRY_PATH)\n",
        "        analyst.run_full_analysis()\n",
        "    else:\n",
        "        print(f\"‚ùå Telemetry file not found at: {TELEMETRY_PATH}\")\n",
        "        print(\"Did you run the Hero script to completion?\")"
      ],
      "metadata": {
        "id": "WJyi6WYNSGtj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "TELEMETRY_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v2/telemetry_hero.parquet\")\n",
        "REPORT_PATH = os.path.join(PROJECT_ROOT, \"reports/janus_final_report.md\")\n",
        "\n",
        "print(f\"üìù JANUS WRITER: Generating Text Report\")\n",
        "print(f\"üìÇ Source: {TELEMETRY_PATH}\")\n",
        "\n",
        "class ForensicWriter:\n",
        "    def __init__(self, filepath):\n",
        "        self.df = pd.read_parquet(filepath)\n",
        "\n",
        "        # Calculate derived columns if missing\n",
        "        if 'sigma_a_avg' not in self.df.columns:\n",
        "            l_cols = [c for c in self.df.columns if 'sigma_a' in c and 'L' in c]\n",
        "            if l_cols: self.df['sigma_a_avg'] = self.df[l_cols].mean(axis=1)\n",
        "            else: self.df['sigma_a_avg'] = 0.0 # Fallback\n",
        "\n",
        "        if 'eff_rank_avg' not in self.df.columns:\n",
        "            l_cols = [c for c in self.df.columns if 'eff_rank' in c and 'L' in c]\n",
        "            if l_cols: self.df['eff_rank_avg'] = self.df[l_cols].mean(axis=1)\n",
        "            else: self.df['eff_rank_avg'] = 0.0\n",
        "\n",
        "        # Define Phases\n",
        "        self.phases = {\n",
        "            'Ignition': self.df[self.df['step'] < 750],\n",
        "            'Pressurization': self.df[(self.df['step'] >= 750) & (self.df['step'] < 2000)],\n",
        "            'Cruising': self.df[self.df['step'] >= 2000]\n",
        "        }\n",
        "\n",
        "    def generate_report(self):\n",
        "        with open(REPORT_PATH, 'w') as f:\n",
        "            # HEADER\n",
        "            f.write(\"# üèõÔ∏è JANUS-HERO v2: Forensic Analysis Report\\n\\n\")\n",
        "            f.write(f\"**Status:** {'‚úÖ COMPLETE' if self.df['step'].max() >= 4900 else '‚ö†Ô∏è INCOMPLETE'}\\n\")\n",
        "            f.write(f\"**Total Steps:** {self.df['step'].max()}\\n\")\n",
        "            f.write(f\"**Checkpoints Logged:** {len(self.df)}\\n\\n\")\n",
        "\n",
        "            # 1. EXECUTIVE SUMMARY\n",
        "            f.write(\"## 1. Executive Summary\\n\")\n",
        "            start_loss = self.df.iloc[0]['loss']\n",
        "            end_loss = self.df.iloc[-1]['loss']\n",
        "            loss_delta = ((end_loss - start_loss) / start_loss) * 100\n",
        "\n",
        "            final_perp = self.df.iloc[-1].get('perplexity', 0.0)\n",
        "\n",
        "            f.write(f\"The model completed the 3-stage burn protocol. \")\n",
        "            f.write(f\"Training Loss moved from **{start_loss:.3f}** to **{end_loss:.3f}** ({loss_delta:.1f}%). \")\n",
        "            if final_perp > 0:\n",
        "                f.write(f\"Final Validation Perplexity settled at **{final_perp:.2f}**.\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"Perplexity data unavailable.\\n\\n\")\n",
        "\n",
        "            # 2. PHASE ANALYSIS\n",
        "            f.write(\"## 2. Phase Analysis\\n\")\n",
        "\n",
        "            # --- IGNITION ---\n",
        "            ign = self.phases['Ignition']\n",
        "            if not ign.empty:\n",
        "                f.write(\"### üî• Phase 1: Ignition (Steps 0-750)\\n\")\n",
        "                f.write(\"*Goal: Natural Feature Formation (Zero Pressure)*\\n\")\n",
        "                f.write(f\"- **Avg Loss:** {ign['loss'].mean():.3f}\\n\")\n",
        "                f.write(f\"- **Avg Redundancy (Sigma_A):** {ign['sigma_a_avg'].mean():.3f}\\n\\n\")\n",
        "\n",
        "            # --- PRESSURIZATION ---\n",
        "            press = self.phases['Pressurization']\n",
        "            if not press.empty and len(press) > 2:\n",
        "                f.write(\"### üèãÔ∏è Phase 2: Pressurization (Steps 750-2000)\\n\")\n",
        "                f.write(\"*Goal: Forced Orthogonality (Ramping Pressure)*\\n\")\n",
        "\n",
        "                # Elasticity Calculation (Corr between Pressure and Sigma_A)\n",
        "                # We want Negative correlation (Pressure UP -> Redundancy DOWN)\n",
        "                corr, _ = pearsonr(press['pressure'], press['sigma_a_avg'])\n",
        "                elasticity = \"Elastic (Responsive)\" if corr < -0.5 else \"Plastic (Resistant)\"\n",
        "\n",
        "                f.write(f\"- **Elasticity Coefficient:** {corr:.3f} ({elasticity})\\n\")\n",
        "                f.write(f\"- **Pressure Delta:** {press['pressure'].min():.2f} -> {press['pressure'].max():.2f}\\n\")\n",
        "                f.write(f\"- **Redundancy Response:** {press.iloc[0]['sigma_a_avg']:.3f} -> {press.iloc[-1]['sigma_a_avg']:.3f}\\n\\n\")\n",
        "\n",
        "            # --- CRUISING ---\n",
        "            cruise = self.phases['Cruising']\n",
        "            if not cruise.empty:\n",
        "                f.write(\"### ‚úàÔ∏è Phase 3: Cruising (Steps 2000+)\\n\")\n",
        "                f.write(\"*Goal: High-Efficiency Convergence*\\n\")\n",
        "\n",
        "                start_p = cruise.iloc[0]['loss']\n",
        "                end_p = cruise.iloc[-1]['loss']\n",
        "                stability = \"Stable\" if abs(start_p - end_p) < 0.5 else \"Volatile\"\n",
        "\n",
        "                f.write(f\"- **Stability:** {stability}\\n\")\n",
        "                f.write(f\"- **Final Effective Rank:** {cruise.iloc[-1]['eff_rank_avg']:.2f} (Target: >4.0)\\n\")\n",
        "                f.write(f\"- **Final Uniqueness:** {cruise.iloc[-1]['sigma_a_avg']:.3f}\\n\\n\")\n",
        "\n",
        "            # 3. ANOMALY DETECTION\n",
        "            f.write(\"## 3. Anomalies & Warnings\\n\")\n",
        "            # Check for Loss Spikes\n",
        "            spikes = self.df[self.df['loss'].diff() > 0.5]\n",
        "            if not spikes.empty:\n",
        "                f.write(f\"‚ö†Ô∏è **Loss Spikes Detected:** Found {len(spikes)} events where loss jumped > 0.5.\\n\")\n",
        "                for _, row in spikes.iterrows():\n",
        "                    f.write(f\"- Step {int(row['step'])}: Loss {row['loss']:.3f}\\n\")\n",
        "            else:\n",
        "                f.write(\"‚úÖ **Trajectory Clean:** No significant loss spikes detected.\\n\")\n",
        "\n",
        "            # Check for Collapse\n",
        "            if self.df.iloc[-1]['sigma_a_avg'] > 0.8:\n",
        "                f.write(\"‚ö†Ô∏è **CRITICAL WARNING:** High Redundancy (>0.80) detected at end of run. Possible Mode Collapse.\\n\")\n",
        "            elif self.df.iloc[-1]['sigma_a_avg'] < 0.1:\n",
        "                f.write(\"‚ö†Ô∏è **WARNING:** Ultra-low Redundancy (<0.10). Model may be over-regularized (incoherent).\\n\")\n",
        "            else:\n",
        "                f.write(\"‚úÖ **Homeostasis Achieved:** Redundancy is within healthy parameters (0.10 - 0.80).\\n\")\n",
        "\n",
        "        print(f\"‚úÖ Report compiled: {REPORT_PATH}\")\n",
        "        # Print preview to console\n",
        "        with open(REPORT_PATH, 'r') as f:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(f.read())\n",
        "            print(\"=\"*40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(TELEMETRY_PATH):\n",
        "        writer = ForensicWriter(TELEMETRY_PATH)\n",
        "        writer.generate_report()\n",
        "    else:\n",
        "        print(\"‚ùå Telemetry file missing.\")"
      ],
      "metadata": {
        "id": "Mw5wBKIqTPfF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# PATHS\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "MODEL_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v2/janus_hero_final.pt\")\n",
        "# We need the tokenizer data to decode\n",
        "TOKEN_BIN = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üó£Ô∏è JANUS INFERENCE: The Moment of Truth\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. RECONSTRUCT ARCHITECTURE (CLEAN ROOM) ---\n",
        "# Must match the training config exactly\n",
        "class HeroConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.0 # No dropout needed for inference\n",
        "\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out)\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -512:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- 3. LOAD MODEL & TOKENIZER ---\n",
        "# Quick Tokenizer Hack: We use PreTrainedTokenizerFast if available, or fallback\n",
        "try:\n",
        "    from transformers import PreTrainedTokenizerFast\n",
        "    # Assuming you saved the tokenizer in processed/hero_mix or similar\n",
        "    # If not, we can rely on standard GPT2 encoding if vocab matches\n",
        "    # Let's try to load the one you built\n",
        "    TOK_PATH = os.path.join(PROJECT_ROOT, \"data/processed/hero_mix\")\n",
        "    if os.path.exists(TOK_PATH):\n",
        "        tokenizer = PreTrainedTokenizerFast.from_pretrained(TOK_PATH)\n",
        "        print(\"‚úÖ Custom Tokenizer Loaded\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Custom Tokenizer not found. Falling back to GPT2 (Warning: ID mismatch likely)\")\n",
        "        from transformers import GPT2Tokenizer\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå Transformers lib not installed. Install it!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Load Weights\n",
        "cfg = HeroConfig()\n",
        "model = CleanGPT(cfg).to(DEVICE)\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(f\"üîÑ Loading Weights from {MODEL_PATH}\")\n",
        "    # Load state dict\n",
        "    try:\n",
        "        # It might be saved as a dict with 'model' key\n",
        "        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "        if 'model' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model'])\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "        print(\"‚úÖ Weights Loaded Successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Weight Load Failed: {e}\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    print(\"‚ùå Model file not found.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# --- 4. THE PROBE ---\n",
        "prompts = [\n",
        "    \"Once upon a time\",\n",
        "    \"Lily wanted a\",\n",
        "    \"The big red ball\",\n",
        "    \"Tom went to the\",\n",
        "    \"One day, a little\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"üß™ JANUS HERO V2 OUTPUTS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\nüìù PROMPT: {p}\")\n",
        "    try:\n",
        "        # Encode\n",
        "        input_ids = tokenizer.encode(p, return_tensors='pt').to(DEVICE)\n",
        "\n",
        "        # Generate\n",
        "        output_ids = model.generate(input_ids, max_new_tokens=100, temperature=0.6, top_k=40)\n",
        "\n",
        "        # Decode\n",
        "        text = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
        "        print(f\"ü§ñ JANUS: {text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Generation Error: {e}\")"
      ],
      "metadata": {
        "id": "9GGIHF_eUbbW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# 1. Load the Standard GPT-2 Tokenizer (The one likely used for the .bin file)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Re-Run Inference\n",
        "print(f\"\\n‚ú® RE-TESTING WITH GPT-2 TOKENIZER ‚ú®\")\n",
        "prompts = [\"Once upon a time\", \"Lily wanted a\", \"The big red ball\"]\n",
        "\n",
        "for p in prompts:\n",
        "    input_ids = tokenizer.encode(p, return_tensors='pt').to(DEVICE)\n",
        "    output = model.generate(input_ids, max_new_tokens=100, temperature=0.6, top_k=40)\n",
        "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"\\nüìù {p} -> {text}\")"
      ],
      "metadata": {
        "id": "kx8Z0qdnVaJ1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "# New Save Dir for Baseline\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_baseline\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üìâ JANUS BASELINE: The Control Group\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIGURATION (CONTROL) ---\n",
        "class BaselineConfig:\n",
        "    def __init__(self):\n",
        "        # EXACT SAME ARCHITECTURE\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # ZERO PRESSURE (The Variable)\n",
        "        self.max_lambda_div = 0.0\n",
        "        self.max_lambda_coh = 0.0\n",
        "\n",
        "        # SAME TRAINING SPECS\n",
        "        self.max_steps = 5000\n",
        "        self.batch_size = 32\n",
        "        self.grad_accum = 2\n",
        "        self.val_interval = 250\n",
        "        self.val_steps = 50\n",
        "\n",
        "# --- 3. THE ENGINE (NO STEERING) ---\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        # NO STEERING LOSS HERE\n",
        "        steer_loss = 0.0\n",
        "        metrics = {}\n",
        "\n",
        "        # Still log telemetry to prove it's bad\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                # Sigma_P (Focus)\n",
        "                entropy = -torch.sum(attn_probs * torch.log(attn_probs + 1e-9), dim=-1)\n",
        "                max_ent = math.log(S)\n",
        "                metrics['sigma_p'] = (1.0 - (entropy / max_ent)).mean(dim=[0, 2])\n",
        "\n",
        "                # Sigma_A (Uniqueness)\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                # Eff_Rank\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "    def forward(self, x, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "\n",
        "        all_metrics = []\n",
        "        for block in self.blocks:\n",
        "            x, _, m = block(x, return_metrics)\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, 0.0, all_metrics\n",
        "\n",
        "# --- 4. DATA & LOGGING ---\n",
        "class SplitLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        total_tokens = len(self.data)\n",
        "        split_idx = int(total_tokens * 0.95)\n",
        "        self.train_data = self.data[:split_idx]\n",
        "        self.val_data = self.data[split_idx:]\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        print(f\"üì¶ Split: Train {len(self.train_data):,} | Val {len(self.val_data):,}\")\n",
        "\n",
        "    def get_batch(self, split='train'):\n",
        "        source = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(source) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(source[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(source[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "    def log(self, step, loss, val_loss, metrics_list):\n",
        "        row = {\n",
        "            \"step\": step,\n",
        "            \"loss\": loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": 0.0 # Baseline has no pressure\n",
        "        }\n",
        "        for i, layer_m in enumerate(metrics_list):\n",
        "            if not layer_m: continue\n",
        "            row[f\"L{i}_sigma_p\"] = layer_m['sigma_p'].mean().item()\n",
        "            row[f\"L{i}_sigma_a\"] = layer_m['sigma_a'].mean().item()\n",
        "            row[f\"L{i}_eff_rank\"] = layer_m['eff_rank'].mean().item()\n",
        "        self.buffer.append(row)\n",
        "\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_baseline.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            existing = pd.read_parquet(fpath)\n",
        "            df = pd.concat([existing, df])\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "\n",
        "# --- 5. MAIN ---\n",
        "def run_baseline():\n",
        "    cfg = BaselineConfig()\n",
        "    loader = SplitLoader(DATA_FILE, cfg.max_seq_len, cfg.batch_size)\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4, weight_decay=1e-4)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    print(f\"\\nüìâ STARTING BASELINE RUN: 0 -> {cfg.max_steps}\")\n",
        "    pbar = tqdm(range(cfg.max_steps))\n",
        "\n",
        "    for step in pbar:\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch('train')\n",
        "            do_metrics = (_ == cfg.grad_accum - 1)\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, _, metrics = model(x, y, return_metrics=do_metrics)\n",
        "                total = loss / cfg.grad_accum\n",
        "            optimizer.zero_grad(); total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % cfg.val_interval == 0 or step == cfg.max_steps - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(cfg.val_steps):\n",
        "                    vx, vy = loader.get_batch('val')\n",
        "                    vl, _, _ = model(vx, vy)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, metrics)\n",
        "            recorder.flush()\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|V:{val_loss:.3f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}\")\n",
        "\n",
        "    print(\"\\n‚úÖ BASELINE COMPLETE.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_baseline()"
      ],
      "metadata": {
        "id": "bMOub_nud_ex",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "HERO_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v2/telemetry_hero.parquet\")\n",
        "BASE_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_baseline/telemetry_baseline.parquet\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚öîÔ∏è JANUS DUEL: Hero vs. Baseline\")\n",
        "\n",
        "def load_data(path, label):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ùå Missing: {path}\")\n",
        "        return None\n",
        "    df = pd.read_parquet(path)\n",
        "    df['Model'] = label\n",
        "\n",
        "    # Calc averages if missing\n",
        "    if 'sigma_a_avg' not in df.columns:\n",
        "        l_cols = [c for c in df.columns if 'sigma_a' in c and 'L' in c]\n",
        "        if l_cols: df['sigma_a_avg'] = df[l_cols].mean(axis=1)\n",
        "\n",
        "    if 'eff_rank_avg' not in df.columns:\n",
        "        l_cols = [c for c in df.columns if 'eff_rank' in c and 'L' in c]\n",
        "        if l_cols: df['eff_rank_avg'] = df[l_cols].mean(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def run_comparison():\n",
        "    hero = load_data(HERO_PATH, \"Hero (Pressure)\")\n",
        "    base = load_data(BASE_PATH, \"Baseline (Control)\")\n",
        "\n",
        "    if hero is None or base is None: return\n",
        "\n",
        "    # Combine\n",
        "    combined = pd.concat([hero, base])\n",
        "\n",
        "    # PLOTTING\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "\n",
        "    # 1. Loss (The Score)\n",
        "    sns.lineplot(data=combined, x='step', y='val_loss', hue='Model', ax=axes[0], palette=['tab:green', 'tab:gray'])\n",
        "    axes[0].set_title(\"Validation Loss (Performance)\")\n",
        "    axes[0].set_ylabel(\"Cross Entropy\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Redundancy (The Laziness)\n",
        "    sns.lineplot(data=combined, x='step', y='sigma_a_avg', hue='Model', ax=axes[1], palette=['tab:green', 'tab:gray'])\n",
        "    axes[1].set_title(\"Head Redundancy (Sigma_A)\")\n",
        "    axes[1].set_ylabel(\"Correlation (Lower = Better)\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Efficiency (The Capacity Usage)\n",
        "    sns.lineplot(data=combined, x='step', y='eff_rank_avg', hue='Model', ax=axes[2], palette=['tab:green', 'tab:gray'])\n",
        "    axes[2].set_title(\"Effective Rank (Dimensional Usage)\")\n",
        "    axes[2].set_ylabel(\"Rank (Max 64)\")\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(REPORT_DIR, \"janus_duel_comparison.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"‚úÖ Comparison Chart Saved: {save_path}\")\n",
        "\n",
        "    # PRINT STATS\n",
        "    print(\"\\n--- FINAL STATS (Step 5000) ---\")\n",
        "    h_final = hero.iloc[-1]\n",
        "    b_final = base.iloc[-1]\n",
        "\n",
        "    print(f\"LOSS:       Hero {h_final['val_loss']:.4f} vs Base {b_final['val_loss']:.4f}\")\n",
        "    print(f\"REDUNDANCY: Hero {h_final['sigma_a_avg']:.4f} vs Base {b_final['sigma_a_avg']:.4f}\")\n",
        "    print(f\"RANK:       Hero {h_final['eff_rank_avg']:.2f}   vs Base {b_final['eff_rank_avg']:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_comparison()"
      ],
      "metadata": {
        "id": "YXbnxTsTBmBM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "HERO_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v2/telemetry_hero.parquet\")\n",
        "BASE_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_baseline/telemetry_baseline.parquet\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚öîÔ∏è JANUS DUEL: Hero vs. Baseline (Telemetry Analysis)\")\n",
        "\n",
        "def load_data(path, label):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ùå Missing: {path}\")\n",
        "        return None\n",
        "    df = pd.read_parquet(path)\n",
        "    df['Model'] = label\n",
        "\n",
        "    # Calc averages if missing\n",
        "    if 'sigma_a_avg' not in df.columns:\n",
        "        l_cols = [c for c in df.columns if 'sigma_a' in c and 'L' in c]\n",
        "        if l_cols: df['sigma_a_avg'] = df[l_cols].mean(axis=1)\n",
        "\n",
        "    if 'eff_rank_avg' not in df.columns:\n",
        "        l_cols = [c for c in df.columns if 'eff_rank' in c and 'L' in c]\n",
        "        if l_cols: df['eff_rank_avg'] = df[l_cols].mean(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def run_comparison():\n",
        "    hero = load_data(HERO_PATH, \"Hero (Pressure)\")\n",
        "    base = load_data(BASE_PATH, \"Baseline (Control)\")\n",
        "\n",
        "    if hero is None or base is None: return\n",
        "\n",
        "    # Combine\n",
        "    combined = pd.concat([hero, base])\n",
        "\n",
        "    # PLOTTING\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "\n",
        "    # 1. Loss (The Score)\n",
        "    sns.lineplot(data=combined, x='step', y='val_loss', hue='Model', ax=axes[0], palette=['tab:green', 'tab:gray'])\n",
        "    axes[0].set_title(\"Validation Loss (Performance)\")\n",
        "    axes[0].set_ylabel(\"Cross Entropy\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Redundancy (The Laziness)\n",
        "    sns.lineplot(data=combined, x='step', y='sigma_a_avg', hue='Model', ax=axes[1], palette=['tab:green', 'tab:gray'])\n",
        "    axes[1].set_title(\"Head Redundancy (Sigma_A)\")\n",
        "    axes[1].set_ylabel(\"Correlation (Lower = Better)\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Efficiency (The Capacity Usage)\n",
        "    sns.lineplot(data=combined, x='step', y='eff_rank_avg', hue='Model', ax=axes[2], palette=['tab:green', 'tab:gray'])\n",
        "    axes[2].set_title(\"Effective Rank (Dimensional Usage)\")\n",
        "    axes[2].set_ylabel(\"Rank (Max 8.0)\")\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(REPORT_DIR, \"janus_duel_comparison.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"‚úÖ Comparison Chart Saved: {save_path}\")\n",
        "\n",
        "    # PRINT STATS\n",
        "    print(\"\\n--- FINAL STATS (Step 5000) ---\")\n",
        "    h_final = hero.iloc[-1]\n",
        "    b_final = base.iloc[-1]\n",
        "\n",
        "    print(f\"LOSS:       Hero {h_final['val_loss']:.4f} vs Base {b_final['val_loss']:.4f}\")\n",
        "    print(f\"REDUNDANCY: Hero {h_final['sigma_a_avg']:.4f} vs Base {b_final['sigma_a_avg']:.4f}\")\n",
        "    print(f\"RANK:       Hero {h_final['eff_rank_avg']:.2f}   vs Base {b_final['eff_rank_avg']:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_comparison()"
      ],
      "metadata": {
        "id": "sHJK4vXpCY5t",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_prewarm\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚ùÑÔ∏è JANUS PRE-WARM (CHECKPOINT EDITION)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. ARCHITECTURE (Clean Room) ---\n",
        "class CleanConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 256 # Lite Mode\n",
        "        self.dropout = 0.0\n",
        "\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, l_div):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        if l_div > 0.0:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "    def forward(self, x, l_div):\n",
        "        a, s = self.attn(self.ln1(x), l_div)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, idx, targets=None, l_div=0.0):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "\n",
        "        total_steer = 0.0\n",
        "        for block in self.blocks:\n",
        "            x, s = block(x, l_div)\n",
        "            total_steer += s\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, total_steer\n",
        "\n",
        "# --- 3. LOADER ---\n",
        "class BinLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "    def get_noise_batch(self):\n",
        "        x = torch.randint(0, 50304, (self.batch_size, self.block_size)).to(DEVICE)\n",
        "        return x, None\n",
        "\n",
        "# --- 4. THE DUEL ---\n",
        "def run_sprint(mode, steps=500, save_name=\"unknown\"):\n",
        "    # CLEAN SLATE\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\nüèÉ RUNNING: {mode}\")\n",
        "    cfg = CleanConfig()\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "    loader = BinLoader(DATA_FILE, 256, 16)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    # --- PHASE A: PRE-WARMING ---\n",
        "    if mode == \"Pre-Warmed\":\n",
        "        print(\"   üî• Warming Up (Orthogonality on Noise)...\")\n",
        "        WARM_STEPS = 200\n",
        "        for i in tqdm(range(WARM_STEPS), desc=\"Warming\", leave=False):\n",
        "            x, _ = loader.get_noise_batch()\n",
        "            _, steer = model(x, targets=None, l_div=0.5)\n",
        "            optimizer.zero_grad(); steer.backward(); optimizer.step()\n",
        "\n",
        "    # --- PHASE B: TRAINING ---\n",
        "    print(\"   üöÄ Training on TinyStories...\")\n",
        "    model.train()\n",
        "    for step in tqdm(range(steps), desc=\"Training\"):\n",
        "        x, y = loader.get_batch()\n",
        "\n",
        "        # Zero pressure race\n",
        "        loss, _ = model(x, y, l_div=0.0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    # --- SAVE CHECKPOINT ---\n",
        "    save_path = os.path.join(MODEL_DIR, f\"{save_name}.pt\")\n",
        "    print(f\"   üíæ Saving Checkpoint to {save_path}...\")\n",
        "    torch.save({\n",
        "        'step': steps,\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'config': cfg.__dict__\n",
        "    }, save_path)\n",
        "\n",
        "    # CLEANUP\n",
        "    del model\n",
        "    del optimizer\n",
        "    gc.collect()\n",
        "    return loss_history\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "def main():\n",
        "    steps = 500\n",
        "\n",
        "    # Run Both\n",
        "    hist_control = run_sprint(\"Control\", steps, \"control_500\")\n",
        "    hist_warm = run_sprint(\"Pre-Warmed\", steps, \"prewarmed_500\")\n",
        "\n",
        "    print(\"\\nüìä Generating Report...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(hist_control, label='Control (Random Init)', alpha=0.7)\n",
        "    plt.plot(hist_warm, label='Pre-Warmed (Orthogonal Init)', linewidth=2)\n",
        "    plt.title(\"Geometric Pre-Warming: Loss Trajectory (500 Steps)\")\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(REPORT_DIR, \"prewarm_duel_500.png\")\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"‚úÖ Comparison saved to {save_path}\")\n",
        "\n",
        "    avg_c = np.mean(hist_control[-50:])\n",
        "    avg_w = np.mean(hist_warm[-50:])\n",
        "    print(f\"\\nüèÅ FINAL LOSS (Step {steps}):\")\n",
        "    print(f\"   Control:    {avg_c:.4f}\")\n",
        "    print(f\"   Pre-Warmed: {avg_w:.4f}\")\n",
        "\n",
        "    if avg_w < avg_c: print(\"üèÜ RESULT: Pre-Warming Improved Performance!\")\n",
        "    else: print(\"üìâ RESULT: No Improvement.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "cUOrHaflEKde",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP & INSTALL ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Install Stanza if missing\n",
        "try:\n",
        "    import stanza\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing Stanza...\")\n",
        "    !pip install stanza -q\n",
        "    import stanza\n",
        "\n",
        "# Download Stanza English model (lightweight)\n",
        "print(\"üì¶ Downloading NLP Models...\")\n",
        "stanza.download('en', processors='tokenize,pos,constituency', logging_level='WARN')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- 2. DATA PROCESSOR (The \"Professor\") ---\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "class CurriculumBuilder:\n",
        "    def __init__(self, bin_path, sample_tokens=500_000):\n",
        "        self.bin_path = bin_path\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.nlp = stanza.Pipeline('en', processors='tokenize,pos,constituency', use_gpu=True, logging_level='WARN')\n",
        "\n",
        "    def build(self):\n",
        "        print(f\"\\nüìö HARVESTING CURRICULUM ({self.sample_tokens} tokens)...\")\n",
        "        # 1. Load Raw Data\n",
        "        data = np.memmap(self.bin_path, dtype=np.uint16, mode='r')\n",
        "        # Grab a chunk from the middle to avoid header/intro bias\n",
        "        start_idx = len(data) // 2\n",
        "        chunk = data[start_idx : start_idx + self.sample_tokens].astype(np.int64)\n",
        "\n",
        "        # 2. Decode to Text\n",
        "        print(\"   -> Decoding text...\")\n",
        "        text_blob = tokenizer.decode(chunk)\n",
        "\n",
        "        # 3. Analyze with Stanza\n",
        "        print(\"   -> Parsing Syntax (This may take 2-3 mins)...\")\n",
        "        doc = self.nlp(text_blob[:200000]) # Limit char count to save time\n",
        "\n",
        "        simple = []\n",
        "        complex_sents = []\n",
        "\n",
        "        for sentence in doc.sentences:\n",
        "            text = sentence.text\n",
        "            # Heuristic via Constituency Parse\n",
        "            # S = Simple declarative clause\n",
        "            # SBAR = Clause introduced by subordinating conjunction\n",
        "            const_str = str(sentence.constituency)\n",
        "\n",
        "            if \"SBAR\" in const_str:\n",
        "                complex_sents.append(text)\n",
        "            elif const_str.count(\"(S \") == 1: # Single clause\n",
        "                simple.append(text)\n",
        "            else:\n",
        "                complex_sents.append(text) # Treat compound as complex for this test\n",
        "\n",
        "        print(f\"   -> Found {len(simple)} Simple | {len(complex_sents)} Complex sentences\")\n",
        "\n",
        "        # 4. Re-Tokenize\n",
        "        def tokenize_batch(sents):\n",
        "            ids = []\n",
        "            for s in sents:\n",
        "                ids.extend(tokenizer.encode(s) + [tokenizer.eos_token_id])\n",
        "            return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "        return tokenize_batch(simple), tokenize_batch(complex_sents)\n",
        "\n",
        "# --- 3. MODEL ARCHITECTURE (Clean Room) ---\n",
        "class CleanConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 256\n",
        "        self.dropout = 0.0\n",
        "\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, l_div):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        head_out = attn @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        if l_div > 0.0:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "    def forward(self, x, l_div):\n",
        "        a, s = self.attn(self.ln1(x), l_div)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, idx, targets=None, l_div=0.0):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "\n",
        "        total_steer = 0.0\n",
        "        for block in self.blocks:\n",
        "            x, s = block(x, l_div)\n",
        "            total_steer += s\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, total_steer\n",
        "\n",
        "# --- 4. LOADER ---\n",
        "class TensorLoader:\n",
        "    def __init__(self, tensor_data, block_size, batch_size):\n",
        "        self.data = tensor_data\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([self.data[i:i+self.block_size] for i in ix])\n",
        "        y = torch.stack([self.data[i+1:i+1+self.block_size] for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BinLoader: # For the full dataset\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# --- 5. THE DUEL ---\n",
        "def run_sprint(mode, simple_data, complex_data, steps=500):\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    print(f\"\\nüèÉ RUNNING: {mode}\")\n",
        "\n",
        "    cfg = CleanConfig()\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "\n",
        "    # Loaders\n",
        "    simple_loader = TensorLoader(simple_data, 256, 16)\n",
        "    complex_loader = TensorLoader(complex_data, 256, 16)\n",
        "    full_loader = BinLoader(DATA_FILE, 256, 16)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    # --- CURRICULUM PHASE ---\n",
        "    if mode == \"Curriculum\":\n",
        "        # Step 1: Simple (Force Subject/Verb distinction)\n",
        "        print(\"   üë∂ Phase 1: Simple Sentences (High Pressure)\")\n",
        "        for i in tqdm(range(100), leave=False):\n",
        "            x, y = simple_loader.get_batch()\n",
        "            # High Pressure to crystallize simple grammar\n",
        "            loss, steer = model(x, y, l_div=0.3)\n",
        "            optimizer.zero_grad(); (loss + steer).backward(); optimizer.step()\n",
        "\n",
        "        # Step 2: Complex (Force Clause distinction)\n",
        "        print(\"   üéì Phase 2: Complex Sentences (Med Pressure)\")\n",
        "        for i in tqdm(range(100), leave=False):\n",
        "            x, y = complex_loader.get_batch()\n",
        "            loss, steer = model(x, y, l_div=0.1)\n",
        "            optimizer.zero_grad(); (loss + steer).backward(); optimizer.step()\n",
        "\n",
        "    # --- MAIN PHASE ---\n",
        "    print(\"   üöÄ Phase 3: Full Dataset (Race Mode)\")\n",
        "    for step in tqdm(range(steps), desc=\"Training\"):\n",
        "        x, y = full_loader.get_batch()\n",
        "\n",
        "        # Zero pressure for the race to see natural performance\n",
        "        loss, _ = model(x, y, l_div=0.0)\n",
        "\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    del model; del optimizer\n",
        "    return loss_history\n",
        "\n",
        "# --- 6. EXECUTION ---\n",
        "def main():\n",
        "    # 1. Build Curriculum\n",
        "    builder = CurriculumBuilder(DATA_FILE)\n",
        "    simple_t, complex_t = builder.build()\n",
        "\n",
        "    # 2. Run Duel\n",
        "    steps = 500\n",
        "    hist_ctrl = run_sprint(\"Control\", simple_t, complex_t, steps) # Ignores curr data\n",
        "    hist_curr = run_sprint(\"Curriculum\", simple_t, complex_t, steps)\n",
        "\n",
        "    # 3. Report\n",
        "    print(\"\\nüìä Generating Report...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(hist_ctrl, label='Control (Random Init)', alpha=0.7)\n",
        "    plt.plot(hist_curr, label='Curriculum (Syntax Pre-Warm)', linewidth=2)\n",
        "    plt.title(\"Syntax-Aware Pre-Warming: Loss Trajectory\")\n",
        "    plt.xlabel(\"Training Steps (Post-Warmup)\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(REPORT_DIR, \"curriculum_duel.png\")\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "    avg_c = np.mean(hist_ctrl[-50:])\n",
        "    avg_curr = np.mean(hist_curr[-50:])\n",
        "    print(f\"\\nüèÅ FINAL LOSS (Step {steps}):\")\n",
        "    print(f\"   Control:    {avg_c:.4f}\")\n",
        "    print(f\"   Curriculum: {avg_curr:.4f}\")\n",
        "\n",
        "    if avg_curr < avg_c: print(\"üèÜ RESULT: Syntax Pre-Warming Worked!\")\n",
        "    else: print(\"üìâ RESULT: No Improvement.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xV48ur8qKNHi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# PATHS\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "CURRIC_DIR = os.path.join(PROJECT_ROOT, \"data/processed/curriculum_cache\")\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_curriculum\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "\n",
        "os.makedirs(CURRIC_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üéì JANUS CURRICULUM PROTOCOL\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. ARCHITECTURE (Clean Room) ---\n",
        "class CleanConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 256 # Lite Mode\n",
        "        self.dropout = 0.0\n",
        "\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, l_div, spatial_mult=1.0):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        head_out = attn @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        # Apply pressure if requested\n",
        "        if l_div > 0.0:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            # Apply spatial multiplier (Cubic schedule happens here via caller)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div * spatial_mult\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.layer_id = layer_id\n",
        "        self.total_layers = config.n_layers\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, l_div):\n",
        "        # Cubic Spatial Schedule Logic\n",
        "        # ratio = (layer + 1) / total\n",
        "        # mult = ratio^3\n",
        "        ratio = (self.layer_id + 1) / self.total_layers\n",
        "        spatial_mult = ratio ** 3\n",
        "\n",
        "        a, s = self.attn(self.ln1(x), l_div, spatial_mult)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config, i) for i in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, idx, targets=None, l_div=0.0):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "\n",
        "        total_steer = 0.0\n",
        "        for block in self.blocks:\n",
        "            x, s = block(x, l_div)\n",
        "            total_steer += s\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, total_steer\n",
        "\n",
        "# --- 3. LOADERS ---\n",
        "class TensorLoader:\n",
        "    def __init__(self, tensor_path, batch_size):\n",
        "        self.data = torch.load(tensor_path)\n",
        "        self.batch_size = batch_size\n",
        "        print(f\"üì¶ Loaded Tensor: {len(self.data):,} items from {os.path.basename(tensor_path)}\")\n",
        "\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data), (self.batch_size,))\n",
        "        # Data was saved as (N, SeqLen) tensors\n",
        "        chunk = torch.stack([self.data[i] for i in ix])\n",
        "        x = chunk[:, :-1].to(DEVICE)\n",
        "        y = chunk[:, 1:].to(DEVICE)\n",
        "        return x, y\n",
        "\n",
        "class BinLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# --- 4. STEP 1: CREATE CURRICULUM ---\n",
        "def step_1_create_curriculum():\n",
        "    simple_path = os.path.join(CURRIC_DIR, \"simple_curr.pt\")\n",
        "    complex_path = os.path.join(CURRIC_DIR, \"complex_curr.pt\")\n",
        "\n",
        "    if os.path.exists(simple_path) and os.path.exists(complex_path):\n",
        "        print(\"‚úÖ Curriculum files found. Skipping generation.\")\n",
        "        return simple_path, complex_path\n",
        "\n",
        "    print(\"üõ†Ô∏è Generating Curriculum (One-Time Cost)...\")\n",
        "    # Install Stanza only if needed\n",
        "    try: import stanza\n",
        "    except:\n",
        "        print(\"üì¶ Installing Stanza...\")\n",
        "        os.system('pip install stanza -q')\n",
        "        import stanza\n",
        "    stanza.download('en', processors='tokenize,pos,constituency', logging_level='WARN')\n",
        "\n",
        "    # Process\n",
        "    from transformers import GPT2Tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    nlp = stanza.Pipeline('en', processors='tokenize,pos,constituency', use_gpu=True, logging_level='WARN')\n",
        "\n",
        "    # Load raw data sample (Enough for 200 steps * 16 batch = 3200 sentences roughly)\n",
        "    data = np.memmap(DATA_FILE, dtype=np.uint16, mode='r')\n",
        "    start = len(data) // 2\n",
        "    # Sample 2M tokens to ensure we get enough valid sentences\n",
        "    chunk = data[start:start+2000000].astype(np.int64)\n",
        "    text_blob = tokenizer.decode(chunk)\n",
        "\n",
        "    # Parse\n",
        "    doc = nlp(text_blob[:500000]) # Limit chars for speed\n",
        "\n",
        "    simple_sents = []\n",
        "    complex_sents = []\n",
        "\n",
        "    print(\"   -> Analyzing Syntax...\")\n",
        "    for s in doc.sentences:\n",
        "        if len(simple_sents) > 2000 and len(complex_sents) > 2000: break\n",
        "\n",
        "        txt = s.text\n",
        "        if len(txt) < 10: continue\n",
        "\n",
        "        const = str(s.constituency)\n",
        "        if \"SBAR\" in const: complex_sents.append(txt)\n",
        "        elif const.count(\"(S \") == 1: simple_sents.append(txt)\n",
        "        else: complex_sents.append(txt)\n",
        "\n",
        "    print(f\"   -> Harvested {len(simple_sents)} Simple | {len(complex_sents)} Complex\")\n",
        "\n",
        "    # Save as Tensors\n",
        "    def save_batch(sents, path):\n",
        "        ids_list = []\n",
        "        for s in sents:\n",
        "            # Pad/Truncate to 257 (256 + 1 for X/Y)\n",
        "            ids = tokenizer.encode(s) + [tokenizer.eos_token_id]\n",
        "            if len(ids) < 257: ids = ids + [tokenizer.eos_token_id] * (257 - len(ids))\n",
        "            ids = ids[:257]\n",
        "            ids_list.append(torch.tensor(ids, dtype=torch.long))\n",
        "        torch.save(ids_list, path)\n",
        "\n",
        "    save_batch(simple_sents, simple_path)\n",
        "    save_batch(complex_sents, complex_path)\n",
        "    print(\"‚úÖ Curriculum Saved to Disk.\")\n",
        "    return simple_path, complex_path\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "def main():\n",
        "    # STEP 1\n",
        "    simp_path, comp_path = step_1_create_curriculum()\n",
        "\n",
        "    # STEP 2: TRAIN BASELINE\n",
        "    print(\"\\nüìâ STEP 2: Baseline Run (500 Steps)...\")\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = CleanConfig()\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "    loader = BinLoader(DATA_FILE, 256, 16)\n",
        "\n",
        "    base_loss = []\n",
        "    for step in tqdm(range(500), desc=\"Baseline\"):\n",
        "        x, y = loader.get_batch()\n",
        "        loss, _ = model(x, y, l_div=0.0) # No Pressure\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        base_loss.append(loss.item())\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"janus_baseline.pt\"))\n",
        "    print(\"üíæ Baseline Saved.\")\n",
        "    del model; del optimizer\n",
        "\n",
        "    # STEP 3 & 4: JANUS WARMUP + RUN\n",
        "    print(\"\\nüî• STEP 3: Janus Warm-Up (200 Steps)...\")\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    model = CleanGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "\n",
        "    s_loader = TensorLoader(simp_path, 16)\n",
        "    c_loader = TensorLoader(comp_path, 16)\n",
        "\n",
        "    # 100 Simple (Pressure 0.27)\n",
        "    for i in tqdm(range(100), desc=\"Simple Warmup\"):\n",
        "        x, y = s_loader.get_batch()\n",
        "        loss, steer = model(x, y, l_div=0.27)\n",
        "        optimizer.zero_grad(); (loss + steer).backward(); optimizer.step()\n",
        "\n",
        "    # 100 Complex (Pressure 0.27)\n",
        "    for i in tqdm(range(100), desc=\"Complex Warmup\"):\n",
        "        x, y = c_loader.get_batch()\n",
        "        loss, steer = model(x, y, l_div=0.27)\n",
        "        optimizer.zero_grad(); (loss + steer).backward(); optimizer.step()\n",
        "\n",
        "    print(\"\\nüöÄ STEP 4: Janus Training (500 Steps - No Pressure)...\")\n",
        "    janus_loss = []\n",
        "    for step in tqdm(range(500), desc=\"Race Mode\"):\n",
        "        x, y = loader.get_batch()\n",
        "        loss, _ = model(x, y, l_div=0.0) # Pressure OFF\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        janus_loss.append(loss.item())\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"JanusWUCuric.pt\"))\n",
        "    print(\"üíæ JanusWUCuric Saved.\")\n",
        "\n",
        "    # STEP 5: COMPARE\n",
        "    print(\"\\nüìä STEP 5: Generating Report...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(base_loss, label='Baseline (Random Init)', alpha=0.7)\n",
        "    plt.plot(janus_loss, label='Janus (Curriculum Pre-Warm)', linewidth=2)\n",
        "    plt.title(\"Curriculum Pre-Warming: Loss Trajectory (Race Phase)\")\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.savefig(os.path.join(REPORT_DIR, \"curriculum_final_duel.png\"))\n",
        "\n",
        "    b_avg = np.mean(base_loss[-50:])\n",
        "    j_avg = np.mean(janus_loss[-50:])\n",
        "    print(f\"\\nüèÅ FINAL STATS (Step 500):\")\n",
        "    print(f\"   Baseline: {b_avg:.4f}\")\n",
        "    print(f\"   Janus:    {j_avg:.4f}\")\n",
        "\n",
        "    if j_avg < b_avg: print(\"üèÜ RESULT: Pre-Warming Success!\")\n",
        "    else: print(\"üìâ RESULT: Baseline Won.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lddxyyRsMeFx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_curriculum\")\n",
        "BASELINE_PATH = os.path.join(MODEL_DIR, \"janus_baseline.pt\")\n",
        "JANUS_PATH = os.path.join(MODEL_DIR, \"JanusWUCuric.pt\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üó£Ô∏è CURRICULUM INFERENCE DUEL\")\n",
        "\n",
        "# --- 2. ARCHITECTURE ---\n",
        "class CleanConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 256\n",
        "        self.dropout = 0.0\n",
        "\n",
        "class CleanAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out)\n",
        "\n",
        "class CleanBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CleanAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class CleanGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([CleanBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -256:] # Match lite mode seq len\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- 3. LOAD MODELS ---\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "cfg = CleanConfig()\n",
        "\n",
        "# Load Baseline\n",
        "model_base = CleanGPT(cfg).to(DEVICE)\n",
        "try:\n",
        "    model_base.load_state_dict(torch.load(BASELINE_PATH, map_location=DEVICE))\n",
        "    print(\"‚úÖ Baseline Loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Baseline Load Fail: {e}\")\n",
        "\n",
        "# Load Janus\n",
        "model_janus = CleanGPT(cfg).to(DEVICE)\n",
        "try:\n",
        "    model_janus.load_state_dict(torch.load(JANUS_PATH, map_location=DEVICE))\n",
        "    print(\"‚úÖ Janus (Curriculum) Loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Janus Load Fail: {e}\")\n",
        "\n",
        "# --- 4. THE PROBE ---\n",
        "prompts = [\"Lily wanted a\", \"The big red ball\", \"Once upon a time\"]\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\nüìù PROMPT: {p}\")\n",
        "    input_ids = tokenizer.encode(p, return_tensors='pt').to(DEVICE)\n",
        "\n",
        "    # Baseline Output\n",
        "    out_b = model_base.generate(input_ids, max_new_tokens=80, temperature=0.6)\n",
        "    text_b = tokenizer.decode(out_b[0], skip_special_tokens=True)\n",
        "    print(f\"üìâ BASELINE: {text_b}\")\n",
        "\n",
        "    # Janus Output\n",
        "    out_j = model_janus.generate(input_ids, max_new_tokens=80, temperature=0.6)\n",
        "    text_j = tokenizer.decode(out_j[0], skip_special_tokens=True)\n",
        "    print(f\"üöÄ JANUS:    {text_j}\")"
      ],
      "metadata": {
        "id": "HiOdWDdAKNPs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üèéÔ∏è JANUS ARCH DUEL (FIXED): Stock vs. Spec\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIG ---\n",
        "class DuelConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 256\n",
        "        self.dropout = 0.0\n",
        "\n",
        "# --- 3. OLD JANUS (Stock GPT-2) ---\n",
        "class OldAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.c_attn = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        # Calculate QKV\n",
        "        qkv = self.c_attn(x).view(B, S, 3, self.n_heads, self.d_head).permute(2, 0, 1, 3, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # FIX: Transpose to (B, H, S, D) for proper attention\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.c_proj(y)\n",
        "\n",
        "class OldBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = OldAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class OldGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([OldBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # WEIGHT TYING (Crucial for Param Efficiency)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss\n",
        "\n",
        "# --- 4. NEW JANUS (Janus Spec) ---\n",
        "# RMSNorm, RoPE, SwiGLU, Bias=False, Tied Weights\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        # Bias = False\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        x_out = torch.view_as_real(x_c * freqs_cis[:x.shape[1]]).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        # Apply RoPE (No Transpose needed yet, operates on S dim)\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "\n",
        "        # Now Transpose\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(y)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        # Adjusted ratio 8/3 to keep parameter count roughly equal to standard MLP\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # WEIGHT TYING\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss\n",
        "\n",
        "# --- 5. LOADER ---\n",
        "class BinLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# --- 6. DUEL ---\n",
        "def run_sprint(model_class, label, steps=500):\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    print(f\"\\nüèÉ RUNNING: {label}\")\n",
        "\n",
        "    cfg = DuelConfig()\n",
        "    model = model_class(cfg).to(DEVICE)\n",
        "    params = sum(p.numel() for p in model.parameters())/1e6\n",
        "    print(f\"   Params: {params:.2f}M\")\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "    loader = BinLoader(DATA_FILE, 256, 16)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    model.train()\n",
        "    for step in tqdm(range(steps), desc=label):\n",
        "        x, y = loader.get_batch()\n",
        "        loss = model(x, y)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    del model; del optimizer\n",
        "    return loss_history\n",
        "\n",
        "def main():\n",
        "    steps = 500\n",
        "\n",
        "    hist_old = run_sprint(OldGPT, \"Old Janus (Stock)\", steps)\n",
        "    hist_new = run_sprint(NewGPT, \"New Janus (Spec)\", steps)\n",
        "\n",
        "    print(\"\\nüìä Generating Report...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(hist_old, label='Old Janus (GPT-2)', alpha=0.7)\n",
        "    plt.plot(hist_new, label='New Janus (RoPE/RMS/SwiGLU)', linewidth=2)\n",
        "    plt.title(\"Architecture Duel: Loss Trajectory (500 Steps)\")\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(REPORT_DIR, \"arch_duel_fixed.png\")\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "    avg_o = np.mean(hist_old[-50:])\n",
        "    avg_n = np.mean(hist_new[-50:])\n",
        "    print(f\"\\nüèÅ FINAL LOSS (Step {steps}):\")\n",
        "    print(f\"   Old Janus: {avg_o:.4f}\")\n",
        "    print(f\"   New Janus: {avg_n:.4f}\")\n",
        "\n",
        "    if avg_n < avg_o: print(\"üèÜ RESULT: New Spec Wins!\")\n",
        "    else: print(\"üìâ RESULT: Old School Wins.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UtwfY2evR4RY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the RoPE test script"
      ],
      "metadata": {
        "id": "z5mZI4B7l_GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üèéÔ∏è JANUS ARCH DUEL (FIXED): Stock vs. Spec\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIG ---\n",
        "class DuelConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 256\n",
        "        self.dropout = 0.0\n",
        "\n",
        "# --- 3. OLD JANUS (Stock GPT-2) ---\n",
        "class OldAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.c_attn = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        # Calculate QKV\n",
        "        qkv = self.c_attn(x).view(B, S, 3, self.n_heads, self.d_head).permute(2, 0, 1, 3, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # FIX: Transpose to (B, H, S, D) for proper attention\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.c_proj(y)\n",
        "\n",
        "class OldBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = OldAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class OldGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.blocks = nn.ModuleList([OldBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # WEIGHT TYING (Crucial for Param Efficiency)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx) + self.pos_emb(torch.arange(S, device=idx.device))\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss\n",
        "\n",
        "# --- 4. NEW JANUS (Janus Spec) ---\n",
        "# RMSNorm, RoPE, SwiGLU, Bias=False, Tied Weights\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        # Bias = False\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        # x shape: [B, S, n_heads, d_head]\n",
        "        # freqs_cis shape: [max_seq_len, d_head//2]\n",
        "        # We need to slice freqs_cis to match sequence length and unsqueeze for heads\n",
        "        B, S, H, D = x.shape\n",
        "        freqs_cis = freqs_cis[:S].unsqueeze(1)  # [S, 1, d_head//2]\n",
        "\n",
        "        x_c = torch.view_as_complex(x.float().reshape(B, S, H, -1, 2))  # [B, S, H, d_head//2]\n",
        "        x_out = torch.view_as_real(x_c * freqs_cis).flatten(3)  # [B, S, H, d_head]\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        # Apply RoPE\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "\n",
        "        # Transpose to [B, H, S, D]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(y)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        # Adjusted ratio 8/3 to keep parameter count roughly equal to standard MLP\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # WEIGHT TYING\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss\n",
        "\n",
        "# --- 5. LOADER ---\n",
        "class BinLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# --- 6. DUEL ---\n",
        "def run_sprint(model_class, label, steps=500):\n",
        "    # Full cleanup between models\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    print(f\"\\nüèÉ RUNNING: {label}\")\n",
        "\n",
        "    cfg = DuelConfig()\n",
        "    model = model_class(cfg).to(DEVICE)\n",
        "    params = sum(p.numel() for p in model.parameters())/1e6\n",
        "    print(f\"   Params: {params:.2f}M\")\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "    loader = BinLoader(DATA_FILE, 256, 16)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    model.train()\n",
        "    for step in tqdm(range(steps), desc=label):\n",
        "        x, y = loader.get_batch()\n",
        "        loss = model(x, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    # Cleanup\n",
        "    del model\n",
        "    del optimizer\n",
        "    del loader\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "def main():\n",
        "    steps = 500\n",
        "\n",
        "    hist_old = run_sprint(OldGPT, \"Old Janus (Stock)\", steps)\n",
        "    hist_new = run_sprint(NewGPT, \"New Janus (Spec)\", steps)\n",
        "\n",
        "    print(\"\\nüìä Generating Report...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(hist_old, label='Old Janus (GPT-2)', alpha=0.7)\n",
        "    plt.plot(hist_new, label='New Janus (RoPE/RMS/SwiGLU)', linewidth=2)\n",
        "    plt.title(\"Architecture Duel: Loss Trajectory (500 Steps)\")\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(REPORT_DIR, \"arch_duel_fixed.png\")\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"üíæ Report saved: {save_path}\")\n",
        "\n",
        "    avg_o = np.mean(hist_old[-50:])\n",
        "    avg_n = np.mean(hist_new[-50:])\n",
        "    print(f\"\\nüèÅ FINAL LOSS (Last 50 Steps Average):\")\n",
        "    print(f\"   Old Janus: {avg_o:.4f}\")\n",
        "    print(f\"   New Janus: {avg_n:.4f}\")\n",
        "\n",
        "    if avg_n < avg_o:\n",
        "        print(\"üèÜ RESULT: New Spec Wins!\")\n",
        "    else:\n",
        "        print(\"üìâ RESULT: Old School Wins.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "vaZJ1nryKNfy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_v3\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ JANUS v3 PREVIEW: The Synthesis\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "class V3Config:\n",
        "    def __init__(self):\n",
        "        # Architecture\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 256\n",
        "        self.dropout = 0.0\n",
        "\n",
        "        # Training\n",
        "        self.max_steps = 1000\n",
        "        self.batch_size = 32 # Higher batch size since model is efficient? Let's stick to safe 32.\n",
        "        self.grad_accum = 1\n",
        "\n",
        "        # Scheduler\n",
        "        self.warmup_steps = 200\n",
        "        self.max_pressure = 0.15\n",
        "        self.spatial_schedule = 'cubic'\n",
        "\n",
        "# --- 3. ARCHITECTURE (The Winner) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        # RoPE\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "\n",
        "        # Transpose\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        head_out = attn @ v\n",
        "\n",
        "        # VSM Pressure\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        if l_div > 0.0:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, head_out # Return head_out for telemetry\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.layer_id = layer_id\n",
        "        self.n_layers = config.n_layers\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "\n",
        "    def forward(self, x, lambdas):\n",
        "        # Spatial Schedule Injection\n",
        "        # We can modify lambdas here based on layer_id if needed,\n",
        "        # but Scheduler usually handles the calculation.\n",
        "        # Let's assume Scheduler returns the raw base values, and we scale here?\n",
        "        # Actually, let's keep it simple: Scheduler passes (coh, div) specific to this layer.\n",
        "\n",
        "        a, s, heads = self.attn(self.ln1(x), lambdas)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, heads\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config, i) for i in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight # Tied Weights\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "\n",
        "        total_steer = 0.0\n",
        "        all_heads = []\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, h = block(x, lambdas_list[i])\n",
        "            total_steer += s\n",
        "            all_heads.append(h)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, total_steer, all_heads\n",
        "\n",
        "# --- 4. SCHEDULER (Trapezoidal with Delay) ---\n",
        "class DelayedTrapezoidScheduler:\n",
        "    def __init__(self, config):\n",
        "        self.warmup = config.warmup_steps\n",
        "        self.total = config.max_steps\n",
        "        self.max_p = config.max_pressure\n",
        "        self.n_layers = config.n_layers\n",
        "\n",
        "        # Calculate trapezoid phases for the ACTIVE region\n",
        "        self.active_steps = self.total - self.warmup\n",
        "        self.ramp_steps = int(self.active_steps * 0.25)\n",
        "        self.decay_steps = int(self.active_steps * 0.25)\n",
        "        self.hold_steps = self.active_steps - self.ramp_steps - self.decay_steps\n",
        "\n",
        "    def get_pressure(self, step):\n",
        "        if step < self.warmup:\n",
        "            return 0.0\n",
        "\n",
        "        t = step - self.warmup\n",
        "\n",
        "        if t < self.ramp_steps:\n",
        "            return self.max_p * (t / self.ramp_steps)\n",
        "        elif t < (self.ramp_steps + self.hold_steps):\n",
        "            return self.max_p\n",
        "        else:\n",
        "            # Decay phase\n",
        "            remaining = self.active_steps - t\n",
        "            return self.max_p * (remaining / self.decay_steps)\n",
        "\n",
        "    def get_layer_lambdas(self, step):\n",
        "        base_div = self.get_pressure(step)\n",
        "        base_coh = base_div * 0.2 # Standard ratio\n",
        "\n",
        "        lambdas = []\n",
        "        for i in range(self.n_layers):\n",
        "            # Cubic Spatial Schedule\n",
        "            ratio = (i + 1) / self.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "\n",
        "            lambdas.append((base_coh * s_mult, base_div * s_mult))\n",
        "        return lambdas, base_div\n",
        "\n",
        "# --- 5. LOADER ---\n",
        "class BinLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "    def get_batch(self):\n",
        "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def run_v3():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = V3Config()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n",
        "    scheduler = DelayedTrapezoidScheduler(cfg)\n",
        "    loader = BinLoader(DATA_FILE, cfg.max_seq_len, cfg.batch_size)\n",
        "\n",
        "    history = []\n",
        "\n",
        "    model.train()\n",
        "    pbar = tqdm(range(cfg.max_steps), desc=\"Janus v3\")\n",
        "\n",
        "    for step in pbar:\n",
        "        x, y = loader.get_batch()\n",
        "\n",
        "        # Get Lambdas\n",
        "        layer_lambdas, current_p = scheduler.get_layer_lambdas(step)\n",
        "\n",
        "        loss, steer, heads = model(x, layer_lambdas, y)\n",
        "        total = loss + steer\n",
        "\n",
        "        optimizer.zero_grad(); total.backward(); optimizer.step()\n",
        "\n",
        "        # Quick Telemetry (Layer 11 Redundancy)\n",
        "        if step % 20 == 0:\n",
        "            with torch.no_grad():\n",
        "                h = heads[-1] # Top layer\n",
        "                flat = h.transpose(0, 1).reshape(8, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                gram = torch.mm(norm, norm.t())\n",
        "                mask = ~torch.eye(8, dtype=torch.bool, device=DEVICE)\n",
        "                red = (gram.abs() * mask.float()).sum().item() / 56 # 8*7\n",
        "\n",
        "            history.append({\n",
        "                'step': step,\n",
        "                'loss': loss.item(),\n",
        "                'pressure': current_p,\n",
        "                'red_L11': red\n",
        "            })\n",
        "            pbar.set_description(f\"L:{loss.item():.3f} | P:{current_p:.3f} | R:{red:.3f}\")\n",
        "\n",
        "    # Save Report\n",
        "    df = pd.DataFrame(history)\n",
        "    df.to_csv(os.path.join(REPORT_DIR, \"janus_v3_preview.csv\"), index=False)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "    ax1.set_xlabel('Step')\n",
        "    ax1.set_ylabel('Loss', color='tab:red')\n",
        "    ax1.plot(df['step'], df['loss'], color='tab:red', alpha=0.6, label='Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel('Pressure', color='tab:blue')\n",
        "    ax2.plot(df['step'], df['pressure'], color='tab:blue', linestyle='--', label='Pressure')\n",
        "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "    plt.title(\"Janus v3 Preview: The Synthesis\")\n",
        "    plt.savefig(os.path.join(REPORT_DIR, \"janus_v3_chart.png\"))\n",
        "\n",
        "    print(f\"\\nüèÅ FINAL LOSS: {df.iloc[-1]['loss']:.4f}\")\n",
        "    print(f\"üèÅ FINAL REDUNDANCY (L11): {df.iloc[-1]['red_L11']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_v3()"
      ],
      "metadata": {
        "id": "Pa0P26BXpWSS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY CLEANSE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_FILE = os.path.join(PROJECT_ROOT, \"data/processed/TinyStories-train_full.bin\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v3\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ JANUS HERO v3 (FIXED): 512 Context / Batch 8 / Accum 8\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class V3Config:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        self.max_steps = 5000\n",
        "        self.max_pressure = 0.15\n",
        "\n",
        "        self.batch_size = 8\n",
        "        self.grad_accum = 8  # Effective Batch = 64\n",
        "        self.val_interval = 250\n",
        "        self.val_steps = 20\n",
        "\n",
        "# --- 4. ARCHITECTURE (Janus Spec) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        metrics = {}\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        if l_div > 0.0 and self.training:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        # Scale residuals to depth\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 5. SCHEDULER & LOADERS ---\n",
        "class FlightController:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    def get_pressure(self, step):\n",
        "        if step < 1250: return 0.0\n",
        "        elif step < 2500: return self.config.max_pressure * ((step - 1250) / 1250)\n",
        "        elif step < 4000: return self.config.max_pressure\n",
        "        else: return self.config.max_pressure * ((5000 - step) / 1000)\n",
        "    def get_lambdas(self, step):\n",
        "        p = self.get_pressure(step)\n",
        "        base_coh = p * 0.2\n",
        "        lambdas = []\n",
        "        for i in range(self.config.n_layers):\n",
        "            ratio = (i + 1) / self.config.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "            lambdas.append((base_coh * s_mult, p * s_mult))\n",
        "        return lambdas, p\n",
        "\n",
        "class BinLoader:\n",
        "    def __init__(self, bin_path, block_size, batch_size):\n",
        "        self.data = np.memmap(bin_path, dtype=np.uint16, mode='r')\n",
        "        total_tokens = len(self.data)\n",
        "        split = int(total_tokens * 0.95)\n",
        "        self.train_data = self.data[:split]\n",
        "        self.val_data = self.data[split:]\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        print(f\"üì¶ Data Split | Train: {len(self.train_data):,} | Val: {len(self.val_data):,}\")\n",
        "    def get_batch(self, split='train'):\n",
        "        d = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(d) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(d[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(d[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure\n",
        "        }\n",
        "        for i, m in enumerate(metrics):\n",
        "            if not m: continue\n",
        "            row[f\"L{i}_sigma_a\"] = m['sigma_a'].mean().item()\n",
        "            row[f\"L{i}_eff_rank\"] = m['eff_rank'].mean().item()\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_v3.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def run_v3():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = V3Config()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4, weight_decay=0.1)\n",
        "    scheduler = FlightController(cfg)\n",
        "    loader = BinLoader(DATA_FILE, cfg.max_seq_len, cfg.batch_size)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    ckpt_path = os.path.join(SAVE_DIR, \"ckpt_latest.pt\")\n",
        "\n",
        "    # ‚ö†Ô∏è SAFETY: Check if we should resume or restart\n",
        "    # If the last run exploded, we MUST restart.\n",
        "    if os.path.exists(ckpt_path):\n",
        "        try:\n",
        "            c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "            # Sanity check loss\n",
        "            # If we can't check loss, we assume it's valid, but user implies last run was bad.\n",
        "            # Let's force restart if user wants (comment out below to resume)\n",
        "            print(\"‚ö†Ô∏è CHECKPOINT FOUND. DELETING TO START FRESH (PER USER REQUEST).\")\n",
        "            os.remove(ckpt_path)\n",
        "            start_step = 0\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è Corrupt checkpoint found. Starting fresh.\")\n",
        "            start_step = 0\n",
        "\n",
        "    print(f\"\\nüèÉ STARTING RUN: {start_step} -> {cfg.max_steps}\")\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    # [FIXED] ACCUMULATION LOGIC\n",
        "    for step in pbar:\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        lambdas, pressure = scheduler.get_lambdas(step)\n",
        "\n",
        "        optimizer.zero_grad() # <--- MOVED OUTSIDE LOOP (CORRECT)\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch('train')\n",
        "            do_metrics = (_ == cfg.grad_accum - 1) and (step % cfg.val_interval == 0 or step == cfg.max_steps - 1)\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas, y, return_metrics=do_metrics)\n",
        "                total = (loss + steer) / cfg.grad_accum # Scale loss\n",
        "\n",
        "            total.backward() # Accumulate gradients\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # LOGGING\n",
        "        if step % cfg.val_interval == 0 or step == cfg.max_steps - 1:\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(cfg.val_steps):\n",
        "                    vx, vy = loader.get_batch('val')\n",
        "                    vl, _, _ = model(vx, [(0.0,0.0)]*cfg.n_layers, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, pressure, metrics)\n",
        "            recorder.flush()\n",
        "            torch.save({'step': step, 'model': model.state_dict(), 'optim': optimizer.state_dict()}, ckpt_path)\n",
        "\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|V:{val_loss:.3f}|P:{pressure:.3f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|P:{pressure:.3f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"janus_v3_final.pt\"))\n",
        "    print(\"\\nüèÜ MISSION COMPLETE.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_v3()"
      ],
      "metadata": {
        "id": "nPCo7ZCVuVu8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Project_XAI_Physical_Janus/data/models/janus_hero_v3\"\n",
        "\n",
        "print(f\"üïµÔ∏è JANUS BLACK BOX: Inspecting {MODEL_DIR}...\\n\")\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    print(\"‚ùå Directory not found.\")\n",
        "else:\n",
        "    files = [f for f in os.listdir(MODEL_DIR) if f.endswith(\".pt\")]\n",
        "\n",
        "    if not files:\n",
        "        print(\"‚ö†Ô∏è No .pt model files found.\")\n",
        "    else:\n",
        "        for f in files:\n",
        "            path = os.path.join(MODEL_DIR, f)\n",
        "            print(f\"üìÑ Found: {f}\")\n",
        "            try:\n",
        "                # Load metadata only (map to CPU to save memory)\n",
        "                checkpoint = torch.load(path, map_location='cpu')\n",
        "\n",
        "                # Check for 'step' key\n",
        "                if isinstance(checkpoint, dict) and 'step' in checkpoint:\n",
        "                    step = checkpoint['step']\n",
        "                    print(f\"   ‚úÖ STATUS: Recoverable\")\n",
        "                    print(f\"   üî¢ STEP COUNT: {step}\")\n",
        "\n",
        "                    # Check Logic\n",
        "                    if step > 0:\n",
        "                        print(\"   üöÄ VERDICT: You can resume from this!\")\n",
        "                    else:\n",
        "                        print(\"   ‚ö†Ô∏è VERDICT: This is a fresh/empty init.\")\n",
        "\n",
        "                elif isinstance(checkpoint, dict):\n",
        "                    print(\"   ‚ö†Ô∏è STATUS: Dict found, but no 'step' key. (Likely a final weight dump, not a checkpoint)\")\n",
        "                else:\n",
        "                    print(\"   ‚ö†Ô∏è STATUS: Raw model weights (No metadata).\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå ERROR: File corrupted or unreadable. ({e})\")\n",
        "            print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "eluW0cmTXbw_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "MODEL_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v3/janus_v3_final.pt\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üîÆ JANUS v3: THE ORACLE\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "class V3Config:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50304\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.0\n",
        "\n",
        "# --- 3. ARCHITECTURE (Must Match Training) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(y)\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -512:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- 4. EXECUTION ---\n",
        "try:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "except:\n",
        "    print(\"‚ùå Installing transformers...\")\n",
        "    os.system('pip install transformers')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "cfg = V3Config()\n",
        "model = NewGPT(cfg).to(DEVICE)\n",
        "\n",
        "print(f\"üîÑ Loading Weights from {MODEL_PATH}...\")\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    try:\n",
        "        # Load weights\n",
        "        state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"‚úÖ Weights Loaded Successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Load Failed: {e}\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    print(\"‚ùå Model file missing.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# --- 5. THE PROBE ---\n",
        "prompts = [\n",
        "    \"Once upon a time\",\n",
        "    \"Lily wanted a\",\n",
        "    \"The big red ball\",\n",
        "    \"Tom went to the\",\n",
        "    \"One day, a little\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"üß™ JANUS v3 (New Arch) OUTPUTS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for p in prompts:\n",
        "    input_ids = tokenizer.encode(p, return_tensors='pt').to(DEVICE)\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=100, temperature=0.6, top_k=40)\n",
        "    text = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
        "    print(f\"\\nüìù PROMPT: {p}\")\n",
        "    print(f\"ü§ñ JANUS: {text}\")"
      ],
      "metadata": {
        "id": "SyJJUlh6YA1A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "V2_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v2/telemetry_hero.parquet\")\n",
        "V3_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_hero_v3/telemetry_v3.parquet\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚öñÔ∏è JANUS TITLE FIGHT: v2 vs. v3\")\n",
        "\n",
        "def load_clean_data(path, label):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ùå Missing: {path}\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_parquet(path)\n",
        "\n",
        "    # Clean restarts (if step count went backwards, keep the latest run)\n",
        "    # Simple heuristic: sort by step, drop duplicates keeping last\n",
        "    df = df.sort_values('step')\n",
        "    df = df.drop_duplicates(subset='step', keep='last')\n",
        "\n",
        "    df['Model'] = label\n",
        "\n",
        "    # Calculate Averages if missing\n",
        "    if 'sigma_a_avg' not in df.columns:\n",
        "        l_cols = [c for c in df.columns if 'sigma_a' in c and 'L' in c]\n",
        "        if l_cols: df['sigma_a_avg'] = df[l_cols].mean(axis=1)\n",
        "        else: df['sigma_a_avg'] = 0.0\n",
        "\n",
        "    if 'eff_rank_avg' not in df.columns:\n",
        "        l_cols = [c for c in df.columns if 'eff_rank' in c and 'L' in c]\n",
        "        if l_cols: df['eff_rank_avg'] = df[l_cols].mean(axis=1)\n",
        "        else: df['eff_rank_avg'] = 0.0\n",
        "\n",
        "    return df\n",
        "\n",
        "def run_analysis():\n",
        "    v2 = load_clean_data(V2_PATH, \"v2 (Old Arch)\")\n",
        "    v3 = load_clean_data(V3_PATH, \"v3 (Spec Arch)\")\n",
        "\n",
        "    if v2 is None or v3 is None: return\n",
        "\n",
        "    # Combine\n",
        "    combined = pd.concat([v2, v3])\n",
        "\n",
        "    # PLOTTING\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "    palette = {'v2 (Old Arch)': 'gray', 'v3 (Spec Arch)': 'tab:red'}\n",
        "\n",
        "    # 1. Validation Loss\n",
        "    sns.lineplot(data=combined, x='step', y='val_loss', hue='Model', ax=axes[0], palette=palette, linewidth=2)\n",
        "    axes[0].set_title(\"Validation Loss (Intelligence)\")\n",
        "    axes[0].set_ylabel(\"Cross Entropy\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Redundancy (Sigma A)\n",
        "    sns.lineplot(data=combined, x='step', y='sigma_a_avg', hue='Model', ax=axes[1], palette=palette, linewidth=2)\n",
        "    axes[1].set_title(\"Head Redundancy (Uniqueness)\")\n",
        "    axes[1].set_ylabel(\"Correlation (Lower is Better)\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].set_ylim(0, 0.1) # Zoom in on the low end\n",
        "\n",
        "    # 3. Pressure Schedule\n",
        "    sns.lineplot(data=combined, x='step', y='pressure', hue='Model', ax=axes[2], palette=palette, linestyle='--')\n",
        "    axes[2].set_title(\"Pressure Schedule (Force Applied)\")\n",
        "    axes[2].set_ylabel(\"Lambda Div\")\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(REPORT_DIR, \"janus_v2_vs_v3.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"\\nüìä Chart Saved: {save_path}\")\n",
        "\n",
        "    # STATS CARD\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üèÜ FINAL SCORECARD (Step 5000)\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    def get_final(df):\n",
        "        return df.iloc[-1]\n",
        "\n",
        "    f2 = get_final(v2)\n",
        "    f3 = get_final(v3)\n",
        "\n",
        "    # Percent Improvement\n",
        "    loss_imp = ((f2['val_loss'] - f3['val_loss']) / f2['val_loss']) * 100\n",
        "\n",
        "    print(f\"LOSS (Lower Wins):\")\n",
        "    print(f\"   v2: {f2['val_loss']:.4f}\")\n",
        "    print(f\"   v3: {f3['val_loss']:.4f}  (Improvement: +{loss_imp:.1f}%)\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"REDUNDANCY (Target ~0.003):\")\n",
        "    print(f\"   v2: {f2['sigma_a_avg']:.4f}\")\n",
        "    print(f\"   v3: {f3['sigma_a_avg']:.4f}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"PRESSURE REQUIRED:\")\n",
        "    print(f\"   v2: {f2['pressure']:.2f}\")\n",
        "    print(f\"   v3: {f3['pressure']:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_analysis()"
      ],
      "metadata": {
        "id": "wNepmggEeTQm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìö WIKITEXT-103 PREP (AUTHENTICATED & SANITIZED)\")\n",
        "\n",
        "# --- 2. AUTHENTICATION ---\n",
        "TOKEN_FILE = \"/content/hf_token.txt\"\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import login\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing huggingface_hub...\")\n",
        "    os.system('pip install huggingface_hub')\n",
        "    from huggingface_hub import login\n",
        "\n",
        "if os.path.exists(TOKEN_FILE):\n",
        "    with open(TOKEN_FILE, 'r', encoding='utf-8') as f:\n",
        "        # üü¢ CRITICAL FIX: Strip BOM and whitespace\n",
        "        token = f.read().replace('\\ufeff', '').strip()\n",
        "\n",
        "    print(f\"üîë Token found (length: {len(token)}). Logging in...\")\n",
        "    try:\n",
        "        login(token=token)\n",
        "        print(\"‚úÖ Authenticated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Login Failed: {e}\")\n",
        "        # Continue anyway, WikiText might be public enough to not need it\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: '{TOKEN_FILE}' not found. Attempting anonymous...\")\n",
        "\n",
        "# --- 3. LOAD DATASET ---\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing datasets library...\")\n",
        "    os.system('pip install datasets')\n",
        "    from datasets import load_dataset\n",
        "\n",
        "print(\"‚¨áÔ∏è  Fetching WikiText-103 via Hugging Face...\")\n",
        "# Using 'wikitext-103-raw-v1'\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "\n",
        "# --- 4. PROCESSING ---\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def process_split(hf_split, bin_name):\n",
        "    bin_path = os.path.join(DATA_DIR, bin_name)\n",
        "\n",
        "    if os.path.exists(bin_path):\n",
        "        print(f\"‚úÖ {bin_name} exists. Skipping.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚öôÔ∏è  Processing {hf_split} -> {bin_name}...\")\n",
        "\n",
        "    texts = dataset[hf_split]['text']\n",
        "    all_ids = []\n",
        "\n",
        "    # Process in chunks\n",
        "    batch_size = 1000\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        # Filter empty lines\n",
        "        valid_batch = [t for t in batch if len(t) > 0]\n",
        "        if not valid_batch: continue\n",
        "\n",
        "        text_chunk = tokenizer.eos_token.join(valid_batch)\n",
        "        ids = tokenizer.encode(text_chunk)\n",
        "        all_ids.extend(ids)\n",
        "\n",
        "    # Save\n",
        "    arr = np.array(all_ids, dtype=np.uint16)\n",
        "    arr.tofile(bin_path)\n",
        "    print(f\"   üíæ Saved {len(arr):,} tokens to {bin_path}\")\n",
        "\n",
        "# Run\n",
        "process_split(\"train\", \"train.bin\")\n",
        "process_split(\"validation\", \"val.bin\")\n",
        "process_split(\"test\", \"test.bin\")\n",
        "\n",
        "print(\"\\nüéâ WikiText-103 Ready for the Arena.\")"
      ],
      "metadata": {
        "id": "FLq6iJ4_q-SF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 Wiki 103 Baseline below"
      ],
      "metadata": {
        "id": "HzjpQ3W8zYqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_baseline\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üìâ JANUS WIKI BASELINE (SPORT MODE)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class WikiConfig:\n",
        "    def __init__(self):\n",
        "        # Architecture (Janus Spec)\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # Training\n",
        "        self.max_steps = 6000\n",
        "        # üü¢ OPTIMIZATION: Bumping Batch to 16 (from 8)\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4      # Effective Batch still 64 (16 * 4)\n",
        "\n",
        "        # Logging\n",
        "        self.lite_interval = 500\n",
        "        self.full_interval = 1000\n",
        "\n",
        "# --- 4. ARCHITECTURE ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, return_metrics=False):\n",
        "        a, m = self.attn(self.ln1(x), return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "\n",
        "        all_metrics = []\n",
        "        for block in self.blocks:\n",
        "            x, m = block(x, return_metrics)\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, all_metrics\n",
        "\n",
        "# --- 5. DATA & LOGGING ---\n",
        "class BinLoader:\n",
        "    def __init__(self, data_dir, block_size, batch_size):\n",
        "        self.train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode='r')\n",
        "        self.val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        print(f\"üì¶ WikiText | Train: {len(self.train_data):,} | Val: {len(self.val_data):,}\")\n",
        "\n",
        "    def get_batch(self, split='train'):\n",
        "        d = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(d) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(d[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(d[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "    def log(self, step, loss, val_loss, metrics):\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "        }\n",
        "        if metrics and len(metrics) > 0:\n",
        "            for i, m in enumerate(metrics):\n",
        "                if not m: continue\n",
        "                if 'sigma_a' in m: row[f\"L{i}_sigma_a\"] = m['sigma_a'].mean().item()\n",
        "                if 'eff_rank' in m: row[f\"L{i}_eff_rank\"] = m['eff_rank'].mean().item()\n",
        "        self.buffer.append(row)\n",
        "\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_base.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def run_baseline():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = WikiConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4, weight_decay=0.1)\n",
        "    loader = BinLoader(DATA_DIR, cfg.max_seq_len, cfg.batch_size)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    ckpt_path = os.path.join(SAVE_DIR, \"ckpt_baseline.pt\")\n",
        "    if os.path.exists(ckpt_path):\n",
        "        print(\"üîÑ Resuming Checkpoint...\")\n",
        "        try:\n",
        "            c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "            model.load_state_dict(c['model'])\n",
        "            optimizer.load_state_dict(c['optim'])\n",
        "            start_step = c['step']\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è Checkpoint corrupt. Starting fresh.\")\n",
        "\n",
        "    print(f\"\\nüèÉ STARTING BASELINE (SPORT MODE): {start_step} -> {cfg.max_steps}\")\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    for step in pbar:\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch('train')\n",
        "            is_full = (step % cfg.full_interval == 0) and (_ == cfg.grad_accum - 1)\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, metrics = model(x, y, return_metrics=is_full)\n",
        "                total = loss / cfg.grad_accum\n",
        "\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % cfg.lite_interval == 0 or step == cfg.max_steps - 1:\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(20):\n",
        "                    vx, vy = loader.get_batch('val')\n",
        "                    vl, _ = model(vx, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, metrics)\n",
        "            recorder.flush()\n",
        "            torch.save({'step': step, 'model': model.state_dict(), 'optim': optimizer.state_dict()}, ckpt_path)\n",
        "\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|V:{val_loss:.3f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}\")\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_v3_base.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ BASELINE COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_baseline()"
      ],
      "metadata": {
        "id": "SEb96AeSzR3G",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 Hero WikiText-103 Below"
      ],
      "metadata": {
        "id": "8p-XLmcrsUJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_hero\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ JANUS WIKI HERO (Pressure Active)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class WikiConfig:\n",
        "    def __init__(self):\n",
        "        # Architecture\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # Training\n",
        "        self.max_steps = 6000\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4      # Effective Batch 64\n",
        "\n",
        "        # Scheduler\n",
        "        self.max_pressure = 0.15\n",
        "\n",
        "        # Logging\n",
        "        self.lite_interval = 500\n",
        "        self.full_interval = 1000\n",
        "\n",
        "# --- 4. ARCHITECTURE (With Steering) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        # --- VSM PRESSURE LOGIC ---\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        if l_div > 0.0 and self.training:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 5. SCHEDULER & LOADERS ---\n",
        "class FlightController:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def get_pressure(self, step):\n",
        "        # 0-1500: Clean (25%)\n",
        "        if step < 1500: return 0.0\n",
        "        # 1500-3000: Ramp (25%)\n",
        "        elif step < 3000: return self.config.max_pressure * ((step - 1500) / 1500)\n",
        "        # 3000-4500: Hold (25%)\n",
        "        elif step < 4500: return self.config.max_pressure\n",
        "        # 4500-6000: Decay (25%)\n",
        "        else: return self.config.max_pressure * ((6000 - step) / 1500)\n",
        "\n",
        "    def get_lambdas(self, step):\n",
        "        p = self.get_pressure(step)\n",
        "        base_coh = p * 0.2\n",
        "        lambdas = []\n",
        "        for i in range(self.config.n_layers):\n",
        "            ratio = (i + 1) / self.config.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "            lambdas.append((base_coh * s_mult, p * s_mult))\n",
        "        return lambdas, p\n",
        "\n",
        "class BinLoader:\n",
        "    def __init__(self, data_dir, block_size, batch_size):\n",
        "        self.train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode='r')\n",
        "        self.val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode='r')\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        print(f\"üì¶ WikiText | Train: {len(self.train_data):,} | Val: {len(self.val_data):,}\")\n",
        "    def get_batch(self, split='train'):\n",
        "        d = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(d) - self.block_size, (self.batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(d[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(d[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure\n",
        "        }\n",
        "        if metrics and len(metrics) > 0:\n",
        "            for i, m in enumerate(metrics):\n",
        "                if not m: continue\n",
        "                if 'sigma_a' in m: row[f\"L{i}_sigma_a\"] = m['sigma_a'].mean().item()\n",
        "                if 'eff_rank' in m: row[f\"L{i}_eff_rank\"] = m['eff_rank'].mean().item()\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_hero.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def run_hero():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = WikiConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6e-4, weight_decay=0.1)\n",
        "    scheduler = FlightController(cfg)\n",
        "    loader = BinLoader(DATA_DIR, cfg.max_seq_len, cfg.batch_size)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    ckpt_path = os.path.join(SAVE_DIR, \"ckpt_hero.pt\")\n",
        "    if os.path.exists(ckpt_path):\n",
        "        print(\"üîÑ Resuming Checkpoint...\")\n",
        "        try:\n",
        "            c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "            model.load_state_dict(c['model'])\n",
        "            optimizer.load_state_dict(c['optim'])\n",
        "            start_step = c['step']\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è Checkpoint corrupt. Starting fresh.\")\n",
        "\n",
        "    print(f\"\\nüèÉ STARTING HERO RUN: {start_step} -> {cfg.max_steps}\")\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    for step in pbar:\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        lambdas, pressure = scheduler.get_lambdas(step)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch('train')\n",
        "            is_full = (step % cfg.full_interval == 0) and (_ == cfg.grad_accum - 1)\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas, y, return_metrics=is_full)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % cfg.lite_interval == 0 or step == cfg.max_steps - 1:\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(20):\n",
        "                    vx, vy = loader.get_batch('val')\n",
        "                    # Zero pressure for validation\n",
        "                    zero_l = [(0.0,0.0)]*cfg.n_layers\n",
        "                    vl, _, _ = model(vx, zero_l, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, pressure, metrics)\n",
        "            recorder.flush()\n",
        "            torch.save({'step': step, 'model': model.state_dict(), 'optim': optimizer.state_dict()}, ckpt_path)\n",
        "\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|V:{val_loss:.3f}|P:{pressure:.3f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|P:{pressure:.3f}\")\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_v3_hero.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ HERO RUN COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_hero()"
      ],
      "metadata": {
        "id": "VUPVi1yguC-O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "BASE_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_baseline/telemetry_base.parquet\")\n",
        "HERO_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_hero/telemetry_hero.parquet\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚öñÔ∏è WIKITEXT DUEL: BASELINE vs. HERO\")\n",
        "\n",
        "def load_data(path, label):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ùå Missing: {path}\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_parquet(path)\n",
        "    df = df.sort_values('step').drop_duplicates(subset='step', keep='last')\n",
        "    df['Model'] = label\n",
        "\n",
        "    # Calculate Average Sigma_A across all layers (if columns exist)\n",
        "    sa_cols = [c for c in df.columns if 'sigma_a' in c]\n",
        "    if sa_cols:\n",
        "        df['sigma_a_avg'] = df[sa_cols].mean(axis=1)\n",
        "    else:\n",
        "        df['sigma_a_avg'] = np.nan\n",
        "\n",
        "    # Calculate Average Eff Rank\n",
        "    er_cols = [c for c in df.columns if 'eff_rank' in c]\n",
        "    if er_cols:\n",
        "        df['eff_rank_avg'] = df[er_cols].mean(axis=1)\n",
        "    else:\n",
        "        df['eff_rank_avg'] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "def run_analysis():\n",
        "    base = load_data(BASE_PATH, \"Baseline (Passive)\")\n",
        "    hero = load_data(HERO_PATH, \"Hero (Active Pressure)\")\n",
        "\n",
        "    if base is None or hero is None: return\n",
        "\n",
        "    combined = pd.concat([base, hero])\n",
        "\n",
        "    # PLOTTING\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 18), sharex=True)\n",
        "    palette = {'Baseline (Passive)': 'gray', 'Hero (Active Pressure)': 'tab:blue'}\n",
        "\n",
        "    # 1. Validation Loss (The Scoreboard)\n",
        "    sns.lineplot(data=combined, x='step', y='val_loss', hue='Model', ax=axes[0], palette=palette, linewidth=2.5)\n",
        "    axes[0].set_title(\"Validation Loss (Lower is Better)\")\n",
        "    axes[0].set_ylabel(\"Cross Entropy\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add annotation for the cooldown phase\n",
        "    axes[0].axvspan(4500, 6000, color='green', alpha=0.1, label='Cooldown Phase')\n",
        "\n",
        "    # 2. Perplexity (The Real World Metric)\n",
        "    # Convert loss to PPL for visualization\n",
        "    combined['ppl'] = np.exp(combined['val_loss'])\n",
        "    sns.lineplot(data=combined, x='step', y='ppl', hue='Model', ax=axes[1], palette=palette, linewidth=2.5)\n",
        "    axes[1].set_title(\"Perplexity (The Generalization Gap)\")\n",
        "    axes[1].set_ylabel(\"PPL\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Orthogonality (The Mechanism)\n",
        "    # Only plot points where we have data (every 1000 steps)\n",
        "    subset = combined.dropna(subset=['sigma_a_avg'])\n",
        "    if not subset.empty:\n",
        "        sns.lineplot(data=subset, x='step', y='sigma_a_avg', hue='Model', ax=axes[2], palette=palette, marker='o', linewidth=2)\n",
        "        axes[2].set_title(\"Head Redundancy (Sigma A)\")\n",
        "        axes[2].set_ylabel(\"Correlation (Lower = More Orthogonal)\")\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        # Mark the pressure zone\n",
        "        axes[2].axvspan(1500, 4500, color='red', alpha=0.1, label='Pressure Zone')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(REPORT_DIR, \"wikitext_duel_analysis.png\")\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"\\nüìä Forensic Chart Saved: {save_path}\")\n",
        "\n",
        "    # STATS CARD\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üèÜ FINAL WIKITEXT SCORECARD (Step 6000)\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    b_final = base.iloc[-1]\n",
        "    h_final = hero.iloc[-1]\n",
        "\n",
        "    b_ppl = math.exp(b_final['val_loss'])\n",
        "    h_ppl = math.exp(h_final['val_loss'])\n",
        "    imp = b_ppl - h_ppl\n",
        "\n",
        "    print(f\"PERPLEXITY (Lower Wins):\")\n",
        "    print(f\"   Baseline: {b_ppl:.2f}\")\n",
        "    print(f\"   Hero:     {h_ppl:.2f}\")\n",
        "    print(f\"   Delta:    -{imp:.2f} PPL\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"VALIDATION LOSS:\")\n",
        "    print(f\"   Baseline: {b_final['val_loss']:.4f}\")\n",
        "    print(f\"   Hero:     {h_final['val_loss']:.4f}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Check Mechanism\n",
        "    # Compare Sigma A at Step 4000 (Peak Pressure)\n",
        "    try:\n",
        "        b_mid = base[base['step'] == 4000].iloc[0]['sigma_a_avg']\n",
        "        h_mid = hero[hero['step'] == 4000].iloc[0]['sigma_a_avg']\n",
        "        print(f\"MECHANISM CHECK (Step 4000 - Peak Pressure):\")\n",
        "        print(f\"   Baseline Redundancy: {b_mid:.4f}\")\n",
        "        print(f\"   Hero Redundancy:     {h_mid:.4f}\")\n",
        "        if h_mid < b_mid:\n",
        "            print(\"   ‚úÖ CONFIRMED: Pressure suppressed redundancy.\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è ANOMALY: Pressure did not suppress redundancy.\")\n",
        "    except:\n",
        "        print(\"   (Mid-run metrics missing, skipping check)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import math\n",
        "    run_analysis()"
      ],
      "metadata": {
        "id": "zbROb4w5ULWv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference Testing"
      ],
      "metadata": {
        "id": "-9Bvhgjsiv1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "BASE_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_baseline/janus_v3_base.pt\")\n",
        "HERO_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_hero/janus_v3_hero.pt\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚öîÔ∏è JANUS INFERENCE DUEL: BASELINE vs. HERO\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIGURATION (Identical for Both) ---\n",
        "class WikiConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.0\n",
        "\n",
        "# --- 3. ARCHITECTURE ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(y)\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -512:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- 4. LOAD WEIGHTS ---\n",
        "cfg = WikiConfig()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"üîÑ Loading Baseline...\")\n",
        "model_base = NewGPT(cfg).to(DEVICE)\n",
        "model_base.load_state_dict(torch.load(BASE_PATH, map_location=DEVICE))\n",
        "model_base.eval()\n",
        "\n",
        "print(\"üîÑ Loading Hero...\")\n",
        "model_hero = NewGPT(cfg).to(DEVICE)\n",
        "model_hero.load_state_dict(torch.load(HERO_PATH, map_location=DEVICE))\n",
        "model_hero.eval()\n",
        "\n",
        "# --- 5. THE ARENA ---\n",
        "prompts = [\n",
        "    \"The Roman Empire was\",\n",
        "    \"In the early 19th century, the\",\n",
        "    \"The chemical formula for water is\",\n",
        "    \"Following the release of the album,\",\n",
        "    \"Located in the northern part of\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ HEAD-TO-HEAD: BASELINE vs. HERO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for p in prompts:\n",
        "    input_ids = tokenizer.encode(p, return_tensors='pt').to(DEVICE)\n",
        "\n",
        "    # Generate Baseline\n",
        "    out_b = model_base.generate(input_ids, max_new_tokens=60, temperature=0.6)\n",
        "    text_b = tokenizer.decode(out_b[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    # Generate Hero\n",
        "    out_h = model_hero.generate(input_ids, max_new_tokens=60, temperature=0.6)\n",
        "    text_h = tokenizer.decode(out_h[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\nüìù PROMPT: {p}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"‚ö™ BASELINE: {text_b[len(p):].strip()}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"üîµ HERO:     {text_h[len(p):].strip()}\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "j9TSU3D_itzw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WikiText-103 Partitioning Cell Below"
      ],
      "metadata": {
        "id": "y6MPkyc25_p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "SOURCE_FILE = os.path.join(PROJECT_ROOT, \"data/wikitext/train.bin\")\n",
        "DEST_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "os.makedirs(DEST_DIR, exist_ok=True)\n",
        "\n",
        "# Spec Constants\n",
        "TOTAL_STEPS = 20000\n",
        "CHUNK_STEPS = 500\n",
        "NUM_CHUNKS = TOTAL_STEPS // CHUNK_STEPS  # Should be 40\n",
        "BATCH_SIZE = 16\n",
        "GRAD_ACCUM = 4\n",
        "SEQ_LEN = 512\n",
        "\n",
        "# Tokens per Step = 16 * 4 * 512 = 32,768\n",
        "TOKENS_PER_STEP = BATCH_SIZE * GRAD_ACCUM * SEQ_LEN\n",
        "TOKENS_PER_CHUNK = TOKENS_PER_STEP * CHUNK_STEPS # ~16.38M tokens\n",
        "\n",
        "print(f\"üî™ WIKITEXT PARTITIONING PROTOCOL\")\n",
        "print(f\"   Target: {NUM_CHUNKS} Chunks\")\n",
        "print(f\"   Size:   {TOKENS_PER_CHUNK:,} tokens/chunk\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if not os.path.exists(SOURCE_FILE):\n",
        "    print(f\"‚ùå Critical Error: Source file not found at {SOURCE_FILE}\")\n",
        "    exit(1)\n",
        "\n",
        "# Load Source (Memory Map to avoid RAM explosion)\n",
        "data = np.memmap(SOURCE_FILE, dtype=np.uint16, mode='r')\n",
        "total_tokens = len(data)\n",
        "print(f\"   Source: {total_tokens:,} tokens available\")\n",
        "\n",
        "# Validation\n",
        "required_tokens = TOKENS_PER_CHUNK * NUM_CHUNKS\n",
        "if total_tokens < required_tokens:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Source too small! Need {required_tokens:,}, have {total_tokens:,}\")\n",
        "    # We will loop the data if needed during training, but for partitioning,\n",
        "    # we should just wrap around or limit chunks.\n",
        "    # For now, let's just write what we can and warn.\n",
        "else:\n",
        "    print(f\"   Status: Sufficient Data ({total_tokens / required_tokens:.2f}x coverage)\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Slicing Loop\n",
        "for i in range(NUM_CHUNKS):\n",
        "    start = i * TOKENS_PER_CHUNK\n",
        "    end = start + TOKENS_PER_CHUNK\n",
        "\n",
        "    # Handle wrap-around safety (though WikiText is big enough usually)\n",
        "    if end > total_tokens:\n",
        "        print(f\"   ‚ö†Ô∏è Wrapping data for Chunk {i}...\")\n",
        "        # Complex wrap logic or just stop? WikiText-103 is ~100M tokens.\n",
        "        # 40 chunks * 16M = 640M tokens.\n",
        "        # WAIT.\n",
        "        # WikiText-103 is ~103M tokens.\n",
        "        # 20k steps * 64 batch * 512 seq = 655M tokens.\n",
        "        # We need to loop the dataset ~6.5 times.\n",
        "\n",
        "        # FIX: The partitioning script should Creates STATIC chunks by looping the source data.\n",
        "        # This ensures the training loader never has to think about indices.\n",
        "\n",
        "        # Create a buffer for this chunk\n",
        "        chunk_data = np.empty(TOKENS_PER_CHUNK, dtype=np.uint16)\n",
        "\n",
        "        # Fill it\n",
        "        current_fill = 0\n",
        "        src_ptr = start % total_tokens\n",
        "\n",
        "        while current_fill < TOKENS_PER_CHUNK:\n",
        "            available = min(total_tokens - src_ptr, TOKENS_PER_CHUNK - current_fill)\n",
        "            chunk_data[current_fill : current_fill + available] = data[src_ptr : src_ptr + available]\n",
        "            current_fill += available\n",
        "            src_ptr = (src_ptr + available) % total_tokens\n",
        "\n",
        "    else:\n",
        "        # Direct slice\n",
        "        chunk_data = data[start:end]\n",
        "\n",
        "    # Save\n",
        "    fname = f\"train_chunk_{i:03d}.bin\"\n",
        "    fpath = os.path.join(DEST_DIR, fname)\n",
        "\n",
        "    # Write to disk\n",
        "    # We use open/write for safety over memmap flush\n",
        "    with open(fpath, 'wb') as f:\n",
        "        f.write(chunk_data.tobytes())\n",
        "\n",
        "    print(f\"   ‚úÖ Wrote {fname} ({len(chunk_data):,} tokens)\")\n",
        "\n",
        "print(\"\\nüéâ PARTITIONING COMPLETE.\")\n",
        "print(f\"   Ready for {TOTAL_STEPS} step marathon.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V0vPOcGJ5-39",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Chunks"
      ],
      "metadata": {
        "id": "vIQUl4xi6dGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CHUNKS_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "\n",
        "# Specs\n",
        "EXPECTED_COUNT = 40\n",
        "TOKENS_PER_CHUNK = 16_384_000\n",
        "EXPECTED_BYTES = TOKENS_PER_CHUNK * 2 # uint16 = 2 bytes\n",
        "\n",
        "print(f\"üïµÔ∏è JANUS DATA FORENSICS\")\n",
        "print(f\"   Target: {CHUNKS_DIR}\")\n",
        "\n",
        "def verify():\n",
        "    if not os.path.exists(CHUNKS_DIR):\n",
        "        print(\"‚ùå CRITICAL: Directory not found.\")\n",
        "        return\n",
        "\n",
        "    files = sorted([f for f in os.listdir(CHUNKS_DIR) if f.startswith(\"train_chunk_\") and f.endswith(\".bin\")])\n",
        "\n",
        "    # 1. Count Check\n",
        "    if len(files) == EXPECTED_COUNT:\n",
        "        print(f\"‚úÖ Found {len(files)} chunks (Correct).\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  WARNING: Found {len(files)} chunks (Expected {EXPECTED_COUNT}).\")\n",
        "\n",
        "    # 2. Byte Check\n",
        "    size_errors = 0\n",
        "    for f in files:\n",
        "        path = os.path.join(CHUNKS_DIR, f)\n",
        "        size = os.path.getsize(path)\n",
        "        if size != EXPECTED_BYTES:\n",
        "            print(f\"   ‚ùå {f}: SIZE MISMATCH! {size:,} bytes (Expected {EXPECTED_BYTES:,})\")\n",
        "            size_errors += 1\n",
        "\n",
        "    if size_errors == 0:\n",
        "        print(\"‚úÖ All file sizes are byte-perfect.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Abort: {size_errors} chunks are corrupted.\")\n",
        "        return\n",
        "\n",
        "    # 3. Content Logic Check (First and Last)\n",
        "    print(\"\\nüî¨ Inspecting Content...\")\n",
        "\n",
        "    # Check First Chunk\n",
        "    c0 = files[0]\n",
        "    data0 = np.memmap(os.path.join(CHUNKS_DIR, c0), dtype=np.uint16, mode='r')\n",
        "    print(f\"   [{c0}] Sample: {data0[:10]}\")\n",
        "\n",
        "    # Check Last Chunk (Crucial for loop logic verification)\n",
        "    cL = files[-1]\n",
        "    dataL = np.memmap(os.path.join(CHUNKS_DIR, cL), dtype=np.uint16, mode='r')\n",
        "\n",
        "    # Check for \"Zero Death\" (if the buffer wasn't filled, the end would be all zeros)\n",
        "    last_1k = dataL[-1000:]\n",
        "    zeros = np.sum(last_1k == 0)\n",
        "\n",
        "    print(f\"   [{cL}] Last 1000 tokens: {zeros} zeros found.\")\n",
        "\n",
        "    if zeros > 900:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: The end of the last chunk is mostly zeros. The dataset loop might have failed.\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ Loop Logic Confirmed: Data appears dense through the end of the last chunk.\")\n",
        "\n",
        "    print(\"\\nüèÅ DATASET STATUS: READY FOR MARATHON.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    verify()"
      ],
      "metadata": {
        "id": "aYo00-TB6bjT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Analyze] Marathon Telemetry Analyzer\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# --- SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "MARATHON_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_marathon_v3/telemetry_marathon.parquet\")\n",
        "HERO_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_hero/telemetry_hero.parquet\")\n",
        "REPORT_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üïµÔ∏è JANUS MARATHON DIAGNOSTICS\")\n",
        "\n",
        "def load_data(path, label):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ùå Missing: {path}\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_parquet(path)\n",
        "    df = df.sort_values('step').drop_duplicates(subset='step', keep='last')\n",
        "    df['Model'] = label\n",
        "\n",
        "    # Calculate Sigma A Avg\n",
        "    sa_cols = [c for c in df.columns if 'sigma_a' in c]\n",
        "    if sa_cols:\n",
        "        df['sigma_a_avg'] = df[sa_cols].mean(axis=1)\n",
        "    else:\n",
        "        df['sigma_a_avg'] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "def run_diagnostics():\n",
        "    current = load_data(MARATHON_PATH, \"Current Marathon\")\n",
        "    reference = load_data(HERO_PATH, \"Reference Hero (6k)\")\n",
        "\n",
        "    if current is None: return\n",
        "\n",
        "    # Combine for plotting if reference exists\n",
        "    if reference is not None:\n",
        "        combined = pd.concat([reference, current])\n",
        "    else:\n",
        "        combined = current\n",
        "\n",
        "    print(f\"\\nüìä Current Status (Step {current['step'].max()}):\")\n",
        "    last_row = current.iloc[-1]\n",
        "    print(f\"   Loss:      {last_row['loss']:.4f}\")\n",
        "    print(f\"   Val Loss:  {last_row['val_loss']:.4f}\")\n",
        "    print(f\"   Pressure:  {last_row['pressure']:.4f}\")\n",
        "    if not np.isnan(last_row['sigma_a_avg']):\n",
        "        print(f\"   Sigma A:   {last_row['sigma_a_avg']:.4f}\")\n",
        "\n",
        "    # PLOTTING\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
        "    palette = {'Reference Hero (6k)': 'gray', 'Current Marathon': 'tab:green'}\n",
        "\n",
        "    # 1. Validation Loss\n",
        "    sns.lineplot(data=combined, x='step', y='val_loss', hue='Model', ax=axes[0], palette=palette, linewidth=2)\n",
        "    axes[0].set_title(\"Validation Loss Tracking\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Pressure Schedule\n",
        "    sns.lineplot(data=combined, x='step', y='pressure', hue='Model', ax=axes[1], palette=palette, linewidth=2, linestyle='--')\n",
        "    axes[1].set_title(\"Pressure Schedule Check\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Orthogonality (Sigma A)\n",
        "    subset = combined.dropna(subset=['sigma_a_avg'])\n",
        "    if not subset.empty:\n",
        "        sns.lineplot(data=subset, x='step', y='sigma_a_avg', hue='Model', ax=axes[2], palette=palette, marker='o')\n",
        "        axes[2].set_title(\"Head Redundancy (Sigma A)\")\n",
        "        axes[2].set_ylim(0, 0.02) # Zoom in\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(REPORT_DIR, \"marathon_diagnostic.png\")\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"\\n‚úÖ Diagnostic Chart Saved: {save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_diagnostics()"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "OtW7AwJppoLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Data Prep] WikiText-2 Tokenization & Preparation\n",
        "# Janus Engineering Framework - Sub-module 4.1\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP & PATHS ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "# Target directory as specified: Project_XAI_Physical_Janus/data/Wikitext_2/\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data/Wikitext_2\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìö WIKITEXT-2 PREP (JANUS V3 SPECS)\")\n",
        "print(f\"üìÇ Output Directory: {DATA_DIR}\")\n",
        "\n",
        "# --- 2. AUTHENTICATION (The \"Janus Workaround\") ---\n",
        "TOKEN_FILE = \"/content/hf_token.txt\"\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import login\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing huggingface_hub...\")\n",
        "    os.system('pip install huggingface_hub')\n",
        "    from huggingface_hub import login\n",
        "\n",
        "if os.path.exists(TOKEN_FILE):\n",
        "    with open(TOKEN_FILE, 'r', encoding='utf-8') as f:\n",
        "        # üü¢ CRITICAL FIX: Strip Byte Order Marks (BOM) and whitespace for HF Login\n",
        "        token = f.read().replace('\\ufeff', '').strip()\n",
        "\n",
        "    print(f\"üîë Token found (length: {len(token)}). Logging in...\")\n",
        "    try:\n",
        "        login(token=token)\n",
        "        print(\"‚úÖ Authenticated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Login Failed: {e}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: '{TOKEN_FILE}' not found. Dataset must be public.\")\n",
        "\n",
        "# --- 3. LOAD DATASET ---\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing datasets library...\")\n",
        "    os.system('pip install datasets')\n",
        "    from datasets import load_dataset\n",
        "\n",
        "print(\"‚¨áÔ∏è  Fetching WikiText-2 via Hugging Face...\")\n",
        "# Standard WikiText-2 raw split for Janus v3 generalization tests\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "# --- 4. PROCESSING ---\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def process_split(hf_split, bin_name):\n",
        "    bin_path = os.path.join(DATA_DIR, bin_name)\n",
        "\n",
        "    if os.path.exists(bin_path):\n",
        "        print(f\"‚úÖ {bin_name} already exists. Skipping.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚öôÔ∏è  Processing {hf_split} -> {bin_name}...\")\n",
        "\n",
        "    texts = dataset[hf_split]['text']\n",
        "    all_ids = []\n",
        "\n",
        "    # Process in chunks to manage L4 memory efficiently\n",
        "    batch_size = 1000\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        # Filter empty lines to preserve expressive bandwidth\n",
        "        valid_batch = [t for t in batch if len(t.strip()) > 0]\n",
        "        if not valid_batch: continue\n",
        "\n",
        "        # Join with EOS token to delineate narrative boundaries\n",
        "        text_chunk = tokenizer.eos_token.join(valid_batch)\n",
        "        ids = tokenizer.encode(text_chunk)\n",
        "        all_ids.extend(ids)\n",
        "\n",
        "    # Save as uint16 for parameter efficiency and scaling compatibility\n",
        "    arr = np.array(all_ids, dtype=np.uint16)\n",
        "    arr.tofile(bin_path)\n",
        "    print(f\"   üíæ Saved {len(arr):,} tokens to {bin_path}\")\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "# Updated filenames for WikiText-2 specific training\n",
        "process_split(\"train\", \"W2train.bin\")\n",
        "process_split(\"validation\", \"W2val.bin\")\n",
        "process_split(\"test\", \"W2test.bin\")\n",
        "\n",
        "print(\"\\nüéâ WikiText-2 Vector Space Homeostasis Ready.\")"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "Oj7GXGsvlmVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Janus v3 Spec Convergence Run\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CHUNKS_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_marathon_v3\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üèÉ JANUS MARATHON (FIXED)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class MarathonConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        self.max_steps = 20000\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4\n",
        "        self.max_pressure = 0.15\n",
        "\n",
        "        self.ckpt_interval = 2000\n",
        "        self.chunk_steps = 500\n",
        "\n",
        "        self.lite_interval = 1000\n",
        "        self.full_interval = 2500\n",
        "\n",
        "# --- 4. ARCHITECTURE ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "        if l_div > 0.0 and self.training:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 5. LOGIC CONTROL ---\n",
        "class FlightController:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    def get_pressure(self, step):\n",
        "        if step < 1500: return 0.0\n",
        "        elif step < 3000: return self.config.max_pressure * ((step - 1500) / 1500)\n",
        "        elif step < 4500: return self.config.max_pressure\n",
        "        elif step < 7500:\n",
        "            progress = min((step - 4500) / 3000, 1.0)\n",
        "            return self.config.max_pressure * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "        else: return 0.0\n",
        "    def get_lambdas(self, step):\n",
        "        p = self.get_pressure(step)\n",
        "        base_coh = p * 0.2\n",
        "        lambdas = []\n",
        "        for i in range(self.config.n_layers):\n",
        "            ratio = (i + 1) / self.config.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "            lambdas.append((base_coh * s_mult, p * s_mult))\n",
        "        return lambdas, p\n",
        "\n",
        "class ChunkLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.current_chunk_idx = -1\n",
        "        self.data = None\n",
        "        self.val_data = np.memmap(os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\"), dtype=np.uint16, mode='r')\n",
        "    def load_for_step(self, step):\n",
        "        target_chunk = step // self.config.chunk_steps\n",
        "        if target_chunk != self.current_chunk_idx:\n",
        "            fname = f\"train_chunk_{target_chunk:03d}.bin\"\n",
        "            fpath = os.path.join(CHUNKS_DIR, fname)\n",
        "            if not os.path.exists(fpath):\n",
        "                fallback_idx = target_chunk % 40\n",
        "                fpath = os.path.join(CHUNKS_DIR, f\"train_chunk_{fallback_idx:03d}.bin\")\n",
        "            with open(fpath, 'rb') as f:\n",
        "                self.data = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "            self.current_chunk_idx = target_chunk\n",
        "    def get_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "    def get_val_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.val_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.val_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.val_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "        self.start_step = 0 # To filter garbage\n",
        "    def set_start_step(self, step):\n",
        "        self.start_step = step\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        # üõ°Ô∏è TELEMETRY FILTER: Don't log garbage steps from restart attempts\n",
        "        if step < self.start_step: return\n",
        "\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure\n",
        "        }\n",
        "        if metrics and len(metrics) > 0:\n",
        "            for i, m in enumerate(metrics):\n",
        "                if not m: continue\n",
        "                if 'sigma_a' in m: row[f\"L{i}_sigma_a\"] = m['sigma_a'].mean().item()\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_marathon.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def run_marathon():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = MarathonConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6.29e-4, weight_decay=1.34e-4)\n",
        "    scheduler = FlightController(cfg)\n",
        "    loader = ChunkLoader(cfg)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "\n",
        "    if ckpts:\n",
        "        ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "        latest = ckpts[-1]\n",
        "        ckpt_path = os.path.join(SAVE_DIR, latest)\n",
        "        print(f\"üîÑ Found checkpoints. Loading {latest}...\")\n",
        "\n",
        "        # üü¢ CRITICAL SAFETY: No try/except. Let it crash if corrupt.\n",
        "        c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(c['model'])\n",
        "        optimizer.load_state_dict(c['optim'])\n",
        "\n",
        "        # üü¢ CRITICAL FIX: Ensure RNG states are on CPU\n",
        "        cpu_rng = c['rng_cpu'].cpu() if c['rng_cpu'].is_cuda else c['rng_cpu']\n",
        "        gpu_rng = c['rng_gpu'].cpu() if c['rng_gpu'].is_cuda else c['rng_gpu']\n",
        "\n",
        "        torch.set_rng_state(cpu_rng)\n",
        "        torch.cuda.set_rng_state(gpu_rng)\n",
        "\n",
        "        start_step = c['step'] + 1\n",
        "        recorder.set_start_step(start_step) # Tell logger to ignore steps < start_step\n",
        "        print(f\"‚úÖ State Restored (Step {start_step}). RNG Verified.\")\n",
        "\n",
        "    print(f\"\\nüèÉ STARTING MARATHON: {start_step} -> {cfg.max_steps}\")\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    for step in pbar:\n",
        "        loader.load_for_step(step)\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        lambdas, pressure = scheduler.get_lambdas(step)\n",
        "        optimizer.zero_grad()\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch(cfg.batch_size)\n",
        "            is_full = (step % cfg.full_interval == 0) and (_ == cfg.grad_accum - 1)\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas, y, return_metrics=is_full)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step > 0 and (step % cfg.lite_interval == 0 or step == cfg.max_steps - 1):\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(20):\n",
        "                    vx, vy = loader.get_val_batch(cfg.batch_size)\n",
        "                    vl, _, _ = model(vx, [(0.0,0.0)]*cfg.n_layers, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, pressure, metrics)\n",
        "            recorder.flush()\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|V:{val_loss:.3f}|P:{pressure:.3f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|P:{pressure:.3f}\")\n",
        "\n",
        "        if step > 0 and step % cfg.ckpt_interval == 0:\n",
        "            ckpt_name = f\"ckpt_step_{step}.pt\"\n",
        "            ckpt_path = os.path.join(SAVE_DIR, ckpt_name)\n",
        "            save_dict = {\n",
        "                'step': step,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'rng_cpu': torch.get_rng_state(),\n",
        "                'rng_gpu': torch.cuda.get_rng_state()\n",
        "            }\n",
        "            torch.save(save_dict, ckpt_path)\n",
        "            all_ckpts = sorted([f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\")])\n",
        "            if len(all_ckpts) > 3: os.remove(os.path.join(SAVE_DIR, all_ckpts[0]))\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_v3_marathon_final.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ MARATHON COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_marathon()"
      ],
      "metadata": {
        "id": "AbLifWyA6_8z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Janus 20k Baseline\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CHUNKS_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_marathon_baseline\") # New Folder\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üìâ JANUS MARATHON BASELINE (Control Group - 20k)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class MarathonConfig:\n",
        "    def __init__(self):\n",
        "        # Architecture (Identical)\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # Training (Identical)\n",
        "        self.max_steps = 20000\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4\n",
        "\n",
        "        # Checkpointing (Reinstated)\n",
        "        self.ckpt_interval = 2000\n",
        "        self.chunk_steps = 500\n",
        "\n",
        "        # Telemetry (Identical Schedule)\n",
        "        self.lite_interval = 1000\n",
        "        self.full_interval = 2500\n",
        "\n",
        "# --- 4. ARCHITECTURE (Identical - No Steering) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        # NO STEERING LOGIC - BASELINE\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, return_metrics=False):\n",
        "        a, m = self.attn(self.ln1(x), return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        all_metrics = []\n",
        "        for block in self.blocks:\n",
        "            x, m = block(x, return_metrics)\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, all_metrics\n",
        "\n",
        "# --- 5. DATA & TELEMETRY ---\n",
        "class ChunkLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.current_chunk_idx = -1\n",
        "        self.data = None\n",
        "        self.val_data = np.memmap(os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\"), dtype=np.uint16, mode='r')\n",
        "\n",
        "    def load_for_step(self, step):\n",
        "        target_chunk = step // self.config.chunk_steps\n",
        "        if target_chunk != self.current_chunk_idx:\n",
        "            fname = f\"train_chunk_{target_chunk:03d}.bin\"\n",
        "            fpath = os.path.join(CHUNKS_DIR, fname)\n",
        "            if not os.path.exists(fpath):\n",
        "                # Fallback wrap\n",
        "                fallback_idx = target_chunk % 40\n",
        "                fpath = os.path.join(CHUNKS_DIR, f\"train_chunk_{fallback_idx:03d}.bin\")\n",
        "            with open(fpath, 'rb') as f:\n",
        "                self.data = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "            self.current_chunk_idx = target_chunk\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "    def get_val_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.val_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.val_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.val_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "        self.start_step = 0\n",
        "    def set_start_step(self, step):\n",
        "        self.start_step = step\n",
        "    def log(self, step, loss, val_loss, metrics):\n",
        "        if step < self.start_step: return\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": 0.0 # Constant for baseline\n",
        "        }\n",
        "        if metrics and len(metrics) > 0:\n",
        "            for i, m in enumerate(metrics):\n",
        "                if not m: continue\n",
        "                if 'sigma_a' in m: row[f\"L{i}_sigma_a\"] = m['sigma_a'].mean().item()\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_baseline_20k.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def run_baseline_20k():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = MarathonConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6.29e-4, weight_decay=1.34e-4)\n",
        "    loader = ChunkLoader(cfg)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "\n",
        "    if ckpts:\n",
        "        ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "        latest = ckpts[-1]\n",
        "        ckpt_path = os.path.join(SAVE_DIR, latest)\n",
        "        print(f\"üîÑ Found checkpoints. Loading {latest}...\")\n",
        "\n",
        "        c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(c['model'])\n",
        "        optimizer.load_state_dict(c['optim'])\n",
        "\n",
        "        cpu_rng = c['rng_cpu'].cpu() if c['rng_cpu'].is_cuda else c['rng_cpu']\n",
        "        gpu_rng = c['rng_gpu'].cpu() if c['rng_gpu'].is_cuda else c['rng_gpu']\n",
        "        torch.set_rng_state(cpu_rng)\n",
        "        torch.cuda.set_rng_state(gpu_rng)\n",
        "\n",
        "        start_step = c['step'] + 1\n",
        "        recorder.set_start_step(start_step)\n",
        "        print(f\"‚úÖ State Restored (Step {start_step}).\")\n",
        "\n",
        "    print(f\"\\nüèÉ STARTING BASELINE 20K: {start_step} -> {cfg.max_steps}\")\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    for step in pbar:\n",
        "        loader.load_for_step(step)\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch(cfg.batch_size)\n",
        "            is_full = (step % cfg.full_interval == 0) and (_ == cfg.grad_accum - 1)\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, metrics = model(x, y, return_metrics=is_full)\n",
        "                total = loss / cfg.grad_accum\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Telemetry\n",
        "        if step > 0 and (step % cfg.lite_interval == 0 or step == cfg.max_steps - 1):\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(20):\n",
        "                    vx, vy = loader.get_val_batch(cfg.batch_size)\n",
        "                    vl, _ = model(vx, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, metrics)\n",
        "            recorder.flush()\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|V:{val_loss:.3f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}\")\n",
        "\n",
        "        # Checkpoints (Every 2k)\n",
        "        if step > 0 and step % cfg.ckpt_interval == 0:\n",
        "            ckpt_name = f\"ckpt_step_{step}.pt\"\n",
        "            ckpt_path = os.path.join(SAVE_DIR, ckpt_name)\n",
        "            save_dict = {\n",
        "                'step': step,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'rng_cpu': torch.get_rng_state(),\n",
        "                'rng_gpu': torch.cuda.get_rng_state()\n",
        "            }\n",
        "            torch.save(save_dict, ckpt_path)\n",
        "            # Retention\n",
        "            all_ckpts = sorted([f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\")])\n",
        "            if len(all_ckpts) > 3: os.remove(os.path.join(SAVE_DIR, all_ckpts[0]))\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_v3_baseline_20k.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ BASELINE COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_baseline_20k()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8TkzDlVJoh5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Janus 20k  Constant Pressure Control\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CHUNKS_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_marathon_constant\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üß± JANUS MARATHON CONSTANT (FIXED)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class MarathonConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        self.max_steps = 20000\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4\n",
        "        self.max_pressure = 0.15\n",
        "\n",
        "        self.ckpt_interval = 2000\n",
        "        self.chunk_steps = 500\n",
        "\n",
        "        self.lite_interval = 1000\n",
        "        self.full_interval = 2500\n",
        "\n",
        "# --- 4. ARCHITECTURE ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        if l_div > 0.0 and self.training:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "            steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "                sub_out = head_out[:, :, :128, :].transpose(1, 2).reshape(self.n_heads, -1, self.d_head)\n",
        "                ranks = []\n",
        "                for h in range(self.n_heads):\n",
        "                    try:\n",
        "                        S_vals = torch.linalg.svdvals(sub_out[h].float())\n",
        "                        p = S_vals / S_vals.sum()\n",
        "                        ent = -torch.sum(p * torch.log(p + 1e-9))\n",
        "                        ranks.append(torch.exp(ent))\n",
        "                    except: ranks.append(torch.tensor(0.0))\n",
        "                metrics['eff_rank'] = torch.stack(ranks).to(x.device)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 5. LOGIC CONTROL ---\n",
        "class FlightController:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    def get_pressure(self, step):\n",
        "        if step < 1500: return self.config.max_pressure * (step / 1500)\n",
        "        else: return self.config.max_pressure\n",
        "    def get_lambdas(self, step):\n",
        "        p = self.get_pressure(step)\n",
        "        base_coh = p * 0.2\n",
        "        lambdas = []\n",
        "        for i in range(self.config.n_layers):\n",
        "            ratio = (i + 1) / self.config.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "            lambdas.append((base_coh * s_mult, p * s_mult))\n",
        "        return lambdas, p\n",
        "\n",
        "class ChunkLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.current_chunk_idx = -1\n",
        "        self.data = None\n",
        "        self.val_data = np.memmap(os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\"), dtype=np.uint16, mode='r')\n",
        "    def load_for_step(self, step):\n",
        "        target_chunk = step // self.config.chunk_steps\n",
        "        if target_chunk != self.current_chunk_idx:\n",
        "            fname = f\"train_chunk_{target_chunk:03d}.bin\"\n",
        "            fpath = os.path.join(CHUNKS_DIR, fname)\n",
        "            if not os.path.exists(fpath):\n",
        "                fallback_idx = target_chunk % 40\n",
        "                fpath = os.path.join(CHUNKS_DIR, f\"train_chunk_{fallback_idx:03d}.bin\")\n",
        "            with open(fpath, 'rb') as f:\n",
        "                self.data = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "            self.current_chunk_idx = target_chunk\n",
        "    def get_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "    def get_val_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.val_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.val_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.val_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "        self.start_step = 0\n",
        "    def set_start_step(self, step):\n",
        "        self.start_step = step\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        if step < self.start_step: return\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure\n",
        "        }\n",
        "        if metrics and len(metrics) > 0:\n",
        "            for i, m in enumerate(metrics):\n",
        "                if not m: continue\n",
        "                if 'sigma_a' in m: row[f\"L{i}_sigma_a\"] = m['sigma_a'].mean().item()\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_constant.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. MAIN ---\n",
        "def run_constant_fixed():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = MarathonConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6.29e-4, weight_decay=1.34e-4)\n",
        "    scheduler = FlightController(cfg)\n",
        "    loader = ChunkLoader(cfg)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "\n",
        "    if ckpts:\n",
        "        # üü¢ CRITICAL FIX: Sort by INTEGER step, not string\n",
        "        ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "        latest = ckpts[-1]\n",
        "        ckpt_path = os.path.join(SAVE_DIR, latest)\n",
        "        print(f\"üîÑ Found checkpoints. Loading {latest}...\")\n",
        "\n",
        "        c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(c['model'])\n",
        "        optimizer.load_state_dict(c['optim'])\n",
        "\n",
        "        cpu_rng = c['rng_cpu'].cpu() if c['rng_cpu'].is_cuda else c['rng_cpu']\n",
        "        gpu_rng = c['rng_gpu'].cpu() if c['rng_gpu'].is_cuda else c['rng_gpu']\n",
        "        torch.set_rng_state(cpu_rng)\n",
        "        torch.cuda.set_rng_state(gpu_rng)\n",
        "\n",
        "        start_step = c['step'] + 1\n",
        "        recorder.set_start_step(start_step)\n",
        "        print(f\"‚úÖ State Restored (Step {start_step}).\")\n",
        "\n",
        "    print(f\"\\nüèÉ STARTING MARATHON (CONSTANT 0.15): {start_step} -> {cfg.max_steps}\")\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    for step in pbar:\n",
        "        loader.load_for_step(step)\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        lambdas, pressure = scheduler.get_lambdas(step)\n",
        "        optimizer.zero_grad()\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch(cfg.batch_size)\n",
        "            is_full = (step % cfg.full_interval == 0) and (_ == cfg.grad_accum - 1)\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas, y, return_metrics=is_full)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step > 0 and (step % cfg.lite_interval == 0 or step == cfg.max_steps - 1):\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(20):\n",
        "                    vx, vy = loader.get_val_batch(cfg.batch_size)\n",
        "                    vl, _, _ = model(vx, [(0.0,0.0)]*cfg.n_layers, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, pressure, metrics)\n",
        "            recorder.flush()\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|V:{val_loss:.3f}|P:{pressure:.3f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.3f}|P:{pressure:.3f}\")\n",
        "\n",
        "        # --- CHECKPOINT RETENTION LOGIC (FIXED) ---\n",
        "        if step > 0 and step % cfg.ckpt_interval == 0:\n",
        "            ckpt_name = f\"ckpt_step_{step}.pt\"\n",
        "            ckpt_path = os.path.join(SAVE_DIR, ckpt_name)\n",
        "            save_dict = {\n",
        "                'step': step,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'rng_cpu': torch.get_rng_state(),\n",
        "                'rng_gpu': torch.cuda.get_rng_state()\n",
        "            }\n",
        "            torch.save(save_dict, ckpt_path)\n",
        "\n",
        "            # üü¢ FIX: Sort by INTEGER value to correctly identify oldest\n",
        "            all_ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "            all_ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "\n",
        "            if len(all_ckpts) > 3:\n",
        "                oldest = all_ckpts[0]\n",
        "                os.remove(os.path.join(SAVE_DIR, oldest))\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_v3_marathon_constant.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ CONSTANT RUN COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_constant_fixed()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wT8YkjsxpUkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] 6k vs 20k Inference Test\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "HERO_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_wikitext_hero/janus_v3_hero.pt\")\n",
        "MARATHON_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_marathon_v3/janus_v3_marathon_final.pt\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚öîÔ∏è MARATHON VERIFICATION: 6k HERO vs. 20k MARATHON\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "class WikiConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.0\n",
        "\n",
        "# --- 3. ARCHITECTURE (Janus v3) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(y)\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -512:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- 4. LOAD WEIGHTS ---\n",
        "cfg = WikiConfig()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"üîÑ Loading 6k Hero (Reference)...\")\n",
        "model_hero = NewGPT(cfg).to(DEVICE)\n",
        "try:\n",
        "    model_hero.load_state_dict(torch.load(HERO_PATH, map_location=DEVICE))\n",
        "    model_hero.eval()\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Hero 6k not found. Skipping.\")\n",
        "    model_hero = None\n",
        "\n",
        "print(\"üîÑ Loading 20k Marathon (Target)...\")\n",
        "model_marathon = NewGPT(cfg).to(DEVICE)\n",
        "if os.path.exists(MARATHON_PATH):\n",
        "    model_marathon.load_state_dict(torch.load(MARATHON_PATH, map_location=DEVICE))\n",
        "    model_marathon.eval()\n",
        "else:\n",
        "    print(f\"‚ùå CRITICAL: Marathon model not found at {MARATHON_PATH}\")\n",
        "    exit(1)\n",
        "\n",
        "# --- 5. THE TEST ---\n",
        "prompts = [\n",
        "    \"The Roman Empire was\",\n",
        "    \"In the early 19th century, the\",\n",
        "    \"The chemical formula for water is\",\n",
        "    \"Following the release of the album,\",\n",
        "    \"Located in the northern part of\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ INFERENCE CHECK: DID THE SNAPBACK BREAK IT?\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for p in prompts:\n",
        "    input_ids = tokenizer.encode(p, return_tensors='pt').to(DEVICE)\n",
        "\n",
        "    # Generate Hero (if avail)\n",
        "    text_h = \"N/A\"\n",
        "    if model_hero:\n",
        "        out_h = model_hero.generate(input_ids, max_new_tokens=60, temperature=0.6)\n",
        "        text_h = tokenizer.decode(out_h[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    # Generate Marathon\n",
        "    out_m = model_marathon.generate(input_ids, max_new_tokens=60, temperature=0.6)\n",
        "    text_m = tokenizer.decode(out_m[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\nüìù PROMPT: {p}\")\n",
        "    print(\"-\" * 20)\n",
        "    if model_hero:\n",
        "        print(f\"üîµ HERO (6k):   {text_h[len(p):].strip()}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"üü¢ MARATHON (20k): {text_m[len(p):].strip()}\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "o6C-cKkasjeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [RUN] Inference Constant\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CONSTANT_DIR = os.path.join(PROJECT_ROOT,\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"ü©∫ JANUS CONSTANT RUN: AUDIT (FIXED)\")\n",
        "print(f\"   Target: {CONSTANT_DIR}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "\n",
        "# --- 2. ARCHITECTURE ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        y = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(y)\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        for block in self.blocks: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=50):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -512:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "class WikiConfig:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.0\n",
        "\n",
        "# --- 3. EXECUTION ---\n",
        "def audit_run():\n",
        "    # A. Find Checkpoint\n",
        "    ckpts = [f for f in os.listdir(CONSTANT_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "    if not ckpts:\n",
        "        print(\"‚ùå No checkpoints found!\")\n",
        "        return\n",
        "\n",
        "    ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "    latest_ckpt = ckpts[-1]\n",
        "    ckpt_step = int(latest_ckpt.split('_')[2].split('.')[0])\n",
        "\n",
        "    print(f\"\\nüìÇ Loading Latest: {latest_ckpt} (Step {ckpt_step})\")\n",
        "\n",
        "    # B. Load Model\n",
        "    cfg = WikiConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    ckpt = torch.load(os.path.join(CONSTANT_DIR, latest_ckpt), map_location=DEVICE)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(ckpt['model'], strict=False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Strict load failed, trying loose: {e}\")\n",
        "        model.load_state_dict(ckpt['model'], strict=False)\n",
        "\n",
        "    model.eval()\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # C. Run Inference\n",
        "    prompts = [\n",
        "        \"The Roman Empire was\",\n",
        "        \"In the early 19th century, the\",\n",
        "        \"The chemical formula for water is\",\n",
        "        \"Following the release of the album,\",\n",
        "        \"Located in the northern part of\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üß™ INFERENCE CHECK (Step {ckpt_step})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for p in prompts:\n",
        "        input_ids = tokenizer.encode(p, return_tensors='pt').to(DEVICE)\n",
        "        out = model.generate(input_ids, max_new_tokens=60, temperature=0.6)\n",
        "        text = tokenizer.decode(out[0].tolist(), skip_special_tokens=True)\n",
        "        print(f\"üìù {text.strip()}\\n\" + \"-\"*30)\n",
        "\n",
        "    # D. Chart Telemetry (FIXED)\n",
        "    telemetry_path = os.path.join(CONSTANT_DIR, \"telemetry_constant.parquet\")\n",
        "    if not os.path.exists(telemetry_path):\n",
        "        print(\"‚ö†Ô∏è No telemetry parquet found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüìä Generating Telemetry Report...\")\n",
        "    df = pd.read_parquet(telemetry_path)\n",
        "\n",
        "    # --- SANITIZATION BLOCK ---\n",
        "    df = df.sort_values('step')\n",
        "    df = df.drop_duplicates(subset='step', keep='last') # Remove logging overlap\n",
        "    df = df.reset_index(drop=True) # Reset index to avoid collisions\n",
        "    # --------------------------\n",
        "\n",
        "    sa_cols = [c for c in df.columns if 'sigma_a' in c]\n",
        "    df['sigma_a_avg'] = df[sa_cols].mean(axis=1) if sa_cols else 0.0\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 14), sharex=True)\n",
        "\n",
        "    # 1. Loss\n",
        "    sns.lineplot(data=df, x='step', y='loss', ax=axes[0], label='Train Loss', color='tab:blue', alpha=0.5)\n",
        "    sns.lineplot(data=df, x='step', y='val_loss', ax=axes[0], label='Val Loss', color='tab:orange', linewidth=2)\n",
        "    axes[0].set_title(f\"Loss Trajectory (Current Val: {df['val_loss'].iloc[-1]:.3f})\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Sigma A\n",
        "    sns.lineplot(data=df, x='step', y='sigma_a_avg', ax=axes[1], color='tab:red', linewidth=2)\n",
        "    axes[1].set_title(f\"Head Redundancy (Current Avg: {df['sigma_a_avg'].iloc[-1]:.4f})\")\n",
        "    axes[1].set_ylabel(\"Sigma A (Lower is Better)\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].axhline(0.003, color='green', linestyle='--', label='Orthogonality Target')\n",
        "\n",
        "    # 3. Layer Redundancy\n",
        "    if sa_cols:\n",
        "        df_melt = df.melt(id_vars=['step'], value_vars=sa_cols, var_name='Layer', value_name='Sigma A')\n",
        "        sns.lineplot(data=df_melt, x='step', y='Sigma A', hue='Layer', ax=axes[2], palette='viridis', legend=False)\n",
        "        axes[2].set_title(\"Layer-wise Redundancy Dynamics\")\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(CONSTANT_DIR, f\"status_report_step_{ckpt_step}.png\")\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"‚úÖ Report saved to: {save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    audit_run()"
      ],
      "metadata": {
        "id": "8wqUPuPdqS8I",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Adaptive Scheduler 3700 steps\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CHUNKS_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_adaptive_v1\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üß† JANUS ADAPTIVE (Homeostatic Control)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class AdaptiveConfig:\n",
        "    def __init__(self):\n",
        "        # Model\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # Training\n",
        "        self.max_steps = 3700\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4\n",
        "\n",
        "        # Adaptive Controller\n",
        "        self.target_sigma = 0.0035\n",
        "        self.k_p = 0.5  # Gain\n",
        "        self.control_interval = 50 # Update every 50 steps\n",
        "        self.warmup_steps = 1500   # Ignition phase\n",
        "\n",
        "        # IO\n",
        "        self.ckpt_interval = 2000\n",
        "        self.chunk_steps = 500\n",
        "        self.lite_interval = 100\n",
        "\n",
        "# --- 4. THE HOMEOSTATIC CONTROLLER ---\n",
        "class Homeostat:\n",
        "    def __init__(self, config):\n",
        "        self.target = config.target_sigma\n",
        "        self.kp = config.k_p\n",
        "        self.warmup = config.warmup_steps\n",
        "        self.current_lambda = 0.0\n",
        "        self.history = []\n",
        "\n",
        "    def update(self, step, current_sigma):\n",
        "        # Phase 1: Ignition (Open Loop Ramp)\n",
        "        if step < self.warmup:\n",
        "            # Ramp from 0.00 to 0.05\n",
        "            self.current_lambda = 0.05 * (step / self.warmup)\n",
        "            return self.current_lambda\n",
        "\n",
        "        # Phase 2: Homeostasis (Closed Loop P-Control)\n",
        "        if current_sigma is None: return self.current_lambda\n",
        "\n",
        "        error = current_sigma - self.target\n",
        "        # CORRECTION: Positive Error (Too Redundant) -> INCREASE Pressure\n",
        "        delta = self.kp * error\n",
        "\n",
        "        # Apply & Clamp\n",
        "        self.current_lambda += delta\n",
        "        # Hard limits to prevent explosion or negative pressure\n",
        "        self.current_lambda = max(0.0, min(0.25, self.current_lambda))\n",
        "\n",
        "        return self.current_lambda\n",
        "\n",
        "# --- 5. ARCHITECTURE (Standard Janus v3) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        # Calculate steering if needed\n",
        "        # We need this during training AND during control steps\n",
        "        if (l_div > 0.0 and self.training) or return_metrics:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "\n",
        "            if l_div > 0.0:\n",
        "                steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                # Re-calc gram for metrics just to be safe/clean\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 6. DATA & LOGGING ---\n",
        "class ChunkLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.current_chunk_idx = -1\n",
        "        self.data = None\n",
        "        self.val_data = np.memmap(os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\"), dtype=np.uint16, mode='r')\n",
        "    def load_for_step(self, step):\n",
        "        target_chunk = step // self.config.chunk_steps\n",
        "        if target_chunk != self.current_chunk_idx:\n",
        "            fname = f\"train_chunk_{target_chunk:03d}.bin\"\n",
        "            fpath = os.path.join(CHUNKS_DIR, fname)\n",
        "            if not os.path.exists(fpath):\n",
        "                fallback_idx = target_chunk % 40\n",
        "                fpath = os.path.join(CHUNKS_DIR, f\"train_chunk_{fallback_idx:03d}.bin\")\n",
        "            with open(fpath, 'rb') as f:\n",
        "                self.data = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "            self.current_chunk_idx = target_chunk\n",
        "    def get_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "    def get_val_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.val_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.val_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.val_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "        self.start_step = 0\n",
        "    def set_start_step(self, step):\n",
        "        self.start_step = step\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        if step < self.start_step: return\n",
        "\n",
        "        # Calculate mean sigma_a across layers for logging\n",
        "        avg_sigma = 0.0\n",
        "        if metrics:\n",
        "            sigmas = [m['sigma_a'].mean().item() for m in metrics if 'sigma_a' in m]\n",
        "            if sigmas: avg_sigma = sum(sigmas) / len(sigmas)\n",
        "\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure,\n",
        "            \"sigma_a_avg\": avg_sigma\n",
        "        }\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_adaptive.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 7. RUN LOOP ---\n",
        "def run_adaptive():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = AdaptiveConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6.29e-4, weight_decay=1.34e-4)\n",
        "    controller = Homeostat(cfg)\n",
        "    loader = ChunkLoader(cfg)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    # START FRESH OR RESUME ADAPTIVE\n",
        "    # We do NOT recommend grafting 4k Constant here,\n",
        "    # better to start fresh 0-1500 warm-up to test the whole curve cleanly.\n",
        "\n",
        "    ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "    if ckpts:\n",
        "        ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "        latest = ckpts[-1]\n",
        "        ckpt_path = os.path.join(SAVE_DIR, latest)\n",
        "        print(f\"üîÑ Resuming Adaptive Run from {latest}...\")\n",
        "        c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(c['model'])\n",
        "        optimizer.load_state_dict(c['optim'])\n",
        "        # Restore Controller State\n",
        "        controller.current_lambda = c.get('current_lambda', 0.0)\n",
        "        start_step = c['step'] + 1\n",
        "        recorder.set_start_step(start_step)\n",
        "\n",
        "    print(f\"\\nüß† STARTING ADAPTIVE RUN: {start_step} -> {cfg.max_steps}\")\n",
        "    print(f\"   Target Sigma: {cfg.target_sigma}\")\n",
        "    print(f\"   Warmup Steps: {cfg.warmup_steps}\")\n",
        "\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    current_sigma_avg = None\n",
        "\n",
        "    for step in pbar:\n",
        "        loader.load_for_step(step)\n",
        "\n",
        "        # --- A. CONTROL UPDATE ---\n",
        "        # Update lambda based on LAST step's sigma metrics\n",
        "        if step % cfg.control_interval == 0 or step < cfg.warmup_steps:\n",
        "             new_lambda = controller.update(step, current_sigma_avg)\n",
        "\n",
        "        # Apply lambda to layers\n",
        "        # Simplified: Same lambda for div, 20% of that for coh\n",
        "        p = controller.current_lambda\n",
        "        base_coh = p * 0.2\n",
        "        lambdas_list = []\n",
        "        for i in range(cfg.n_layers):\n",
        "            # Ratio scaling still applies? Yes, keeps layers distinct.\n",
        "            ratio = (i + 1) / cfg.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "            lambdas_list.append((base_coh * s_mult, p * s_mult))\n",
        "\n",
        "        # --- B. TRAINING STEP ---\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        accum_sigmas = [] # Track sigma during accum for controller\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch(cfg.batch_size)\n",
        "\n",
        "            # Check if we need metrics for logging OR control\n",
        "            # We need metrics every 'control_interval' to feed the controller next step\n",
        "            is_control_step = (step % cfg.control_interval == 0)\n",
        "            is_log_step = (step % cfg.lite_interval == 0)\n",
        "            return_metrics = is_control_step or is_log_step\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas_list, y, return_metrics=return_metrics)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "            if return_metrics and metrics:\n",
        "                # Extract mean sigma for controller\n",
        "                s = [m['sigma_a'].mean().item() for m in metrics]\n",
        "                if s: accum_sigmas.append(sum(s)/len(s))\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update Sigma Average for Next Control Step\n",
        "        if accum_sigmas:\n",
        "            current_sigma_avg = sum(accum_sigmas) / len(accum_sigmas)\n",
        "\n",
        "        # --- C. TELEMETRY ---\n",
        "        if step > 0 and (step % cfg.lite_interval == 0 or step == cfg.max_steps - 1):\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(10): # Reduced val batches for speed\n",
        "                    vx, vy = loader.get_val_batch(cfg.batch_size)\n",
        "                    vl, _, _ = model(vx, [(0.0,0.0)]*cfg.n_layers, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, controller.current_lambda, metrics)\n",
        "            recorder.flush()\n",
        "\n",
        "            # Update Progress Bar with Adaptive Info\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|V:{val_loss:.2f}|P:{controller.current_lambda:.3f}|Sig:{current_sigma_avg:.4f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|P:{controller.current_lambda:.3f}|Sig:{current_sigma_avg if current_sigma_avg else 0:.4f}\")\n",
        "\n",
        "        # --- D. CHECKPOINTS ---\n",
        "        if step > 0 and step % cfg.ckpt_interval == 0:\n",
        "            ckpt_name = f\"ckpt_step_{step}.pt\"\n",
        "            ckpt_path = os.path.join(SAVE_DIR, ckpt_name)\n",
        "            save_dict = {\n",
        "                'step': step,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'current_lambda': controller.current_lambda, # Save controller state\n",
        "                'rng_cpu': torch.get_rng_state(),\n",
        "                'rng_gpu': torch.cuda.get_rng_state()\n",
        "            }\n",
        "            torch.save(save_dict, ckpt_path)\n",
        "\n",
        "            all_ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "            all_ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "            if len(all_ckpts) > 3: os.remove(os.path.join(SAVE_DIR, all_ckpts[0]))\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_adaptive_final.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ ADAPTIVE RUN COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_adaptive()"
      ],
      "metadata": {
        "id": "8JGDPQof2w-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìä Janus Telemetry Audit (Sanitized)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. FORCE MOUNT ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"üîå Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"‚úÖ Drive already mounted.\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_adaptive_v1\")\n",
        "TELEMETRY_FILENAME = \"telemetry_adaptive.parquet\"\n",
        "TELEMETRY_PATH = os.path.join(SAVE_DIR, TELEMETRY_FILENAME)\n",
        "\n",
        "EPOCH_STEPS = 3051\n",
        "\n",
        "# --- 2. VERIFY & LOAD ---\n",
        "if not os.path.exists(TELEMETRY_PATH):\n",
        "    print(f\"\\n‚ùå CRITICAL: File not found at: {TELEMETRY_PATH}\")\n",
        "    print(\"üõë Aborting.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found telemetry file!\")\n",
        "    df = pd.read_parquet(TELEMETRY_PATH)\n",
        "\n",
        "    # --- 3. SANITATION (The Fix) ---\n",
        "    # The resume logic created duplicate steps. We must remove them.\n",
        "    original_len = len(df)\n",
        "\n",
        "    # Sort by step to ensure order\n",
        "    df = df.sort_values('step')\n",
        "\n",
        "    # Drop duplicates, keeping the LAST entry (most recent run data)\n",
        "    df = df.drop_duplicates(subset=['step'], keep='last')\n",
        "\n",
        "    # Reset the index to be perfectly sequential and unique\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    dropped_count = original_len - len(df)\n",
        "    if dropped_count > 0:\n",
        "        print(f\"üßπ SANITIZED: Removed {dropped_count} duplicate rows caused by run resumption.\")\n",
        "\n",
        "    # --- 4. PRE-PROCESSING ---\n",
        "    # Now safe to calculate rolling averages\n",
        "    df['loss_smooth'] = df['loss'].rolling(window=5, center=True).mean()\n",
        "\n",
        "    # Identify Global Minima\n",
        "    min_val_row = df.loc[df['val_loss'].idxmin()]\n",
        "    min_val_step = min_val_row['step']\n",
        "    min_val_loss = min_val_row['val_loss']\n",
        "\n",
        "    # --- 5. VISUALIZATION ---\n",
        "    plt.style.use('dark_background')\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), sharex=True)\n",
        "\n",
        "    # PLOT 1: Convergence\n",
        "    sns.lineplot(data=df, x='step', y='loss', ax=ax1, label='Raw Train Loss', alpha=0.3, color='cyan')\n",
        "    sns.lineplot(data=df, x='step', y='loss_smooth', ax=ax1, label='Smoothed Train', color='cyan', linewidth=2)\n",
        "    sns.lineplot(data=df, x='step', y='val_loss', ax=ax1, label='Validation Loss', color='magenta', linewidth=2, marker='o')\n",
        "\n",
        "    # Markers\n",
        "    ax1.axvline(x=EPOCH_STEPS, color='yellow', linestyle='--', alpha=0.6, label='Epoch 1 Boundary')\n",
        "    ax1.text(EPOCH_STEPS+50, ax1.get_ylim()[0], 'Epoch 2 Start', color='yellow', rotation=90)\n",
        "    ax1.axvline(x=min_val_step, color='lime', linestyle=':', linewidth=2, label=f'Best Val (Step {int(min_val_step)})')\n",
        "\n",
        "    ax1.set_title(f\"DATA WALL AUDIT: Best Val {min_val_loss:.3f} @ Step {int(min_val_step)}\", fontsize=14)\n",
        "    ax1.set_ylabel(\"Cross Entropy Loss\")\n",
        "    ax1.legend()\n",
        "\n",
        "    # PLOT 2: Homeostasis\n",
        "    ax2_twin = ax2.twinx()\n",
        "    sns.lineplot(data=df, x='step', y='pressure', ax=ax2, color='orange', label='Pressure (Lambda)')\n",
        "    sns.lineplot(data=df, x='step', y='sigma_a_avg', ax=ax2_twin, color='green', label='Sigma', linestyle='--')\n",
        "\n",
        "    ax2_twin.axhline(y=0.0035, color='green', linestyle=':', alpha=0.5, label='Target Sigma')\n",
        "\n",
        "    ax2.set_ylabel(\"Pressure\", color='orange')\n",
        "    ax2_twin.set_ylabel(\"Sigma\", color='green')\n",
        "    ax2.set_xlabel(\"Steps\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- 6. REPORT ---\n",
        "    print(\"\\nüîé RECENT DATA (Last 8 Unique Steps):\")\n",
        "    print(df[['step', 'loss', 'val_loss', 'pressure']].tail(8).to_string(index=False))"
      ],
      "metadata": {
        "id": "O1UN2wzzzQ9g",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Analysis] Model Forensics - Adaptive vs Control\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"üî¨ MODEL FORENSICS SUITE\")\n",
        "print(f\"‚öôÔ∏è Device: {DEVICE}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# ARCHITECTURE DEFINITIONS (Copied for standalone operation)\n",
        "# ============================================================================\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config['n_heads']\n",
        "        self.d_head = config['d_head']\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        d_model = config['d_model']\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config['d_head'], config['max_seq_len']))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, return_head_outputs=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        out = self.o_proj(out)\n",
        "\n",
        "        if return_head_outputs:\n",
        "            return out, head_out, attn_probs\n",
        "        return out\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config['d_model'])\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config['d_model'])\n",
        "        self.mlp = SwiGLU(config['d_model'])\n",
        "\n",
        "    def forward(self, x, return_head_outputs=False):\n",
        "        if return_head_outputs:\n",
        "            a, head_out, attn_probs = self.attn(self.ln1(x), return_head_outputs=True)\n",
        "            x = x + a\n",
        "            x = x + self.mlp(self.ln2(x))\n",
        "            return x, head_out, attn_probs\n",
        "        else:\n",
        "            a = self.attn(self.ln1(x))\n",
        "            x = x + a\n",
        "            x = x + self.mlp(self.ln2(x))\n",
        "            return x\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.ln_f = RMSNorm(config['d_model'])\n",
        "        self.head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx, return_head_outputs=False):\n",
        "        x = self.token_emb(idx)\n",
        "        head_outputs = []\n",
        "        attn_patterns = []\n",
        "\n",
        "        for block in self.blocks:\n",
        "            if return_head_outputs:\n",
        "                x, h_out, a_probs = block(x, return_head_outputs=True)\n",
        "                head_outputs.append(h_out)\n",
        "                attn_patterns.append(a_probs)\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        if return_head_outputs:\n",
        "            return logits, head_outputs, attn_patterns\n",
        "        return logits\n",
        "\n",
        "# ============================================================================\n",
        "# FORENSICS SUITE\n",
        "# ============================================================================\n",
        "\n",
        "class ModelForensics:\n",
        "    def __init__(self, model_a_path, model_b_path, model_a_name=\"Adaptive\", model_b_name=\"Control\"):\n",
        "        self.config = {\n",
        "            'vocab_size': 50257,\n",
        "            'd_model': 512,\n",
        "            'n_layers': 12,\n",
        "            'n_heads': 8,\n",
        "            'd_head': 64,\n",
        "            'max_seq_len': 512,\n",
        "            'dropout': 0.0  # Set to 0 for deterministic analysis\n",
        "        }\n",
        "\n",
        "        print(f\"üìÇ Loading models...\")\n",
        "        self.model_a = NewGPT(self.config).to(DEVICE)\n",
        "        self.model_b = NewGPT(self.config).to(DEVICE)\n",
        "\n",
        "        # Load state dicts\n",
        "        state_a = torch.load(model_a_path, map_location=DEVICE)\n",
        "        state_b = torch.load(model_b_path, map_location=DEVICE)\n",
        "\n",
        "        self.model_a.load_state_dict(state_a)\n",
        "        self.model_b.load_state_dict(state_b)\n",
        "\n",
        "        self.model_a.eval()\n",
        "        self.model_b.eval()\n",
        "\n",
        "        self.model_a_name = model_a_name\n",
        "        self.model_b_name = model_b_name\n",
        "\n",
        "        self.results = {}\n",
        "        print(f\"‚úÖ Models loaded: {model_a_name} vs {model_b_name}\\n\")\n",
        "\n",
        "    def compare_weight_statistics(self):\n",
        "        \"\"\"Section 1: Compare raw weight distributions\"\"\"\n",
        "        print(\"üîç Section 1: Weight Statistics Analysis\")\n",
        "\n",
        "        stats_data = []\n",
        "\n",
        "        for name, param_a in self.model_a.named_parameters():\n",
        "            param_b = dict(self.model_b.named_parameters())[name]\n",
        "\n",
        "            a_flat = param_a.detach().cpu().flatten().numpy()\n",
        "            b_flat = param_b.detach().cpu().flatten().numpy()\n",
        "\n",
        "            stats_data.append({\n",
        "                'parameter': name,\n",
        "                f'{self.model_a_name}_mean': np.mean(a_flat),\n",
        "                f'{self.model_b_name}_mean': np.mean(b_flat),\n",
        "                f'{self.model_a_name}_std': np.std(a_flat),\n",
        "                f'{self.model_b_name}_std': np.std(b_flat),\n",
        "                f'{self.model_a_name}_norm': np.linalg.norm(a_flat),\n",
        "                f'{self.model_b_name}_norm': np.linalg.norm(b_flat),\n",
        "                f'{self.model_a_name}_sparsity': np.mean(np.abs(a_flat) < 0.01),\n",
        "                f'{self.model_b_name}_sparsity': np.mean(np.abs(b_flat) < 0.01),\n",
        "                'diff_norm': np.linalg.norm(a_flat - b_flat),\n",
        "                'cosine_sim': np.dot(a_flat, b_flat) / (np.linalg.norm(a_flat) * np.linalg.norm(b_flat))\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(stats_data)\n",
        "        self.results['weight_stats'] = df\n",
        "\n",
        "        # Summary statistics\n",
        "        print(f\"   Total parameters: {sum([p.numel() for p in self.model_a.parameters()]):,}\")\n",
        "        print(f\"   Mean weight difference norm: {df['diff_norm'].mean():.6f}\")\n",
        "        print(f\"   Mean cosine similarity: {df['cosine_sim'].mean():.6f}\")\n",
        "        print(f\"   Parameters analyzed: {len(df)}\\n\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def analyze_attention_heads(self, num_samples=100, seq_len=128):\n",
        "        \"\"\"Section 2: Deep dive into attention head properties\"\"\"\n",
        "        print(\"üîç Section 2: Attention Head Analysis\")\n",
        "\n",
        "        # Generate test inputs\n",
        "        test_inputs = torch.randint(0, self.config['vocab_size'], (num_samples, seq_len)).to(DEVICE)\n",
        "\n",
        "        head_metrics_a = []\n",
        "        head_metrics_b = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get head outputs from both models\n",
        "            _, heads_a, attn_a = self.model_a(test_inputs, return_head_outputs=True)\n",
        "            _, heads_b, attn_b = self.model_b(test_inputs, return_head_outputs=True)\n",
        "\n",
        "            # Analyze each layer\n",
        "            for layer_idx in range(self.config['n_layers']):\n",
        "                h_a = heads_a[layer_idx]  # [B, n_heads, S, d_head]\n",
        "                h_b = heads_b[layer_idx]\n",
        "\n",
        "                # Compute pairwise head similarity (sigma_a metric)\n",
        "                for model_heads, model_name, metric_list in [\n",
        "                    (h_a, self.model_a_name, head_metrics_a),\n",
        "                    (h_b, self.model_b_name, head_metrics_b)\n",
        "                ]:\n",
        "                    # Flatten to [n_heads, B*S*d_head]\n",
        "                    flat = model_heads.transpose(0, 1).reshape(self.config['n_heads'], -1)\n",
        "                    norm = F.normalize(flat, p=2, dim=1)\n",
        "\n",
        "                    # Compute similarity matrix\n",
        "                    sim_matrix = torch.mm(norm, norm.t()).cpu().numpy()\n",
        "\n",
        "                    # Off-diagonal similarities\n",
        "                    mask = ~np.eye(self.config['n_heads'], dtype=bool)\n",
        "                    off_diag = sim_matrix[mask]\n",
        "\n",
        "                    metric_list.append({\n",
        "                        'layer': layer_idx,\n",
        "                        'model': model_name,\n",
        "                        'mean_similarity': np.mean(np.abs(off_diag)),\n",
        "                        'max_similarity': np.max(np.abs(off_diag)),\n",
        "                        'min_similarity': np.min(np.abs(off_diag)),\n",
        "                        'std_similarity': np.std(off_diag),\n",
        "                        'similarity_matrix': sim_matrix\n",
        "                    })\n",
        "\n",
        "                # Attention pattern entropy\n",
        "                for attn_probs, model_name in [(attn_a[layer_idx], self.model_a_name),\n",
        "                                                 (attn_b[layer_idx], self.model_b_name)]:\n",
        "                    # attn_probs: [B, n_heads, S, S]\n",
        "                    entropy = -(attn_probs * torch.log(attn_probs + 1e-9)).sum(dim=-1).mean(dim=[0, 2])\n",
        "                    # entropy: [n_heads]\n",
        "\n",
        "                    for h_idx, ent in enumerate(entropy.cpu().numpy()):\n",
        "                        metric_list = head_metrics_a if model_name == self.model_a_name else head_metrics_b\n",
        "                        metric_list[layer_idx][f'head_{h_idx}_entropy'] = ent\n",
        "\n",
        "        df_a = pd.DataFrame(head_metrics_a)\n",
        "        df_b = pd.DataFrame(head_metrics_b)\n",
        "        df_combined = pd.concat([df_a, df_b])\n",
        "\n",
        "        self.results['head_metrics'] = df_combined\n",
        "        self.results['head_similarity_matrices_a'] = [m['similarity_matrix'] for m in head_metrics_a]\n",
        "        self.results['head_similarity_matrices_b'] = [m['similarity_matrix'] for m in head_metrics_b]\n",
        "\n",
        "        # Summary\n",
        "        avg_sim_a = df_a['mean_similarity'].mean()\n",
        "        avg_sim_b = df_b['mean_similarity'].mean()\n",
        "\n",
        "        print(f\"   {self.model_a_name} mean head similarity (œÉ_a): {avg_sim_a:.6f}\")\n",
        "        print(f\"   {self.model_b_name} mean head similarity (œÉ_a): {avg_sim_b:.6f}\")\n",
        "        print(f\"   Difference: {abs(avg_sim_a - avg_sim_b):.6f}\")\n",
        "        print(f\"   Lower is more diverse (target was 0.0035)\\n\")\n",
        "\n",
        "        return df_combined\n",
        "\n",
        "    def measure_loss_landscape(self, val_data_path, num_points=11, num_batches=20):\n",
        "        \"\"\"Section 3: Loss landscape interpolation between models\"\"\"\n",
        "        print(\"üîç Section 3: Loss Landscape Interpolation\")\n",
        "\n",
        "        # Load validation data\n",
        "        val_data = np.memmap(val_data_path, dtype=np.uint16, mode='r')\n",
        "\n",
        "        alphas = np.linspace(0, 1, num_points)\n",
        "        losses = []\n",
        "\n",
        "        for alpha in tqdm(alphas, desc=\"Interpolating\"):\n",
        "            # Create interpolated model: theta = alpha*A + (1-alpha)*B\n",
        "            interp_model = NewGPT(self.config).to(DEVICE)\n",
        "\n",
        "            state_dict_interp = {}\n",
        "            for name in self.model_a.state_dict().keys():\n",
        "                state_dict_interp[name] = (\n",
        "                    alpha * self.model_a.state_dict()[name] +\n",
        "                    (1 - alpha) * self.model_b.state_dict()[name]\n",
        "                )\n",
        "\n",
        "            interp_model.load_state_dict(state_dict_interp)\n",
        "            interp_model.eval()\n",
        "\n",
        "            # Compute loss\n",
        "            batch_losses = []\n",
        "            with torch.no_grad():\n",
        "                for _ in range(num_batches):\n",
        "                    ix = np.random.randint(0, len(val_data) - self.config['max_seq_len'])\n",
        "                    x = torch.from_numpy(val_data[ix:ix+self.config['max_seq_len']].astype(np.int64)).unsqueeze(0).to(DEVICE)\n",
        "                    y = torch.from_numpy(val_data[ix+1:ix+1+self.config['max_seq_len']].astype(np.int64)).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "                    logits = interp_model(x)\n",
        "                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "                    batch_losses.append(loss.item())\n",
        "\n",
        "            losses.append(np.mean(batch_losses))\n",
        "\n",
        "        self.results['landscape'] = {'alphas': alphas, 'losses': losses}\n",
        "\n",
        "        print(f\"   Loss at {self.model_a_name} (Œ±=1): {losses[-1]:.4f}\")\n",
        "        print(f\"   Loss at {self.model_b_name} (Œ±=0): {losses[0]:.4f}\")\n",
        "        print(f\"   Max loss along path: {max(losses):.4f}\")\n",
        "        print(f\"   Barrier height: {max(losses) - min(losses):.4f}\\n\")\n",
        "\n",
        "        return alphas, losses\n",
        "\n",
        "    def compute_effective_rank(self):\n",
        "        \"\"\"Section 4: Effective rank of weight matrices\"\"\"\n",
        "        print(\"üîç Section 4: Effective Rank Analysis\")\n",
        "\n",
        "        rank_data = []\n",
        "\n",
        "        for name, param_a in self.model_a.named_parameters():\n",
        "            if len(param_a.shape) != 2:  # Only analyze 2D weight matrices\n",
        "                continue\n",
        "\n",
        "            param_b = dict(self.model_b.named_parameters())[name]\n",
        "\n",
        "            # Compute SVD\n",
        "            U_a, S_a, V_a = torch.svd(param_a.cpu())\n",
        "            U_b, S_b, V_b = torch.svd(param_b.cpu())\n",
        "\n",
        "            # Effective rank: exp(entropy of normalized singular values)\n",
        "            def eff_rank(S):\n",
        "                S_norm = S / S.sum()\n",
        "                entropy = -(S_norm * torch.log(S_norm + 1e-9)).sum()\n",
        "                return torch.exp(entropy).item()\n",
        "\n",
        "            rank_data.append({\n",
        "                'parameter': name,\n",
        "                f'{self.model_a_name}_eff_rank': eff_rank(S_a),\n",
        "                f'{self.model_b_name}_eff_rank': eff_rank(S_b),\n",
        "                f'{self.model_a_name}_top_sv': S_a[0].item(),\n",
        "                f'{self.model_b_name}_top_sv': S_b[0].item(),\n",
        "                f'{self.model_a_name}_condition': (S_a[0] / S_a[-1]).item(),\n",
        "                f'{self.model_b_name}_condition': (S_b[0] / S_b[-1]).item(),\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(rank_data)\n",
        "        self.results['effective_rank'] = df\n",
        "\n",
        "        print(f\"   Mean effective rank {self.model_a_name}: {df[f'{self.model_a_name}_eff_rank'].mean():.2f}\")\n",
        "        print(f\"   Mean effective rank {self.model_b_name}: {df[f'{self.model_b_name}_eff_rank'].mean():.2f}\")\n",
        "        print(f\"   Mean condition number {self.model_a_name}: {df[f'{self.model_a_name}_condition'].mean():.2f}\")\n",
        "        print(f\"   Mean condition number {self.model_b_name}: {df[f'{self.model_b_name}_condition'].mean():.2f}\\n\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def generate_visualizations(self, save_dir):\n",
        "        \"\"\"Create comprehensive visualization suite\"\"\"\n",
        "        print(\"üìä Generating Visualizations...\")\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Head Similarity Heatmaps\n",
        "        fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "        fig.suptitle(f'Attention Head Similarity Matrices (Layer 0-11)', fontsize=16)\n",
        "\n",
        "        for layer_idx in range(12):\n",
        "            ax = axes[layer_idx // 4, layer_idx % 4]\n",
        "\n",
        "            sim_a = self.results['head_similarity_matrices_a'][layer_idx]\n",
        "            sim_b = self.results['head_similarity_matrices_b'][layer_idx]\n",
        "\n",
        "            # Show difference\n",
        "            diff = np.abs(sim_a) - np.abs(sim_b)\n",
        "\n",
        "            im = ax.imshow(diff, cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n",
        "            ax.set_title(f'Layer {layer_idx}\\n({self.model_a_name} - {self.model_b_name})')\n",
        "            ax.set_xlabel('Head')\n",
        "            ax.set_ylabel('Head')\n",
        "            plt.colorbar(im, ax=ax)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'head_similarity_heatmaps.png'), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Mean Head Similarity by Layer\n",
        "        df = self.results['head_metrics']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        for model_name in [self.model_a_name, self.model_b_name]:\n",
        "            data = df[df['model'] == model_name]\n",
        "            ax.plot(data['layer'], data['mean_similarity'], marker='o', label=model_name, linewidth=2)\n",
        "\n",
        "        ax.axhline(y=0.0035, color='red', linestyle='--', label='Target œÉ_a = 0.0035')\n",
        "        ax.set_xlabel('Layer', fontsize=12)\n",
        "        ax.set_ylabel('Mean Head Similarity (œÉ_a)', fontsize=12)\n",
        "        ax.set_title('Attention Head Redundancy by Layer', fontsize=14)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'head_similarity_by_layer.png'), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Loss Landscape\n",
        "        if 'landscape' in self.results:\n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            alphas = self.results['landscape']['alphas']\n",
        "            losses = self.results['landscape']['losses']\n",
        "\n",
        "            ax.plot(alphas, losses, marker='o', linewidth=2, markersize=8)\n",
        "            ax.axvline(x=0, color='blue', linestyle='--', alpha=0.5, label=self.model_b_name)\n",
        "            ax.axvline(x=1, color='orange', linestyle='--', alpha=0.5, label=self.model_a_name)\n",
        "            ax.set_xlabel(f'Œ± (0={self.model_b_name}, 1={self.model_a_name})', fontsize=12)\n",
        "            ax.set_ylabel('Validation Loss', fontsize=12)\n",
        "            ax.set_title('Loss Landscape: Linear Interpolation Between Models', fontsize=14)\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, 'loss_landscape.png'), dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "        # 4. Weight Distribution Comparison (sample)\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "        fig.suptitle('Weight Distribution Comparison (Sample Layers)', fontsize=16)\n",
        "\n",
        "        sample_params = [\n",
        "            'blocks.0.attn.q_proj.weight',\n",
        "            'blocks.0.attn.o_proj.weight',\n",
        "            'blocks.6.attn.q_proj.weight',\n",
        "            'blocks.6.attn.o_proj.weight',\n",
        "            'blocks.11.attn.q_proj.weight',\n",
        "            'blocks.11.attn.o_proj.weight'\n",
        "        ]\n",
        "\n",
        "        for idx, param_name in enumerate(sample_params):\n",
        "            ax = axes[idx // 3, idx % 3]\n",
        "\n",
        "            param_a = dict(self.model_a.named_parameters())[param_name].detach().cpu().flatten().numpy()\n",
        "            param_b = dict(self.model_b.named_parameters())[param_name].detach().cpu().flatten().numpy()\n",
        "\n",
        "            ax.hist(param_a, bins=50, alpha=0.5, label=self.model_a_name, density=True)\n",
        "            ax.hist(param_b, bins=50, alpha=0.5, label=self.model_b_name, density=True)\n",
        "            ax.set_title(param_name.replace('blocks.', 'L').replace('.attn.', ' '), fontsize=10)\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'weight_distributions.png'), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"‚úÖ Visualizations saved to {save_dir}\\n\")\n",
        "\n",
        "    def generate_report(self, save_dir):\n",
        "        \"\"\"Generate comprehensive text report\"\"\"\n",
        "        print(\"üìù Generating Report...\")\n",
        "\n",
        "        report_path = os.path.join(save_dir, 'forensics_report.txt')\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(\"MODEL FORENSICS REPORT\\n\")\n",
        "            f.write(f\"{self.model_a_name} vs {self.model_b_name}\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "            # Section 1: Weight Statistics\n",
        "            f.write(\"SECTION 1: WEIGHT STATISTICS\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "            df = self.results['weight_stats']\n",
        "            f.write(f\"Total Parameters: {sum([p.numel() for p in self.model_a.parameters()]):,}\\n\")\n",
        "            f.write(f\"Mean Weight Difference Norm: {df['diff_norm'].mean():.6f}\\n\")\n",
        "            f.write(f\"Mean Cosine Similarity: {df['cosine_sim'].mean():.6f}\\n\")\n",
        "            f.write(f\"Std Cosine Similarity: {df['cosine_sim'].std():.6f}\\n\\n\")\n",
        "\n",
        "            # Section 2: Head Analysis\n",
        "            f.write(\"SECTION 2: ATTENTION HEAD ANALYSIS\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "            df = self.results['head_metrics']\n",
        "            for model_name in [self.model_a_name, self.model_b_name]:\n",
        "                data = df[df['model'] == model_name]\n",
        "                f.write(f\"\\n{model_name}:\\n\")\n",
        "                f.write(f\"  Mean Head Similarity (œÉ_a): {data['mean_similarity'].mean():.6f}\\n\")\n",
        "                f.write(f\"  Std Head Similarity: {data['mean_similarity'].std():.6f}\\n\")\n",
        "                f.write(f\"  Max Head Similarity: {data['max_similarity'].mean():.6f}\\n\")\n",
        "                f.write(f\"  Min Head Similarity: {data['min_similarity'].mean():.6f}\\n\")\n",
        "\n",
        "            f.write(f\"\\nTarget œÉ_a: 0.0035\\n\")\n",
        "            f.write(f\"Difference in œÉ_a: {abs(df[df['model']==self.model_a_name]['mean_similarity'].mean() - df[df['model']==self.model_b_name]['mean_similarity'].mean()):.6f}\\n\\n\")\n",
        "\n",
        "            # Section 3: Loss Landscape\n",
        "            if 'landscape' in self.results:\n",
        "                f.write(\"SECTION 3: LOSS LANDSCAPE\\n\")\n",
        "                f.write(\"-\"*80 + \"\\n\")\n",
        "                losses = self.results['landscape']['losses']\n",
        "                f.write(f\"Loss at {self.model_a_name}: {losses[-1]:.4f}\\n\")\n",
        "                f.write(f\"Loss at {self.model_b_name}: {losses[0]:.4f}\\n\")\n",
        "                f.write(f\"Max Loss Along Path: {max(losses):.4f}\\n\")\n",
        "                f.write(f\"Barrier Height: {max(losses) - min(losses):.4f}\\n\")\n",
        "                f.write(f\"Path Smoothness: {'Smooth' if max(losses) - min(losses) < 0.1 else 'Rough'}\\n\\n\")\n",
        "\n",
        "            # Section 4: Effective Rank\n",
        "            if 'effective_rank' in self.results:\n",
        "                f.write(\"SECTION 4: EFFECTIVE RANK\\n\")\n",
        "                f.write(\"-\"*80 + \"\\n\")\n",
        "                df = self.results['effective_rank']\n",
        "                f.write(f\"{self.model_a_name} Mean Effective Rank: {df[f'{self.model_a_name}_eff_rank'].mean():.2f}\\n\")\n",
        "                f.write(f\"{self.model_b_name} Mean Effective Rank: {df[f'{self.model_b_name}_eff_rank'].mean():.2f}\\n\")\n",
        "                f.write(f\"{self.model_a_name} Mean Condition Number: {df[f'{self.model_a_name}_condition'].mean():.2f}\\n\")\n",
        "                f.write(f\"{self.model_b_name} Mean Condition Number: {df[f'{self.model_b_name}_condition'].mean():.2f}\\n\\n\")\n",
        "\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(\"END REPORT\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        print(f\"‚úÖ Report saved to {report_path}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths\n",
        "    MODEL_ADAPTIVE_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_adaptive_v1/janus_adaptive_final.pt\")\n",
        "    MODEL_CONTROL_PATH = os.path.join(PROJECT_ROOT, \"data/models/janus_control_v1/janus_control_final.pt\")\n",
        "    VAL_DATA_PATH = os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\")\n",
        "    RESULTS_DIR = os.path.join(PROJECT_ROOT, \"forensics_results\")\n",
        "\n",
        "    # Initialize Forensics\n",
        "    forensics = ModelForensics(\n",
        "        MODEL_ADAPTIVE_PATH,\n",
        "        MODEL_CONTROL_PATH,\n",
        "        model_a_name=\"Adaptive\",\n",
        "        model_b_name=\"Control\"\n",
        "    )\n",
        "\n",
        "    # Run analysis pipeline\n",
        "    forensics.compare_weight_statistics()\n",
        "    forensics.analyze_attention_heads(num_samples=50, seq_len=128)\n",
        "\n",
        "    if os.path.exists(VAL_DATA_PATH):\n",
        "        forensics.measure_loss_landscape(VAL_DATA_PATH, num_points=11)\n",
        "\n",
        "    forensics.compute_effective_rank()\n",
        "\n",
        "    # Generate outputs\n",
        "    forensics.generate_visualizations(RESULTS_DIR)\n",
        "    forensics.generate_report(RESULTS_DIR)\n",
        "\n",
        "    print(\"üèÅ Forensics analysis complete.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VHVWxwUgEXAA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Omnibus] Deep Model Forensics - Geometric & Information-Theoretic Analysis\n",
        "# Optimized for L4 GPU | Project Janus XAI Suite\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================================================\n",
        "# 1. SETUP & PATHS\n",
        "# ============================================================================\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Paths based on user specification\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "PATH_ADAPTIVE = os.path.join(PROJECT_ROOT, \"data/models/janus_adaptive_v1/janus_adaptive_final.pt\")\n",
        "PATH_CONTROL = os.path.join(PROJECT_ROOT, \"data/models/janus_control_v1/janus_control_final.pt\")\n",
        "PATH_VAL_DATA = os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\")\n",
        "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"omnibus_forensics_results\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üî¨ INITIALIZING OMNIBUS SUITE\")\n",
        "print(f\"‚öôÔ∏è Device: {DEVICE}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ARCHITECTURE DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads, self.d_head = config['n_heads'], config['d_head']\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        d_model = config['d_model']\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(self.d_head, config['max_seq_len']))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1).to(x.device)\n",
        "        return torch.view_as_real(x_c * freqs).flatten(3).type_as(x)\n",
        "\n",
        "    def forward(self, x, return_internals=False):\n",
        "        B, S, D = x.shape\n",
        "        q, k, v = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
        "        q = self.apply_rope(q.view(B, S, self.n_heads, self.d_head), self.freqs_cis)\n",
        "        k = self.apply_rope(k.view(B, S, self.n_heads, self.d_head), self.freqs_cis)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn_probs = F.softmax(attn.masked_fill(mask == 0, float('-inf')), dim=-1)\n",
        "        head_out = attn_probs @ v\n",
        "        out = self.o_proj(head_out.transpose(1, 2).reshape(B, S, D))\n",
        "\n",
        "        if return_internals: return out, head_out, attn_probs\n",
        "        return out\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1, self.attn = RMSNorm(config['d_model']), NewAttention(config)\n",
        "        self.ln2, self.mlp = RMSNorm(config['d_model']), SwiGLU(config['d_model'])\n",
        "\n",
        "    def forward(self, x, return_internals=False):\n",
        "        if return_internals:\n",
        "            a, h_out, a_probs = self.attn(self.ln1(x), return_internals=True)\n",
        "            x = x + a\n",
        "            m_out = self.mlp(self.ln2(x))\n",
        "            x = x + m_out\n",
        "            return x, h_out, a_probs, m_out\n",
        "        return x + self.attn(self.ln1(x)) + self.mlp(self.ln2(x))\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.ln_f, self.head = RMSNorm(config['d_model']), nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx, return_internals=False):\n",
        "        x = self.token_emb(idx)\n",
        "        internals = {'heads': [], 'attns': [], 'mlps': []}\n",
        "        for block in self.blocks:\n",
        "            if return_internals:\n",
        "                x, h, a, m = block(x, return_internals=True)\n",
        "                internals['heads'].append(h); internals['attns'].append(a); internals['mlps'].append(m)\n",
        "            else: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return (logits, internals) if return_internals else logits\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def get_batch(data_path, batch_size=32, seq_len=128):\n",
        "    data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+seq_len]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+seq_len+1]).astype(np.int64)) for i in ix])\n",
        "    return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. OMNIBUS ANALYZER\n",
        "# ============================================================================\n",
        "\n",
        "class OmnibusForensics:\n",
        "    def __init__(self, path_a, path_b):\n",
        "        self.config = {'vocab_size': 50257, 'd_model': 512, 'n_layers': 12, 'n_heads': 8, 'd_head': 64, 'max_seq_len': 512, 'dropout': 0.0}\n",
        "        self.model_a = NewGPT(self.config).to(DEVICE).eval()\n",
        "        self.model_b = NewGPT(self.config).to(DEVICE).eval()\n",
        "\n",
        "        print(f\"üìÇ Loading Adaptive: {path_a}\")\n",
        "        self.model_a.load_state_dict(torch.load(path_a, map_location=DEVICE))\n",
        "        print(f\"üìÇ Loading Control:  {path_b}\")\n",
        "        self.model_b.load_state_dict(torch.load(path_b, map_location=DEVICE))\n",
        "        self.results = {}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_cka(self, X):\n",
        "        \"\"\"Centered Kernel Alignment for representational similarity.\"\"\"\n",
        "        def linear_HSIC(K, L):\n",
        "            n = K.shape[0]\n",
        "            H = torch.eye(n, device=DEVICE) - (1/n) * torch.ones((n, n), device=DEVICE)\n",
        "            K_centered = H @ K @ H\n",
        "            L_centered = H @ L @ H\n",
        "            return (K_centered * L_centered).sum() / ((n-1)**2)\n",
        "\n",
        "        _, int_a = self.model_a(X, return_internals=True)\n",
        "        _, int_b = self.model_b(X, return_internals=True)\n",
        "\n",
        "        cka_layers = []\n",
        "        for i in range(self.config['n_layers']):\n",
        "            feat_a = int_a['mlps'][i].mean(dim=1) # Average over sequence\n",
        "            feat_b = int_b['mlps'][i].mean(dim=1)\n",
        "\n",
        "            K = feat_a @ feat_a.T\n",
        "            L = feat_b @ feat_b.T\n",
        "\n",
        "            hsic_kl = linear_HSIC(K, L)\n",
        "            hsic_kk = linear_HSIC(K, K)\n",
        "            hsic_ll = linear_HSIC(L, L)\n",
        "            cka_val = hsic_kl / (torch.sqrt(hsic_kk) * torch.sqrt(hsic_ll) + 1e-9)\n",
        "            cka_layers.append(cka_val.item())\n",
        "\n",
        "        self.results['cka'] = cka_layers\n",
        "        return cka_layers\n",
        "\n",
        "    def geometric_drill(self):\n",
        "        \"\"\"Effective Rank and Spectral Analysis.\"\"\"\n",
        "        metrics = []\n",
        "        for name, p_a in self.model_a.named_parameters():\n",
        "            if 'weight' in name and len(p_a.shape) == 2:\n",
        "                p_b = dict(self.model_b.named_parameters())[name]\n",
        "                for p, m_name in [(p_a, 'Adaptive'), (p_b, 'Control')]:\n",
        "                    s = torch.linalg.svdvals(p.float())\n",
        "                    s_norm = s / (s.sum() + 1e-9)\n",
        "                    eff_rank = torch.exp(-(s_norm * torch.log(s_norm + 1e-9)).sum()).item()\n",
        "                    metrics.append({\n",
        "                        'param': name, 'model': m_name,\n",
        "                        'eff_rank': eff_rank,\n",
        "                        'cond': (s[0]/(s[-1] + 1e-9)).item()\n",
        "                    })\n",
        "        self.results['geometry'] = pd.DataFrame(metrics)\n",
        "        return self.results['geometry']\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def analyze_head_synergy(self, X):\n",
        "        \"\"\"Head Redundancy (Sigma_a).\"\"\"\n",
        "        _, int_a = self.model_a(X, return_internals=True)\n",
        "        _, int_b = self.model_b(X, return_internals=True)\n",
        "\n",
        "        synergy_data = []\n",
        "        for layer in range(self.config['n_layers']):\n",
        "            for model_name, internals in [('Adaptive', int_a), ('Control', int_b)]:\n",
        "                h_out = internals['heads'][layer]\n",
        "                flat = h_out.transpose(0,1).reshape(self.config['n_heads'], -1)\n",
        "                norm_flat = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm_flat, norm_flat.T)\n",
        "                mask = ~torch.eye(self.config['n_heads'], device=DEVICE).bool()\n",
        "                avg_sim = sim[mask].abs().mean().item()\n",
        "                synergy_data.append({'layer': layer, 'model': model_name, 'sigma_a': avg_sim})\n",
        "\n",
        "        self.results['synergy'] = pd.DataFrame(synergy_data)\n",
        "        return self.results['synergy']\n",
        "\n",
        "    def run_interpolation(self, data_path, steps=11):\n",
        "        \"\"\"High-resolution Loss Landscape.\"\"\"\n",
        "        alphas = np.linspace(0, 1, steps)\n",
        "        path_losses = []\n",
        "        for alpha in tqdm(alphas, desc=\"Profiling Landscape\"):\n",
        "            interp = NewGPT(self.config).to(DEVICE).eval()\n",
        "            sd = {k: alpha*self.model_a.state_dict()[k] + (1-alpha)*self.model_b.state_dict()[k]\n",
        "                  for k in self.model_a.state_dict()}\n",
        "            interp.load_state_dict(sd)\n",
        "\n",
        "            x, y = get_batch(data_path, batch_size=16) # Smaller batch for profiling speed\n",
        "            with torch.no_grad():\n",
        "                logits = interp(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "                path_losses.append(loss.item())\n",
        "        self.results['landscape'] = {'alpha': alphas, 'loss': path_losses}\n",
        "\n",
        "# ============================================================================\n",
        "# 5. VISUALIZATION ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "def generate_report(omni, out_dir):\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    # FIG 1: Dimensional Drilling\n",
        "    geo = omni.results['geometry']\n",
        "    o_proj = geo[geo['param'].str.contains('o_proj')].copy()\n",
        "    o_proj['layer'] = o_proj['param'].apply(lambda x: int(x.split('.')[1]))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    sns.lineplot(data=o_proj, x='layer', y='eff_rank', hue='model', marker='s')\n",
        "    plt.title(\"Information Bandwidth (Effective Rank: o_proj)\")\n",
        "    plt.savefig(f\"{out_dir}/dimensional_drill.png\")\n",
        "\n",
        "    # FIG 2: Redundancy & CKA\n",
        "    syn = omni.results['synergy']\n",
        "    cka = omni.results['cka']\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "    sns.lineplot(data=syn, x='layer', y='sigma_a', hue='model', ax=ax1, palette=['tab:blue', 'tab:orange'])\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(range(12), cka, color='tab:green', linestyle='--', label='CKA Similarity', marker='x')\n",
        "    ax1.set_title(\"Synergy: œÉ_a (Redundancy) vs Representational CKA\")\n",
        "    ax1.legend(loc='upper left'); ax2.legend(loc='upper right')\n",
        "    plt.savefig(f\"{out_dir}/redundancy_cka.png\")\n",
        "\n",
        "    # FIG 3: Landscape\n",
        "    ls = omni.results['landscape']\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(ls['alpha'], ls['loss'], 'r-o')\n",
        "    plt.title(\"Loss Landscape Barrier\")\n",
        "    plt.savefig(f\"{out_dir}/landscape_barrier.png\")\n",
        "\n",
        "    print(f\"‚úÖ OMNIBUS REPORT SAVED TO {out_dir}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. RUN\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = OmnibusForensics(PATH_ADAPTIVE, PATH_CONTROL)\n",
        "\n",
        "    print(\"üíé Running Geometric Analysis...\")\n",
        "    analyzer.geometric_drill()\n",
        "\n",
        "    print(\"üíé Running CKA & Head Redundancy...\")\n",
        "    x_sample, _ = get_batch(PATH_VAL_DATA, batch_size=32)\n",
        "    analyzer.compute_cka(x_sample)\n",
        "    analyzer.analyze_head_synergy(x_sample)\n",
        "\n",
        "    print(\"üíé Running Landscape Profiling...\")\n",
        "    analyzer.run_interpolation(PATH_VAL_DATA)\n",
        "\n",
        "    generate_report(analyzer, RESULTS_DIR)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "50D8v9pLOJzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Forensics] Gradient Flow & Signal Propagation Analysis\n",
        "# Standalone execution - Optimized for L4 GPU\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "# --- PATH CONFIGURATION ---\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "PATH_ADAPTIVE = os.path.join(PROJECT_ROOT, \"data/models/janus_adaptive_v1/janus_adaptive_final.pt\")\n",
        "PATH_CONTROL  = os.path.join(PROJECT_ROOT, \"data/models/janus_control_v1/janus_control_final.pt\")\n",
        "PATH_VAL_DATA = os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\")\n",
        "RESULTS_DIR   = os.path.join(PROJECT_ROOT, \"gradient_flow_results\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. ARCHITECTURE (Required for Standalone Loading)\n",
        "# ============================================================================\n",
        "# (Standard Janus Architecture: RMSNorm, SwiGLU, NewAttention, NewBlock, NewGPT)\n",
        "# [Note: Re-using the architecture classes from your previous Omnibus script]\n",
        "# ============================================================================\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads, self.d_head = config['n_heads'], config['d_head']\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        d_model = config['d_model']\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(self.d_head, config['max_seq_len']))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1).to(x.device)\n",
        "        return torch.view_as_real(x_c * freqs).flatten(3).type_as(x)\n",
        "\n",
        "    def forward(self, x, return_internals=False):\n",
        "        B, S, D = x.shape\n",
        "        q, k, v = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
        "        q = self.apply_rope(q.view(B, S, self.n_heads, self.d_head), self.freqs_cis)\n",
        "        k = self.apply_rope(k.view(B, S, self.n_heads, self.d_head), self.freqs_cis)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn_probs = F.softmax(attn.masked_fill(mask == 0, float('-inf')), dim=-1)\n",
        "        head_out = attn_probs @ v\n",
        "        out = self.o_proj(head_out.transpose(1, 2).reshape(B, S, D))\n",
        "\n",
        "        if return_internals: return out, head_out, attn_probs\n",
        "        return out\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1, self.attn = RMSNorm(config['d_model']), NewAttention(config)\n",
        "        self.ln2, self.mlp = RMSNorm(config['d_model']), SwiGLU(config['d_model'])\n",
        "\n",
        "    def forward(self, x, return_internals=False):\n",
        "        if return_internals:\n",
        "            a, h_out, a_probs = self.attn(self.ln1(x), return_internals=True)\n",
        "            x = x + a\n",
        "            m_out = self.mlp(self.ln2(x))\n",
        "            x = x + m_out\n",
        "            return x, h_out, a_probs, m_out\n",
        "        return x + self.attn(self.ln1(x)) + self.mlp(self.ln2(x))\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config['n_layers'])])\n",
        "        self.ln_f, self.head = RMSNorm(config['d_model']), nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "\n",
        "    def forward(self, idx, return_internals=False):\n",
        "        x = self.token_emb(idx)\n",
        "        internals = {'heads': [], 'attns': [], 'mlps': []}\n",
        "        for block in self.blocks:\n",
        "            if return_internals:\n",
        "                x, h, a, m = block(x, return_internals=True)\n",
        "                internals['heads'].append(h); internals['attns'].append(a); internals['mlps'].append(m)\n",
        "            else: x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return (logits, internals) if return_internals else logits\n",
        "# ============================================================================\n",
        "# 2. GRADIENT FLOW ANALYZER\n",
        "# ============================================================================\n",
        "\n",
        "def get_gradient_stats(model, x, y):\n",
        "    \"\"\"Performs a forward and backward pass to collect layer-wise gradient norms.\"\"\"\n",
        "    model.zero_grad()\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    layer_grads = []\n",
        "    for i, block in enumerate(model.blocks):\n",
        "        # We track the gradient norm of the attention output projection as a proxy for layer energy\n",
        "        if hasattr(block.attn.o_proj, 'weight') and block.attn.o_proj.weight.grad is not None:\n",
        "            grad_norm = block.attn.o_proj.weight.grad.norm().item()\n",
        "            layer_grads.append({'layer': i, 'grad_norm': grad_norm})\n",
        "\n",
        "    return layer_grads\n",
        "\n",
        "def run_gradient_forensics():\n",
        "    if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
        "\n",
        "    # 1. Load Models\n",
        "    config = {'vocab_size': 50257, 'd_model': 512, 'n_layers': 12, 'n_heads': 8, 'd_head': 64, 'max_seq_len': 512, 'dropout': 0.0}\n",
        "\n",
        "    print(\"üìÇ Loading Models from Drive...\")\n",
        "    model_a = NewGPT(config).to(DEVICE)\n",
        "    model_b = NewGPT(config).to(DEVICE)\n",
        "    model_a.load_state_dict(torch.load(PATH_ADAPTIVE, map_location=DEVICE))\n",
        "    model_b.load_state_dict(torch.load(PATH_CONTROL, map_location=DEVICE))\n",
        "\n",
        "    # 2. Prepare Data\n",
        "    print(\"üìñ Loading Validation Data...\")\n",
        "    data = np.memmap(PATH_VAL_DATA, dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - 128, (32,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+128]).astype(np.int64)) for i in ix]).to(DEVICE)\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+128+1]).astype(np.int64)) for i in ix]).to(DEVICE)\n",
        "\n",
        "    # 3. Compute Gradients\n",
        "    print(\"‚ö° Analyzing Gradient Propagation...\")\n",
        "    grads_a = get_gradient_stats(model_a, x, y)\n",
        "    grads_b = get_gradient_stats(model_b, x, y)\n",
        "\n",
        "    # 4. Process Results\n",
        "    df_a = pd.DataFrame(grads_a); df_a['model'] = 'Adaptive'\n",
        "    df_b = pd.DataFrame(grads_b); df_b['model'] = 'Control'\n",
        "    df = pd.concat([df_a, df_b])\n",
        "\n",
        "    # 5. Visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(data=df, x='layer', y='grad_norm', hue='model', marker='o', linewidth=2.5)\n",
        "    plt.yscale('log') # Gradients often span orders of magnitude\n",
        "    plt.title(\"Gradient Flow Profile: Signal Strength Across Layers\", fontsize=14)\n",
        "    plt.ylabel(\"Gradient Norm (Log Scale)\")\n",
        "    plt.xlabel(\"Layer Index (0=Early, 11=Late)\")\n",
        "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, \"gradient_flow_profile.png\"))\n",
        "\n",
        "    # 6. Summary Report\n",
        "    avg_a = df_a['grad_norm'].mean()\n",
        "    avg_b = df_b['grad_norm'].mean()\n",
        "    ratio = avg_a / avg_b\n",
        "\n",
        "    print(f\"\\nüìä GRADIENT FLOW SUMMARY\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Adaptive Avg Grad Norm: {avg_a:.6f}\")\n",
        "    print(f\"Control Avg Grad Norm:  {avg_b:.6f}\")\n",
        "    print(f\"Propagation Efficiency: {ratio:.2f}x\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"‚úÖ Results saved to {RESULTS_DIR}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gradient_forensics()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N0MS2ckjT3C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Adaptive Scheduler (WikiText-2 Anti-Overfit Edition)\n",
        "# Settings: 1500 Steps | Dropout 0.25 | Log Every 10\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data/Wikitext_2\")\n",
        "# Saving to new W2 specific folder\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_adaptive_w2\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üß† JANUS ADAPTIVE (WikiText-2 Correction Run)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION (The Regularization Nuke) ---\n",
        "class AdaptiveConfig:\n",
        "    def __init__(self):\n",
        "        # Model (Janus v3 Specs)\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "\n",
        "        # üõ°Ô∏è ANTI-OVERFIT MEASURES\n",
        "        self.dropout = 0.25       # High dropout to break memorization loops\n",
        "        self.weight_decay = 0.2   # Strong L2 regularization\n",
        "\n",
        "        # Training (Short & Dense)\n",
        "        self.max_steps = 1500     # Cap at ~25 Epochs (3000 was too long)\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4       # Eff BS = 64\n",
        "\n",
        "        # Adaptive Controller (Fast Reaction)\n",
        "        self.target_sigma = 0.0035\n",
        "        self.k_p = 0.8            # High gain\n",
        "        self.control_interval = 10 # Check sigma every 10 steps\n",
        "        self.warmup_steps = 200    # Start pressure early (Step 200)\n",
        "\n",
        "        # IO\n",
        "        self.ckpt_interval = 500\n",
        "        self.lite_interval = 10   # üì∏ High-res telemetry (Requested)\n",
        "\n",
        "# --- 4. THE HOMEOSTATIC CONTROLLER ---\n",
        "class Homeostat:\n",
        "    def __init__(self, config):\n",
        "        self.target = config.target_sigma\n",
        "        self.kp = config.k_p\n",
        "        self.warmup = config.warmup_steps\n",
        "        self.current_lambda = 0.0\n",
        "\n",
        "    def update(self, step, current_sigma):\n",
        "        # Phase 1: Ignition (Open Loop Ramp)\n",
        "        if step < self.warmup:\n",
        "            # Gentle ramp to 0.05\n",
        "            self.current_lambda = 0.05 * (step / max(1, self.warmup))\n",
        "            return self.current_lambda\n",
        "\n",
        "        # Phase 2: Homeostasis (Closed Loop P-Control)\n",
        "        if current_sigma is None: return self.current_lambda\n",
        "\n",
        "        error = current_sigma - self.target\n",
        "        # Positive Error (Too Redundant) -> INCREASE Pressure\n",
        "        delta = self.kp * error\n",
        "\n",
        "        # Apply & Clamp\n",
        "        self.current_lambda += delta\n",
        "        self.current_lambda = max(0.0, min(0.25, self.current_lambda))\n",
        "\n",
        "        return self.current_lambda\n",
        "\n",
        "# --- 5. ARCHITECTURE ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        if (l_div > 0.0 and self.training) or return_metrics:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "\n",
        "            if l_div > 0.0:\n",
        "                steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 6. DATA & LOGGING ---\n",
        "class W2Loader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        train_path = os.path.join(DATA_DIR, \"W2train.bin\")\n",
        "        val_path = os.path.join(DATA_DIR, \"W2val.bin\")\n",
        "\n",
        "        if not os.path.exists(train_path): raise FileNotFoundError(f\"Missing {train_path}\")\n",
        "        if not os.path.exists(val_path): raise FileNotFoundError(f\"Missing {val_path}\")\n",
        "\n",
        "        self.train_data = np.fromfile(train_path, dtype=np.uint16)\n",
        "        self.val_data = np.fromfile(val_path, dtype=np.uint16)\n",
        "        print(f\"üìñ Loaded W2 Train: {len(self.train_data):,} | Val: {len(self.val_data):,}\")\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.train_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.train_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.train_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "    def get_val_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.val_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.val_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.val_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "        self.start_step = 0\n",
        "    def set_start_step(self, step):\n",
        "        self.start_step = step\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        if step < self.start_step: return\n",
        "        avg_sigma = 0.0\n",
        "        if metrics:\n",
        "            sigmas = [m['sigma_a'].mean().item() for m in metrics if 'sigma_a' in m]\n",
        "            if sigmas: avg_sigma = sum(sigmas) / len(sigmas)\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure, \"sigma_a_avg\": avg_sigma\n",
        "        }\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_adaptive.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "\n",
        "# --- 7. EXECUTION ---\n",
        "def run_adaptive():\n",
        "    cfg = AdaptiveConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    # Using stronger weight decay per config\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6.0e-4, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    controller = Homeostat(cfg)\n",
        "    loader = W2Loader(cfg)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    # Resume Logic\n",
        "    start_step = 0\n",
        "    ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "    if ckpts:\n",
        "        ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "        latest = ckpts[-1]\n",
        "        print(f\"üîÑ Found checkpoint {latest}, but forcing FRESH START due to Regularization Nuke.\")\n",
        "        # NOTE: Intentionally NOT loading checkpoint to clear the memorized weights.\n",
        "\n",
        "    print(f\"\\nüß† STARTING W2 ADAPTIVE RUN (CORRECTION)\")\n",
        "    print(f\"   Steps: {cfg.max_steps}\")\n",
        "    print(f\"   Target Sigma: {cfg.target_sigma}\")\n",
        "    print(f\"   Dropout: {cfg.dropout}\")\n",
        "\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps))\n",
        "    current_sigma_avg = None\n",
        "\n",
        "    for step in pbar:\n",
        "        # A. Control\n",
        "        if step % cfg.control_interval == 0 or step < cfg.warmup_steps:\n",
        "             controller.update(step, current_sigma_avg)\n",
        "\n",
        "        p = controller.current_lambda\n",
        "        lambdas_list = []\n",
        "        for i in range(cfg.n_layers):\n",
        "            ratio = (i + 1) / cfg.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "            lambdas_list.append((0.0, p * s_mult))\n",
        "\n",
        "        # B. Train\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        accum_sigmas = []\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch(cfg.batch_size)\n",
        "\n",
        "            # Metrics more frequent now (every 10 steps)\n",
        "            return_metrics = (step % cfg.control_interval == 0) or (step % cfg.lite_interval == 0)\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas_list, y, return_metrics=return_metrics)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "            if metrics:\n",
        "                s = [m['sigma_a'].mean().item() for m in metrics]\n",
        "                if s: accum_sigmas.append(sum(s)/len(s))\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if accum_sigmas:\n",
        "            current_sigma_avg = sum(accum_sigmas) / len(accum_sigmas)\n",
        "\n",
        "        # C. Telemetry (High Res: Every 10 steps)\n",
        "        if step > 0 and (step % cfg.lite_interval == 0 or step == cfg.max_steps - 1):\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(5):\n",
        "                    vx, vy = loader.get_val_batch(cfg.batch_size)\n",
        "                    vl, _, _ = model(vx, [(0.0,0.0)]*cfg.n_layers, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, controller.current_lambda, metrics)\n",
        "            recorder.flush()\n",
        "\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|V:{val_loss:.2f}|P:{p:.3f}|Sig:{current_sigma_avg:.4f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|P:{p:.3f}\")\n",
        "\n",
        "        # D. Checkpoint\n",
        "        if step > 0 and step % cfg.ckpt_interval == 0:\n",
        "            torch.save(model.state_dict(), os.path.join(SAVE_DIR, f\"ckpt_step_{step}.pt\"))\n",
        "\n",
        "    # Finish\n",
        "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"adaptive_w2_final.pt\"))\n",
        "    print(f\"\\nüèÜ Run Complete. Model saved to {SAVE_DIR}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_adaptive()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OOp0tuiJncDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [RUN] Project Janus: Platinum Master (L4 Turbo Edition)\n",
        "# @markdown **System Status:** Platinum (Bugfix 3.0.7)\n",
        "# @markdown **Version:** 3.0.7 (Enhanced Credits Filter + Optimized Thresholds)\n",
        "# @markdown **Security:** HIGH | **Performance:** EXTREME\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import logging\n",
        "import signal\n",
        "import uuid\n",
        "import random\n",
        "import warnings\n",
        "import re\n",
        "import unicodedata\n",
        "import functools\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- 0. CRITICAL FIXES & SETUP ---\n",
        "\n",
        "# [FIX 1] Explicit Drive Mount\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Mounting Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Drive Mount Failed: {e}\")\n",
        "\n",
        "# [FIX 2] Pinned Dependencies\n",
        "try:\n",
        "    import stanza\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    import datasets\n",
        "    from transformers import AutoTokenizer\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing dependencies (Pinned)...\")\n",
        "    os.system('pip install stanza==1.8.2 transformers==4.44.2 datasets==2.21.0 huggingface_hub==0.24.6 -q')\n",
        "    import stanza\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    import datasets\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "# [FIX 3] Monkey-Patch torch.load for Stanza Compatibility\n",
        "original_load = torch.load\n",
        "def safe_load(*args, **kwargs):\n",
        "    if 'weights_only' not in kwargs:\n",
        "        kwargs['weights_only'] = False\n",
        "    return original_load(*args, **kwargs)\n",
        "torch.load = safe_load\n",
        "print(\"üîß Applied Stanza/PyTorch compatibility patch.\")\n",
        "\n",
        "# [FIX 4] Modern TF32 Enabler\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "        print(\"üöÄ TensorFloat-32 (TF32) Enabled for L4.\")\n",
        "    except AttributeError:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Global Seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# --- 1. Configuration (L4 Tuned) ---\n",
        "\n",
        "class JanusConfig:\n",
        "    def __init__(self):\n",
        "        self.PILOT_MODE = False\n",
        "        self.PILOT_LIMIT = 5000\n",
        "        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        # [L4 TURBO OPTIMIZATIONS]\n",
        "        self.STANZA_BATCH_SIZE = 150\n",
        "        self.SMART_BATCH_BUFFER_SIZE = 5000\n",
        "\n",
        "        self.CONTEXT_WINDOW = 1024\n",
        "        self.EOS_TOKEN = \"<endoftext>\"\n",
        "\n",
        "        # Heuristics (Strict)\n",
        "        self.MAX_S_COUNT = 1\n",
        "        self.MAX_SBAR_COUNT = 0\n",
        "        self.MAX_CC_COUNT = 1\n",
        "        self.MAX_WORD_LEN = 25\n",
        "\n",
        "        # Paths\n",
        "        self.BASE_DIR = Path(\"/content/drive/MyDrive/Project_Janus_Data\")\n",
        "        self.MANIFEST_FILE = self.BASE_DIR / \"janus_manifest.json\"\n",
        "        self.LOG_FILE = self.BASE_DIR / \"execution_log.txt\"\n",
        "\n",
        "        # Persistent Cache\n",
        "        self.STANZA_DIR = self.BASE_DIR / \"models/stanza_resources\"\n",
        "\n",
        "        self.SIMPLE_DIR = self.BASE_DIR / \"shards\" / \"simple\"\n",
        "        self.COMPLEX_DIR = self.BASE_DIR / \"shards\" / \"complex\"\n",
        "        self.TEMP_DIR = self.BASE_DIR / \"temp_staging\"\n",
        "\n",
        "    def initialize_filesystem(self):\n",
        "        for p in [self.SIMPLE_DIR, self.COMPLEX_DIR, self.TEMP_DIR, self.STANZA_DIR]:\n",
        "            p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CONFIG = JanusConfig()\n",
        "\n",
        "# --- 2. Logging & Utils (Atomic) ---\n",
        "\n",
        "def log_event(message, level=\"INFO\"):\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    msg = f\"[{ts}] {level}: {message}\"\n",
        "    print(msg)\n",
        "    try:\n",
        "        with open(CONFIG.LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(msg + \"\\n\")\n",
        "            f.flush()\n",
        "            os.fsync(f.fileno())\n",
        "    except: pass\n",
        "\n",
        "class JanusUtils:\n",
        "    @staticmethod\n",
        "    def atomic_write(data: str, target_path: Path):\n",
        "        temp_name = f\"{uuid.uuid4()}.tmp\"\n",
        "        temp_path = CONFIG.TEMP_DIR / temp_name\n",
        "        try:\n",
        "            with open(temp_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(data)\n",
        "                f.flush()\n",
        "                os.fsync(f.fileno())\n",
        "            shutil.move(str(temp_path), str(target_path))\n",
        "        except Exception as e:\n",
        "            if temp_path.exists(): os.remove(temp_path)\n",
        "            raise e\n",
        "\n",
        "    @staticmethod\n",
        "    def load_manifest():\n",
        "        if CONFIG.MANIFEST_FILE.exists():\n",
        "            try:\n",
        "                with open(CONFIG.MANIFEST_FILE, 'r') as f: return json.load(f)\n",
        "            except: pass\n",
        "        return {\"last_idx\": -1, \"total_simple\": 0, \"total_complex\": 0, \"total_lines\": 0}\n",
        "\n",
        "    @staticmethod\n",
        "    def update_manifest(idx, s_cnt, c_cnt, lines):\n",
        "        state = {\"last_idx\": idx, \"total_simple\": s_cnt, \"total_complex\": c_cnt, \"total_lines\": lines, \"ts\": datetime.now().isoformat()}\n",
        "        JanusUtils.atomic_write(json.dumps(state, indent=2), CONFIG.MANIFEST_FILE)\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize():\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "# --- 3. Data Cleaning (Robust + Aggressive) ---\n",
        "\n",
        "def clean_line(line):\n",
        "    \"\"\"\n",
        "    Aggressive cleaning for WikiText-103.\n",
        "    Targets: Headers, Lists, Timecodes, Credits, and Fragments.\n",
        "    \"\"\"\n",
        "    # 1. Unicode Normalization (NFKC)\n",
        "    # Converts compatibility characters (like fancy fractions or ligatures) to standard ASCII/Unicode equivalents.\n",
        "    line = unicodedata.normalize('NFKC', line)\n",
        "    line = line.strip()\n",
        "\n",
        "    # 2. Length Filter\n",
        "    # \"No.\" is 3 chars. We want to avoid single words or tiny fragments.\n",
        "    if len(line) < 5: return None\n",
        "\n",
        "    # 3. Aggressive Header Filter\n",
        "    # Catches \"= Header =\", \"==Header==\", \" = = Header = = \"\n",
        "    # Note: The previous .strip() removes the leading spaces from \"= = Header = =\", so startswith('=') works.\n",
        "    if line.startswith(\"=\") and line.endswith(\"=\"):\n",
        "        return None\n",
        "\n",
        "    # 4. List/Bullet Markers\n",
        "    # Filters lines starting with common bullet points used in Wiki lists.\n",
        "    if line.startswith(\"*\") or line.startswith(\"‚Ä¢\") or line.startswith(\"-\"):\n",
        "        return None\n",
        "\n",
        "    # 5. Timecode/Tracklist Filter\n",
        "    # Catches \"10:30\", \"4 : 24\", \"3:16\"\n",
        "    if re.search(r'\\d\\s*:\\s*\\d', line):\n",
        "        return None\n",
        "\n",
        "    # 6. Credits / Definition Filter [NEW]\n",
        "    # Catches \"Joe Gastwirt ‚Äì mastering\" or \"Word ‚Äì Definition\"\n",
        "    # Matches: Start -> Text -> Spaced Dash -> Text -> End\n",
        "    if re.match(r'^[\\w\\s\\.]+\\s+[‚Äì-]\\s+[\\w\\s\\.]+$', line):\n",
        "        return None\n",
        "\n",
        "    # 7. Fragment Detection (Density Check) [FIXED]\n",
        "    # Fixed the string literal bug to correctly include quotes.\n",
        "    # Lowered threshold to 15% to be slightly more aggressive on \"garbage\" lines.\n",
        "    special_chars = set(\"‚Äì‚Äî:;()[]{}\\\"\\'/|\\\\\")\n",
        "    special_char_count = sum(1 for c in line if c in special_chars)\n",
        "    if len(line) > 0 and (special_char_count / len(line)) > 0.15:\n",
        "        return None\n",
        "\n",
        "    # 8. All-caps Filter\n",
        "    # Good for shouting headers, but ensure it's long enough to not kill acronyms like \"NASA\".\n",
        "    if line.isupper() and len(line) > 4:\n",
        "        return None\n",
        "\n",
        "    return line\n",
        "\n",
        "# --- 4. Syntax Engine (L4 Accelerated & Fixed) ---\n",
        "\n",
        "class SyntaxFilter:\n",
        "    def __init__(self):\n",
        "        log_event(\"Initializing Stanza (L4 Mode - FP32 GPU)...\")\n",
        "        stanza.download('en', model_dir=str(CONFIG.STANZA_DIR), processors='tokenize,pos,constituency', logging_level='WARN')\n",
        "        self.nlp = stanza.Pipeline('en', dir=str(CONFIG.STANZA_DIR), processors='tokenize,pos,constituency',\n",
        "                                   use_gpu=(CONFIG.DEVICE=='cuda'), pos_batch_size=5000, logging_level='ERROR')\n",
        "\n",
        "    def _is_simple(self, sent):\n",
        "        try:\n",
        "            tree = sent.constituency\n",
        "            if tree is None: return False\n",
        "            s_str = str(tree)\n",
        "            return (s_str.count(\"(S \") <= CONFIG.MAX_S_COUNT and\n",
        "                    s_str.count(\"(SBAR \") == CONFIG.MAX_SBAR_COUNT and\n",
        "                    s_str.count(\"(CC \") <= CONFIG.MAX_CC_COUNT and\n",
        "                    len(sent.words) < CONFIG.MAX_WORD_LEN)\n",
        "        except: return False\n",
        "\n",
        "    def process_batch(self, lines: List[str]) -> List[Dict]:\n",
        "        results = []\n",
        "        if not lines: return results\n",
        "\n",
        "        batch_sorted = sorted([(len(l), l) for l in lines], key=lambda x: x[0])\n",
        "        texts = [x[1] for x in batch_sorted]\n",
        "\n",
        "        for i in range(0, len(texts), CONFIG.STANZA_BATCH_SIZE):\n",
        "            chunk = texts[i : i + CONFIG.STANZA_BATCH_SIZE]\n",
        "            try:\n",
        "                in_docs = [stanza.Document([], text=d) for d in chunk]\n",
        "\n",
        "                # [CRITICAL FIX] Removed autocast - Stanza doesn't support mixed precision\n",
        "                docs = self.nlp(in_docs)\n",
        "\n",
        "                for j, doc in enumerate(docs):\n",
        "                    for sent in doc.sentences:\n",
        "                        cat = 'simple' if self._is_simple(sent) else 'complex'\n",
        "                        results.append({'text': sent.text, 'category': cat})\n",
        "\n",
        "            except Exception as e:\n",
        "                # Fallback: Line-by-Line\n",
        "                log_event(f\"Batch fail ({str(e)[:50]}). Fallback active.\", \"WARN\")\n",
        "                JanusUtils.sanitize()\n",
        "                for line in chunk:\n",
        "                    try:\n",
        "                        doc = self.nlp(line)\n",
        "                        for sent in doc.sentences:\n",
        "                            cat = 'simple' if self._is_simple(sent) else 'complex'\n",
        "                            results.append({'text': sent.text, 'category': cat})\n",
        "                    except: pass\n",
        "        return results\n",
        "\n",
        "# --- 5. Packer ---\n",
        "\n",
        "class SequencePacker:\n",
        "    def __init__(self, tokenizer, out_dir, prefix):\n",
        "        self.tok = tokenizer\n",
        "        self.out_dir = out_dir\n",
        "        self.prefix = prefix\n",
        "        self.buffer = []\n",
        "        self.count = len(list(out_dir.glob(f\"{prefix}_shard_*.txt\")))\n",
        "\n",
        "    def add(self, text):\n",
        "        ids = self.tok.encode(text, add_special_tokens=False)\n",
        "        eos = self.tok.encode(CONFIG.EOS_TOKEN, add_special_tokens=False)\n",
        "        self.buffer.extend(ids + eos)\n",
        "\n",
        "        while len(self.buffer) >= CONFIG.CONTEXT_WINDOW:\n",
        "            shard_ids = self.buffer[:CONFIG.CONTEXT_WINDOW]\n",
        "            self.buffer = self.buffer[CONFIG.CONTEXT_WINDOW:]\n",
        "            out_txt = self.tok.decode(shard_ids)\n",
        "            fname = f\"{self.prefix}_shard_{self.count:06d}.txt\"\n",
        "            JanusUtils.atomic_write(out_txt, self.out_dir / fname)\n",
        "            self.count += 1\n",
        "\n",
        "    def finalize(self):\n",
        "        if self.buffer:\n",
        "            out_txt = self.tok.decode(self.buffer)\n",
        "            fname = f\"{self.prefix}_shard_{self.count:06d}.txt\"\n",
        "            JanusUtils.atomic_write(out_txt, self.out_dir / fname)\n",
        "            self.buffer = []\n",
        "\n",
        "# --- 6. Main Execution ---\n",
        "\n",
        "def main():\n",
        "    log_event(\"=== STARTING JANUS PLATINUM (L4 TURBO) ===\")\n",
        "    CONFIG.initialize_filesystem()\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained('gpt2')\n",
        "    ds = datasets.load_dataset('wikitext', 'wikitext-103-raw-v1', split='train', streaming=True)\n",
        "\n",
        "    filt = SyntaxFilter()\n",
        "    s_pack = SequencePacker(tok, CONFIG.SIMPLE_DIR, \"simple\")\n",
        "    c_pack = SequencePacker(tok, CONFIG.COMPLEX_DIR, \"complex\")\n",
        "\n",
        "    manifest = JanusUtils.load_manifest()\n",
        "    start_idx = manifest['last_idx'] + 1\n",
        "    total_lines = manifest['total_lines']\n",
        "\n",
        "    log_event(f\"Resuming at Index {start_idx}\")\n",
        "    if torch.cuda.is_available():\n",
        "        log_event(f\"GPU Active: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    buffer = []\n",
        "    limit = CONFIG.PILOT_LIMIT if CONFIG.PILOT_MODE else 2000000\n",
        "    pbar = tqdm(initial=start_idx, total=limit)\n",
        "\n",
        "    try:\n",
        "        for i, row in enumerate(ds):\n",
        "            if i < start_idx: continue\n",
        "            if CONFIG.PILOT_MODE and i >= start_idx + limit: break\n",
        "\n",
        "            clean = clean_line(row['text'])\n",
        "            if clean: buffer.append(clean)\n",
        "\n",
        "            if len(buffer) >= CONFIG.SMART_BATCH_BUFFER_SIZE:\n",
        "                results = filt.process_batch(buffer)\n",
        "                for res in results:\n",
        "                    if res['category'] == 'simple': s_pack.add(res['text'])\n",
        "                    else: c_pack.add(res['text'])\n",
        "\n",
        "                processed_lines = total_lines + len(buffer)\n",
        "                JanusUtils.update_manifest(i, s_pack.count, c_pack.count, processed_lines)\n",
        "\n",
        "                buffer = []\n",
        "                pbar.update(CONFIG.SMART_BATCH_BUFFER_SIZE)\n",
        "                JanusUtils.sanitize()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        log_event(\"Interrupt. Shutting down...\")\n",
        "    except Exception as e:\n",
        "        log_event(f\"FATAL: {e}\", \"ERROR\")\n",
        "        raise e\n",
        "    finally:\n",
        "        s_pack.finalize()\n",
        "        c_pack.finalize()\n",
        "        log_event(\"DONE.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KwFcTBPfO9yJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Write to Drive] SAE Modules\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Ensure this matches your Drive structure\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "MODULE_DIR = os.path.join(PROJECT_ROOT, \"src/SAE\")\n",
        "\n",
        "print(f\"üìÇ Creating Module Directory: {MODULE_DIR}\")\n",
        "os.makedirs(MODULE_DIR, exist_ok=True)\n",
        "# Create an empty __init__.py to make it importable\n",
        "with open(os.path.join(MODULE_DIR, \"__init__.py\"), \"w\") as f:\n",
        "    f.write(\"\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIG MODULE (config.py)\n",
        "# ==========================================\n",
        "config_code = \"\"\"\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class SAEConfig:\n",
        "    # Architecture\n",
        "    head_dim: int = 64\n",
        "    expansion_factor: int = 8\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 2048\n",
        "    lr: float = 1e-3\n",
        "    l1_coeff: float = 0.005  # The sparsity penalty\n",
        "    steps: int = 2000 # Short runs for fast validation\n",
        "\n",
        "    # Validation\n",
        "    target_l0: float = 15.0 # We want ~15 active neurons per token\n",
        "\n",
        "    @property\n",
        "    def hidden_dim(self):\n",
        "        return self.head_dim * self.expansion_factor\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(MODULE_DIR, \"config.py\"), \"w\") as f:\n",
        "    f.write(config_code)\n",
        "print(\"‚úÖ Written: config.py\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. MODEL MODULE (model.py)\n",
        "# ==========================================\n",
        "model_code = \"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HeadSAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dim = config.head_dim\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "\n",
        "        # 1. Learned Pre-Bias (Centering)\n",
        "        # We subtract this before encoding to handle non-zero mean activations\n",
        "        self.b_pre = nn.Parameter(torch.zeros(self.dim))\n",
        "\n",
        "        # 2. Encoder (Untied Weights)\n",
        "        # Projects to higher dimensional sparse space\n",
        "        self.W_enc = nn.Linear(self.dim, self.hidden_dim, bias=True)\n",
        "\n",
        "        # 3. Decoder (Untied Weights)\n",
        "        # Reconstructs original signal.\n",
        "        # Bias False because we use a separate explicit parameter for clarity\n",
        "        self.W_dec = nn.Linear(self.hidden_dim, self.dim, bias=False)\n",
        "        self.b_dec = nn.Parameter(torch.zeros(self.dim))\n",
        "\n",
        "        # Initialization\n",
        "        nn.init.kaiming_uniform_(self.W_enc.weight)\n",
        "        nn.init.kaiming_uniform_(self.W_dec.weight)\n",
        "\n",
        "        # Normalize decoder columns immediately\n",
        "        self.normalize_decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [Batch, head_dim]\n",
        "\n",
        "        # 1. Pre-process\n",
        "        x_centered = x - self.b_pre\n",
        "\n",
        "        # 2. Encode\n",
        "        # Relu ensures sparsity (most values become 0)\n",
        "        acts = torch.relu(self.W_enc(x_centered))\n",
        "\n",
        "        # 3. Decode\n",
        "        recon = self.W_dec(acts) + self.b_dec\n",
        "\n",
        "        return recon, acts\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def normalize_decoder(self):\n",
        "        # Constrain decoder columns to Unit Norm\n",
        "        # This prevents the model from \"cheating\" by making features huge\n",
        "        self.W_dec.weight.data = F.normalize(self.W_dec.weight.data, p=2, dim=0)\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(MODULE_DIR, \"model.py\"), \"w\") as f:\n",
        "    f.write(model_code)\n",
        "print(\"‚úÖ Written: model.py\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. HARVESTER MODULE (harvester.py)\n",
        "# ==========================================\n",
        "harvester_code = \"\"\"\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "\n",
        "class HeadHarvester:\n",
        "    \\\"\\\"\\\"\n",
        "    Surgically extracts attention head outputs BEFORE they are mixed.\n",
        "    Hooks into the input of the Output Projection (o_proj) layer.\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self, model, layers=[3, 6, 9], num_heads=8, head_dim=64):\n",
        "        self.model = model\n",
        "        self.layers = layers\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        # Buffer: {layer_idx: [tensor_chunk1, tensor_chunk2]}\n",
        "        self.buffer = {l: [] for l in layers}\n",
        "        self.handles = []\n",
        "\n",
        "    def _hook_fn(self, module, input, output, layer_idx):\n",
        "        # The input to o_proj is the Concatenated Heads.\n",
        "        # Shape: [Batch, Seq, n_heads * head_dim]\n",
        "        # We need to detach immediately to save VRAM.\n",
        "        mixed_heads = input[0].detach()\n",
        "        B, S, _ = mixed_heads.shape\n",
        "\n",
        "        # Reshape to separate heads: [Batch, Seq, 8, 64]\n",
        "        separated = mixed_heads.view(B, S, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Flatten Batch and Seq: [Batch*Seq, 8, 64]\n",
        "        # We use .half() (float16) to save disk space\n",
        "        flat = separated.view(-1, self.num_heads, self.head_dim).cpu().half()\n",
        "\n",
        "        self.buffer[layer_idx].append(flat)\n",
        "\n",
        "    def attach(self):\n",
        "        \\\"\\\"\\\"\n",
        "        Attaches hooks to the specific layers in the Janus model.\n",
        "        NOTE: Adjust 'self.model.blocks' if your architecture varies.\n",
        "        \\\"\\\"\\\"\n",
        "        for i in self.layers:\n",
        "            # We target the o_proj layer. The *input* to this layer\n",
        "            # is the raw output of the attention heads.\n",
        "            try:\n",
        "                # Try standard Janus/NewGPT structure\n",
        "                target_module = self.model.blocks[i].attn.o_proj\n",
        "            except AttributeError:\n",
        "                # Fallback for standard HF Llama\n",
        "                target_module = self.model.model.layers[i].self_attn.o_proj\n",
        "\n",
        "            handle = target_module.register_forward_hook(\n",
        "                lambda m, inp, out, idx=i: self._hook_fn(m, inp, out, idx)\n",
        "            )\n",
        "            self.handles.append(handle)\n",
        "            print(f\"ü™ù Hook attached to Layer {i} Output Projection\")\n",
        "\n",
        "    def save(self, directory, prefix):\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        for layer, chunks in self.buffer.items():\n",
        "            if not chunks: continue\n",
        "\n",
        "            # Concatenate all chunks\n",
        "            data = torch.cat(chunks, dim=0) # [Total_Tokens, 8, 64]\n",
        "\n",
        "            fname = f\"{prefix}_L{layer}_heads.pt\"\n",
        "            save_path = os.path.join(directory, fname)\n",
        "            torch.save(data, save_path)\n",
        "            print(f\"üíæ Saved {fname}: {data.shape}\")\n",
        "\n",
        "        # Clear buffer to free RAM\n",
        "        self.buffer = {l: [] for l in self.layers}\n",
        "        gc.collect()\n",
        "\n",
        "    def detach(self):\n",
        "        for h in self.handles: h.remove()\n",
        "        self.handles = []\n",
        "        print(\"ü™ù Hooks removed.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(MODULE_DIR, \"harvester.py\"), \"w\") as f:\n",
        "    f.write(harvester_code)\n",
        "print(\"‚úÖ Written: harvester.py\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAINER MODULE (trainer.py)\n",
        "# ==========================================\n",
        "trainer_code = \"\"\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from .model import HeadSAE\n",
        "\n",
        "def train_sae_on_head(activations, config, device=\"cuda\"):\n",
        "    \\\"\\\"\\\"\n",
        "    Trains a single SAE on a specific head's activations.\n",
        "    activations: Tensor [N_Tokens, 64]\n",
        "    \\\"\\\"\\\"\n",
        "    sae = HeadSAE(config).to(device)\n",
        "    optimizer = optim.Adam(sae.parameters(), lr=config.lr)\n",
        "\n",
        "    # Create Loader\n",
        "    dataset = TensorDataset(activations)\n",
        "    loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "    final_mse = 0.0\n",
        "    final_l0 = 0.0\n",
        "\n",
        "    # Training Loop\n",
        "    sae.train()\n",
        "    iter_loader = iter(loader)\n",
        "\n",
        "    # We define steps rather than epochs for consistency\n",
        "    for step in range(config.steps):\n",
        "        try:\n",
        "            batch = next(iter_loader)[0]\n",
        "        except StopIteration:\n",
        "            iter_loader = iter(loader)\n",
        "            batch = next(iter_loader)[0]\n",
        "\n",
        "        batch = batch.to(device).float() # Ensure float32 for training\n",
        "\n",
        "        # Forward\n",
        "        recon, acts = sae(batch)\n",
        "\n",
        "        # Losses\n",
        "        mse = F.mse_loss(recon, batch)\n",
        "        # L1 penalty on activations (sum over features, mean over batch)\n",
        "        l1 = acts.sum(dim=1).mean()\n",
        "\n",
        "        loss = mse + (config.l1_coeff * l1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Constraint: Unit Norm Decoder\n",
        "        sae.normalize_decoder()\n",
        "\n",
        "        # Metrics (Moving average or just last step)\n",
        "        if step % 100 == 0:\n",
        "            with torch.no_grad():\n",
        "                l0 = (acts > 0).float().sum(dim=1).mean()\n",
        "                # print(f\"Step {step}: MSE={mse.item():.5f} L0={l0.item():.1f}\")\n",
        "\n",
        "        final_mse = mse.item()\n",
        "        final_l0 = (acts > 0).float().sum(dim=1).mean().item()\n",
        "\n",
        "    return sae, final_mse, final_l0\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(MODULE_DIR, \"trainer.py\"), \"w\") as f:\n",
        "    f.write(trainer_code)\n",
        "print(\"‚úÖ Written: trainer.py\")\n",
        "\n",
        "print(\"\\nüöÄ SAE Modules successfully installed to Drive.\")\n",
        "print(\"‚ÑπÔ∏è  You can now use: 'sys.path.append(PROJECT_ROOT)' followed by 'from src.SAE import ...'\")"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "TYSvNqei3tx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Control (No Adaptive Scheduler) SAE 4k steps\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CHUNKS_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_SAE_Control_v1\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üß† JANUS CONTROL (No Adaptive Scheduler)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class ControlConfig:\n",
        "    def __init__(self):\n",
        "        # Model\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # Training\n",
        "        self.max_steps = 4005\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4\n",
        "\n",
        "        # Control: Fixed lambda (no adaptation)\n",
        "        self.fixed_lambda = 0.0  # NO steering pressure\n",
        "\n",
        "        # IO\n",
        "        self.ckpt_interval = 2000\n",
        "        self.chunk_steps = 500\n",
        "        self.lite_interval = 100\n",
        "\n",
        "# --- 4. ARCHITECTURE (Standard Janus v3) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        # Calculate steering if needed (though lambda should be 0)\n",
        "        if (l_div > 0.0 and self.training) or return_metrics:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "\n",
        "            if l_div > 0.0:\n",
        "                steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 5. DATA & LOGGING ---\n",
        "class ChunkLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.current_chunk_idx = -1\n",
        "        self.data = None\n",
        "        self.val_data = np.memmap(os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\"), dtype=np.uint16, mode='r')\n",
        "    def load_for_step(self, step):\n",
        "        target_chunk = step // self.config.chunk_steps\n",
        "        if target_chunk != self.current_chunk_idx:\n",
        "            fname = f\"train_chunk_{target_chunk:03d}.bin\"\n",
        "            fpath = os.path.join(CHUNKS_DIR, fname)\n",
        "            if not os.path.exists(fpath):\n",
        "                fallback_idx = target_chunk % 40\n",
        "                fpath = os.path.join(CHUNKS_DIR, f\"train_chunk_{fallback_idx:03d}.bin\")\n",
        "            with open(fpath, 'rb') as f:\n",
        "                self.data = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "            self.current_chunk_idx = target_chunk\n",
        "    def get_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "    def get_val_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.val_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.val_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.val_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "        self.start_step = 0\n",
        "    def set_start_step(self, step):\n",
        "        self.start_step = step\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        if step < self.start_step: return\n",
        "\n",
        "        # Calculate mean sigma_a across layers for logging\n",
        "        avg_sigma = 0.0\n",
        "        if metrics:\n",
        "            sigmas = [m['sigma_a'].mean().item() for m in metrics if 'sigma_a' in m]\n",
        "            if sigmas: avg_sigma = sum(sigmas) / len(sigmas)\n",
        "\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure,\n",
        "            \"sigma_a_avg\": avg_sigma\n",
        "        }\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_control.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 6. RUN LOOP ---\n",
        "def run_control():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = ControlConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6.29e-4, weight_decay=1.34e-4)\n",
        "    loader = ChunkLoader(cfg)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "\n",
        "    # Check for existing control checkpoints\n",
        "    ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_control_step_\") and f.endswith(\".pt\")]\n",
        "    if ckpts:\n",
        "        ckpts.sort(key=lambda x: int(x.split('_')[3].split('.')[0]))\n",
        "        latest = ckpts[-1]\n",
        "        ckpt_path = os.path.join(SAVE_DIR, latest)\n",
        "        print(f\"üîÑ Resuming Control Run from {latest}...\")\n",
        "        c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(c['model'])\n",
        "        optimizer.load_state_dict(c['optim'])\n",
        "        start_step = c['step'] + 1\n",
        "        recorder.set_start_step(start_step)\n",
        "\n",
        "    # Set fixed lambda (0.0 for control = no steering)\n",
        "    fixed_lambda = cfg.fixed_lambda\n",
        "    lambdas_list = [(0.0, 0.0)] * cfg.n_layers  # All layers: no steering\n",
        "\n",
        "    print(f\"\\nüß† STARTING CONTROL RUN: {start_step} -> {cfg.max_steps}\")\n",
        "    print(f\"   Fixed Lambda: {fixed_lambda} (No Adaptive Scheduler)\")\n",
        "\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    for step in pbar:\n",
        "        loader.load_for_step(step)\n",
        "\n",
        "        # --- A. TRAINING STEP ---\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch(cfg.batch_size)\n",
        "\n",
        "            is_log_step = (step % cfg.lite_interval == 0)\n",
        "            return_metrics = is_log_step\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas_list, y, return_metrics=return_metrics)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- B. TELEMETRY ---\n",
        "        if step > 0 and (step % cfg.lite_interval == 0 or step == cfg.max_steps - 1):\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(10):\n",
        "                    vx, vy = loader.get_val_batch(cfg.batch_size)\n",
        "                    vl, _, _ = model(vx, lambdas_list, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, fixed_lambda, metrics)\n",
        "            recorder.flush()\n",
        "\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|V:{val_loss:.2f}|Control\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|Control\")\n",
        "\n",
        "        # --- C. CHECKPOINTS ---\n",
        "        if step > 0 and step % cfg.ckpt_interval == 0:\n",
        "            ckpt_name = f\"ckpt_control_step_{step}.pt\"\n",
        "            ckpt_path = os.path.join(SAVE_DIR, ckpt_name)\n",
        "            save_dict = {\n",
        "                'step': step,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'rng_cpu': torch.get_rng_state(),\n",
        "                'rng_gpu': torch.cuda.get_rng_state()\n",
        "            }\n",
        "            torch.save(save_dict, ckpt_path)\n",
        "\n",
        "            all_ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_control_step_\") and f.endswith(\".pt\")]\n",
        "            all_ckpts.sort(key=lambda x: int(x.split('_')[3].split('.')[0]))\n",
        "            if len(all_ckpts) > 3: os.remove(os.path.join(SAVE_DIR, all_ckpts[0]))\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_control_SAE.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ CONTROL RUN COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_control()"
      ],
      "metadata": {
        "id": "mfQ0Ebizz4OF",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run] Adaptive Scheduler SAE 4K steps\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. MEMORY NUKE ---\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/Project_XAI_Physical_Janus\"\n",
        "CHUNKS_DIR = os.path.join(PROJECT_ROOT, \"data/wikitext_chunks\")\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"data/models/janus_SAE_adaptive_v1\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üß† JANUS ADAPTIVE (Homeostatic Control)\")\n",
        "print(f\"‚öôÔ∏è Hardware: {DEVICE}\")\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "class AdaptiveConfig:\n",
        "    def __init__(self):\n",
        "        # Model\n",
        "        self.vocab_size = 50257\n",
        "        self.d_model = 512\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 8\n",
        "        self.d_head = 64\n",
        "        self.max_seq_len = 512\n",
        "        self.dropout = 0.05\n",
        "\n",
        "        # Training\n",
        "        self.max_steps = 4005\n",
        "        self.batch_size = 16\n",
        "        self.grad_accum = 4\n",
        "\n",
        "        # Adaptive Controller\n",
        "        self.target_sigma = 0.0035\n",
        "        self.k_p = 0.5  # Gain\n",
        "        self.control_interval = 50 # Update every 50 steps\n",
        "        self.warmup_steps = 1500   # Ignition phase\n",
        "\n",
        "        # IO\n",
        "        self.ckpt_interval = 2000\n",
        "        self.chunk_steps = 500\n",
        "        self.lite_interval = 100\n",
        "\n",
        "# --- 4. THE HOMEOSTATIC CONTROLLER ---\n",
        "class Homeostat:\n",
        "    def __init__(self, config):\n",
        "        self.target = config.target_sigma\n",
        "        self.kp = config.k_p\n",
        "        self.warmup = config.warmup_steps\n",
        "        self.current_lambda = 0.0\n",
        "        self.history = []\n",
        "\n",
        "    def update(self, step, current_sigma):\n",
        "        # Phase 1: Ignition (Open Loop Ramp)\n",
        "        if step < self.warmup:\n",
        "            # Ramp from 0.00 to 0.05\n",
        "            self.current_lambda = 0.05 * (step / self.warmup)\n",
        "            return self.current_lambda\n",
        "\n",
        "        # Phase 2: Homeostasis (Closed Loop P-Control)\n",
        "        if current_sigma is None: return self.current_lambda\n",
        "\n",
        "        error = current_sigma - self.target\n",
        "        # CORRECTION: Positive Error (Too Redundant) -> INCREASE Pressure\n",
        "        delta = self.kp * error\n",
        "\n",
        "        # Apply & Clamp\n",
        "        self.current_lambda += delta\n",
        "        # Hard limits to prevent explosion or negative pressure\n",
        "        self.current_lambda = max(0.0, min(0.25, self.current_lambda))\n",
        "\n",
        "        return self.current_lambda\n",
        "\n",
        "# --- 5. ARCHITECTURE (Standard Janus v3) ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * 8 / 3)\n",
        "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(d_model, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, d_model, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class NewAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_head = config.d_head\n",
        "        self.scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(config.d_head, config.max_seq_len))\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, end, theta=10000.0):\n",
        "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "        t = torch.arange(end, device=freqs.device)\n",
        "        freqs = torch.outer(t, freqs).float()\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "        return freqs_cis\n",
        "\n",
        "    def apply_rope(self, x, freqs_cis):\n",
        "        x_c = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:x.shape[1]].view(1, x.shape[1], 1, -1)\n",
        "        x_out = torch.view_as_real(x_c * freqs).flatten(3)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        B, S, D = x.shape\n",
        "        q = self.q_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        k = self.k_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "        v = self.v_proj(x).view(B, S, self.n_heads, self.d_head)\n",
        "\n",
        "        q = self.apply_rope(q, self.freqs_cis)\n",
        "        k = self.apply_rope(k, self.freqs_cis)\n",
        "        q = q.transpose(1, 2); k = k.transpose(1, 2); v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = torch.tril(torch.ones(S, S, device=x.device)).view(1, 1, S, S)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "        head_out = attn_probs @ v\n",
        "\n",
        "        steer_loss = 0.0\n",
        "        l_coh, l_div = lambdas\n",
        "\n",
        "        # Calculate steering if needed\n",
        "        # We need this during training AND during control steps\n",
        "        if (l_div > 0.0 and self.training) or return_metrics:\n",
        "            flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "            norm = F.normalize(flat, p=2, dim=1)\n",
        "            gram = torch.mm(norm, norm.t())\n",
        "            identity = torch.eye(self.n_heads, device=x.device)\n",
        "\n",
        "            if l_div > 0.0:\n",
        "                steer_loss += torch.norm(gram - identity, p='fro') * l_div\n",
        "\n",
        "        metrics = {}\n",
        "        if return_metrics:\n",
        "            with torch.no_grad():\n",
        "                # Re-calc gram for metrics just to be safe/clean\n",
        "                flat = head_out.transpose(0, 1).reshape(self.n_heads, -1)\n",
        "                norm = F.normalize(flat, p=2, dim=1)\n",
        "                sim = torch.mm(norm, norm.t())\n",
        "                mask_diag = ~torch.eye(self.n_heads, dtype=torch.bool, device=x.device)\n",
        "                metrics['sigma_a'] = (sim.abs() * mask_diag.float()).sum(dim=1) / (self.n_heads - 1)\n",
        "\n",
        "        out = head_out.transpose(1, 2).reshape(B, S, D)\n",
        "        return self.o_proj(out), steer_loss, metrics\n",
        "\n",
        "class NewBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(config.d_model)\n",
        "        self.attn = NewAttention(config)\n",
        "        self.ln2 = RMSNorm(config.d_model)\n",
        "        self.mlp = SwiGLU(config.d_model)\n",
        "    def forward(self, x, lambdas, return_metrics=False):\n",
        "        a, s, m = self.attn(self.ln1(x), lambdas, return_metrics)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, s, m\n",
        "\n",
        "class NewGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.blocks = nn.ModuleList([NewBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln_f = RMSNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.token_emb.weight = self.head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        for name, p in module.named_parameters():\n",
        "            if \"o_proj.weight\" in name or \"w3.weight\" in name:\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers))\n",
        "\n",
        "    def forward(self, idx, lambdas_list, targets=None, return_metrics=False):\n",
        "        B, S = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        total_steer = 0.0\n",
        "        all_metrics = []\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, s, m = block(x, lambdas_list[i], return_metrics)\n",
        "            total_steer += s\n",
        "            if return_metrics: all_metrics.append(m)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return loss, total_steer, all_metrics\n",
        "\n",
        "# --- 6. DATA & LOGGING ---\n",
        "class ChunkLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.current_chunk_idx = -1\n",
        "        self.data = None\n",
        "        self.val_data = np.memmap(os.path.join(PROJECT_ROOT, \"data/wikitext/val.bin\"), dtype=np.uint16, mode='r')\n",
        "    def load_for_step(self, step):\n",
        "        target_chunk = step // self.config.chunk_steps\n",
        "        if target_chunk != self.current_chunk_idx:\n",
        "            fname = f\"train_chunk_{target_chunk:03d}.bin\"\n",
        "            fpath = os.path.join(CHUNKS_DIR, fname)\n",
        "            if not os.path.exists(fpath):\n",
        "                fallback_idx = target_chunk % 40\n",
        "                fpath = os.path.join(CHUNKS_DIR, f\"train_chunk_{fallback_idx:03d}.bin\")\n",
        "            with open(fpath, 'rb') as f:\n",
        "                self.data = np.frombuffer(f.read(), dtype=np.uint16)\n",
        "            self.current_chunk_idx = target_chunk\n",
        "    def get_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "    def get_val_batch(self, batch_size):\n",
        "        ix = torch.randint(len(self.val_data) - self.config.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy(self.val_data[i:i+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy(self.val_data[i+1:i+1+self.config.max_seq_len].astype(np.int64)) for i in ix])\n",
        "        return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "class BlackBox:\n",
        "    def __init__(self, save_dir):\n",
        "        self.buffer = []\n",
        "        self.save_dir = save_dir\n",
        "        self.start_step = 0\n",
        "    def set_start_step(self, step):\n",
        "        self.start_step = step\n",
        "    def log(self, step, loss, val_loss, pressure, metrics):\n",
        "        if step < self.start_step: return\n",
        "\n",
        "        # Calculate mean sigma_a across layers for logging\n",
        "        avg_sigma = 0.0\n",
        "        if metrics:\n",
        "            sigmas = [m['sigma_a'].mean().item() for m in metrics if 'sigma_a' in m]\n",
        "            if sigmas: avg_sigma = sum(sigmas) / len(sigmas)\n",
        "\n",
        "        row = {\n",
        "            \"step\": step, \"loss\": loss, \"val_loss\": val_loss,\n",
        "            \"perplexity\": math.exp(val_loss) if val_loss < 20 else 0.0,\n",
        "            \"pressure\": pressure,\n",
        "            \"sigma_a_avg\": avg_sigma\n",
        "        }\n",
        "        self.buffer.append(row)\n",
        "    def flush(self):\n",
        "        if not self.buffer: return\n",
        "        df = pd.DataFrame(self.buffer)\n",
        "        fpath = os.path.join(self.save_dir, \"telemetry_adaptive.parquet\")\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                existing = pd.read_parquet(fpath)\n",
        "                df = pd.concat([existing, df])\n",
        "            except: pass\n",
        "        df.to_parquet(fpath)\n",
        "        self.buffer = []\n",
        "        print(\"üíæ Telemetry Flushed.\")\n",
        "\n",
        "# --- 7. RUN LOOP ---\n",
        "def run_adaptive():\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    cfg = AdaptiveConfig()\n",
        "    model = NewGPT(cfg).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=6.29e-4, weight_decay=1.34e-4)\n",
        "    controller = Homeostat(cfg)\n",
        "    loader = ChunkLoader(cfg)\n",
        "    recorder = BlackBox(SAVE_DIR)\n",
        "\n",
        "    start_step = 0\n",
        "    # START FRESH OR RESUME ADAPTIVE\n",
        "    # We do NOT recommend grafting 4k Constant here,\n",
        "    # better to start fresh 0-1500 warm-up to test the whole curve cleanly.\n",
        "\n",
        "    ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "    if ckpts:\n",
        "        ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "        latest = ckpts[-1]\n",
        "        ckpt_path = os.path.join(SAVE_DIR, latest)\n",
        "        print(f\"üîÑ Resuming Adaptive Run from {latest}...\")\n",
        "        c = torch.load(ckpt_path, map_location=DEVICE)\n",
        "        model.load_state_dict(c['model'])\n",
        "        optimizer.load_state_dict(c['optim'])\n",
        "        # Restore Controller State\n",
        "        controller.current_lambda = c.get('current_lambda', 0.0)\n",
        "        start_step = c['step'] + 1\n",
        "        recorder.set_start_step(start_step)\n",
        "\n",
        "    print(f\"\\nüß† STARTING ADAPTIVE RUN: {start_step} -> {cfg.max_steps}\")\n",
        "    print(f\"   Target Sigma: {cfg.target_sigma}\")\n",
        "    print(f\"   Warmup Steps: {cfg.warmup_steps}\")\n",
        "\n",
        "    pbar = tqdm(range(start_step, cfg.max_steps), initial=start_step, total=cfg.max_steps)\n",
        "\n",
        "    current_sigma_avg = None\n",
        "\n",
        "    for step in pbar:\n",
        "        loader.load_for_step(step)\n",
        "\n",
        "        # --- A. CONTROL UPDATE ---\n",
        "        # Update lambda based on LAST step's sigma metrics\n",
        "        if step % cfg.control_interval == 0 or step < cfg.warmup_steps:\n",
        "             new_lambda = controller.update(step, current_sigma_avg)\n",
        "\n",
        "        # Apply lambda to layers\n",
        "        # Simplified: Same lambda for div, 20% of that for coh\n",
        "        p = controller.current_lambda\n",
        "        base_coh = p * 0.2\n",
        "        lambdas_list = []\n",
        "        for i in range(cfg.n_layers):\n",
        "            # Ratio scaling still applies? Yes, keeps layers distinct.\n",
        "            ratio = (i + 1) / cfg.n_layers\n",
        "            s_mult = ratio ** 3\n",
        "            lambdas_list.append((base_coh * s_mult, p * s_mult))\n",
        "\n",
        "        # --- B. TRAINING STEP ---\n",
        "        model.train()\n",
        "        batch_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        accum_sigmas = [] # Track sigma during accum for controller\n",
        "\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = loader.get_batch(cfg.batch_size)\n",
        "\n",
        "            # Check if we need metrics for logging OR control\n",
        "            # We need metrics every 'control_interval' to feed the controller next step\n",
        "            is_control_step = (step % cfg.control_interval == 0)\n",
        "            is_log_step = (step % cfg.lite_interval == 0)\n",
        "            return_metrics = is_control_step or is_log_step\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                loss, steer, metrics = model(x, lambdas_list, y, return_metrics=return_metrics)\n",
        "                total = (loss + steer) / cfg.grad_accum\n",
        "\n",
        "            total.backward()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "            if return_metrics and metrics:\n",
        "                # Extract mean sigma for controller\n",
        "                s = [m['sigma_a'].mean().item() for m in metrics]\n",
        "                if s: accum_sigmas.append(sum(s)/len(s))\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update Sigma Average for Next Control Step\n",
        "        if accum_sigmas:\n",
        "            current_sigma_avg = sum(accum_sigmas) / len(accum_sigmas)\n",
        "\n",
        "        # --- C. TELEMETRY ---\n",
        "        if step > 0 and (step % cfg.lite_interval == 0 or step == cfg.max_steps - 1):\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                v_losses = []\n",
        "                for _ in range(10): # Reduced val batches for speed\n",
        "                    vx, vy = loader.get_val_batch(cfg.batch_size)\n",
        "                    vl, _, _ = model(vx, [(0.0,0.0)]*cfg.n_layers, vy, return_metrics=False)\n",
        "                    v_losses.append(vl.item())\n",
        "                val_loss = np.mean(v_losses)\n",
        "\n",
        "            recorder.log(step, batch_loss/cfg.grad_accum, val_loss, controller.current_lambda, metrics)\n",
        "            recorder.flush()\n",
        "\n",
        "            # Update Progress Bar with Adaptive Info\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|V:{val_loss:.2f}|P:{controller.current_lambda:.3f}|Sig:{current_sigma_avg:.4f}\")\n",
        "        else:\n",
        "            pbar.set_description(f\"L:{batch_loss/cfg.grad_accum:.2f}|P:{controller.current_lambda:.3f}|Sig:{current_sigma_avg if current_sigma_avg else 0:.4f}\")\n",
        "\n",
        "        # --- D. CHECKPOINTS ---\n",
        "        if step > 0 and step % cfg.ckpt_interval == 0:\n",
        "            ckpt_name = f\"ckpt_step_{step}.pt\"\n",
        "            ckpt_path = os.path.join(SAVE_DIR, ckpt_name)\n",
        "            save_dict = {\n",
        "                'step': step,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'current_lambda': controller.current_lambda, # Save controller state\n",
        "                'rng_cpu': torch.get_rng_state(),\n",
        "                'rng_gpu': torch.cuda.get_rng_state()\n",
        "            }\n",
        "            torch.save(save_dict, ckpt_path)\n",
        "\n",
        "            all_ckpts = [f for f in os.listdir(SAVE_DIR) if f.startswith(\"ckpt_step_\") and f.endswith(\".pt\")]\n",
        "            all_ckpts.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "            if len(all_ckpts) > 3: os.remove(os.path.join(SAVE_DIR, all_ckpts[0]))\n",
        "\n",
        "    final_path = os.path.join(SAVE_DIR, \"janus_adaptive_final.pt\")\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f\"\\nüèÜ ADAPTIVE RUN COMPLETE. Saved to {final_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_adaptive()"
      ],
      "metadata": {
        "id": "TvkowkCf30CE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CiZxWb-u31P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3ukAyaB31qw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}